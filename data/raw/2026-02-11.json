[{"paper":{"id":"2602.05400","authors":[{"_id":"698b396b1b2dc6b37d61b4be","user":{"_id":"66968099c952e09a4cb29f78","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp","isPro":false,"fullname":"Wang","user":"Steven-Shaobo","type":"user"},"name":"Shaobo Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:16:57.815Z","hidden":false},{"_id":"698b396b1b2dc6b37d61b4bf","user":{"_id":"67e617d4470f96a302734e16","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png","isPro":false,"fullname":"Xuan Ouyang","user":"YoungXuan","type":"user"},"name":"Xuan Ouyang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:16:55.631Z","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c0","user":{"_id":"6518a144a28f86d3e9e67c34","avatarUrl":"/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg","isPro":false,"fullname":"Tianyi Xu","user":"tianyi0216","type":"user"},"name":"Tianyi Xu","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:16:53.605Z","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c1","name":"Yuzheng Hu","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c2","name":"Jialin Liu","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c3","name":"Guo Chen","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c4","name":"Tianyu Zhang","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c5","name":"Junhao Zheng","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c6","name":"Kexin Yang","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c7","name":"Xingzhang Ren","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c8","name":"Dayiheng Liu","hidden":false},{"_id":"698b396b1b2dc6b37d61b4c9","name":"Linfeng Zhang","hidden":false}],"publishedAt":"2026-02-05T07:34:23.000Z","submittedOnDailyAt":"2026-02-11T02:09:03.945Z","title":"OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration","submittedOnDailyBy":{"_id":"67e617d4470f96a302734e16","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png","isPro":false,"fullname":"Xuan Ouyang","user":"YoungXuan","type":"user"},"summary":"As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.","upvotes":320,"discussionId":"698b396b1b2dc6b37d61b4ca","ai_summary":"OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.","ai_keywords":["data selection","optimizer-induced update space","effective updates","stable in-distribution proxy","Ghost technique","CountSketch","Boltzmann sampling","pre-training","GPT-2","Qwen3-8B-Base","FineWeb","FineWeb-Edu","SciencePedia"],"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}},"publishedAt":"2026-02-05T02:34:23.000Z","title":"OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration","summary":"As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png","numComments":3,"submittedBy":{"_id":"67e617d4470f96a302734e16","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png","fullname":"Xuan Ouyang","name":"YoungXuan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":24,"isUserFollowing":false},"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09856","authors":[{"_id":"698bf5b66052d3bed9630aa7","user":{"_id":"64107c7df52d7eb22e062956","avatarUrl":"/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg","isPro":false,"fullname":"Yuhao Zheng","user":"yhzheng1031","type":"user"},"name":"Yuhao Zheng","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:28.241Z","hidden":false},{"_id":"698bf5b66052d3bed9630aa8","name":"Li'an Zhong","hidden":false},{"_id":"698bf5b66052d3bed9630aa9","user":{"_id":"6773bcaa675a971ddf1e81dd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png","isPro":false,"fullname":"CokeWang","user":"CokeWang","type":"user"},"name":"Yi Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:30.778Z","hidden":false},{"_id":"698bf5b66052d3bed9630aaa","user":{"_id":"661de9defdbc9c247f159d15","avatarUrl":"/avatars/38e21e78327cc908201122405c48f41b.svg","isPro":false,"fullname":"Rui Dai","user":"DerryD","type":"user"},"name":"Rui Dai","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:25.982Z","hidden":false},{"_id":"698bf5b66052d3bed9630aab","name":"Kaikui Liu","hidden":false},{"_id":"698bf5b66052d3bed9630aac","name":"Xiangxiang Chu","hidden":false},{"_id":"698bf5b66052d3bed9630aad","name":"Linyuan Lv","hidden":false},{"_id":"698bf5b66052d3bed9630aae","name":"Philip Torr","hidden":false},{"_id":"698bf5b66052d3bed9630aaf","user":{"_id":"64440be5af034cdfd69ca3a7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg","isPro":false,"fullname":"Qinghong (Kevin) Lin","user":"KevinQHLin","type":"user"},"name":"Kevin Qinghong Lin","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:23.397Z","hidden":false}],"publishedAt":"2026-02-10T14:56:19.000Z","submittedOnDailyAt":"2026-02-11T01:02:42.385Z","title":"Code2World: A GUI World Model via Renderable Code Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.","upvotes":191,"discussionId":"698bf5b66052d3bed9630ab0","projectPage":"https://amap-ml.github.io/Code2World/","githubRepo":"https://github.com/AMAP-ML/Code2World","githubRepoAddedBy":"user","ai_summary":"Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.","ai_keywords":["vision-language coder","GUI World model","action-conditioned prediction","AndroidCode","HTML generation","visual-feedback revision mechanism","SFT","Render-Aware Reinforcement Learning","visual semantic fidelity","action consistency","next UI prediction","AndroidWorld navigation"],"githubStars":184,"organization":{"_id":"67d11771890254196d3174e5","name":"GD-ML","fullname":"AMAP-ML","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}},"publishedAt":"2026-02-10T09:56:19.000Z","title":"Code2World: A GUI World Model via Renderable Code Generation","summary":"Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"67d11771890254196d3174e5","name":"GD-ML","fullname":"AMAP-ML","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09082","authors":[{"_id":"698bea506052d3bed96309cb","name":"Veuns-Team","hidden":false},{"_id":"698bea506052d3bed96309cd","name":"Changlong Gao","hidden":false},{"_id":"698bea506052d3bed96309ce","user":{"_id":"60d2a2984956988b63753371","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg","isPro":false,"fullname":"Zhangxuan Gu","user":"zhangxgu","type":"user"},"name":"Zhangxuan Gu","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:14.456Z","hidden":false},{"_id":"698bea506052d3bed96309cf","name":"Yulin Liu","hidden":false},{"_id":"698bea506052d3bed96309d0","user":{"_id":"66aa0c7ad9b96f88862a192f","avatarUrl":"/avatars/5ebf92c2ca8df51a481dcda3cf43d3c5.svg","isPro":false,"fullname":"xinyu qiu","user":"x1nyu","type":"user"},"name":"Xinyu Qiu","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:50:39.714Z","hidden":false},{"_id":"698bea506052d3bed96309d1","name":"Shuheng Shen","hidden":false},{"_id":"698bea506052d3bed96309d2","name":"Yue Wen","hidden":false},{"_id":"698bea506052d3bed96309d3","name":"Tianyu Xia","hidden":false},{"_id":"698bea506052d3bed96309d4","name":"Zhenyu Xu","hidden":false},{"_id":"698bea506052d3bed96309d5","user":{"_id":"64cb238576200ec80fe988f8","avatarUrl":"/avatars/42c48710c7881c9dfbcc075fec3cb600.svg","isPro":false,"fullname":"zeus","user":"zengw","type":"user"},"name":"Zhengwen Zeng","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:24:43.235Z","hidden":false},{"_id":"698bea506052d3bed96309d6","user":{"_id":"654c9dac09dd7ef524a0be1e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/654c9dac09dd7ef524a0be1e/T4glmZthS0mJydhvGZGKH.png","isPro":false,"fullname":"beitongzhou","user":"syorami","type":"user"},"name":"Beitong Zhou","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:11.859Z","hidden":false},{"_id":"698bea506052d3bed96309d7","name":"Xingran Zhou","hidden":false},{"_id":"698bea506052d3bed96309d8","name":"Weizhi Chen","hidden":false},{"_id":"698bea506052d3bed96309d9","name":"Sunhao Dai","hidden":false},{"_id":"698bea506052d3bed96309da","name":"Jingya Dou","hidden":false},{"_id":"698bea506052d3bed96309db","name":"Yichen Gong","hidden":false},{"_id":"698bea506052d3bed96309dc","name":"Yuan Guo","hidden":false},{"_id":"698bea506052d3bed96309dd","name":"Zhenlin Guo","hidden":false},{"_id":"698bea506052d3bed96309de","user":{"_id":"65e0763a9299e96ee674876e","avatarUrl":"/avatars/0ea342c9f72fa3b8a8f634559d094907.svg","isPro":false,"fullname":"fengdian","user":"fengrudian","type":"user"},"name":"Feng Li","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:16:04.463Z","hidden":false},{"_id":"698bea506052d3bed96309df","name":"Qian Li","hidden":false},{"_id":"698bea506052d3bed96309e0","name":"Jinzhen Lin","hidden":false},{"_id":"698bea506052d3bed96309e1","name":"Yuqi Zhou","hidden":false},{"_id":"698bea506052d3bed96309e2","name":"Linchao Zhu","hidden":false},{"_id":"698bea506052d3bed96309e3","name":"Liang Chen","hidden":false},{"_id":"698bea506052d3bed96309e4","name":"Zhenyu Guo","hidden":false},{"_id":"698bea506052d3bed96309e5","name":"Changhua Meng","hidden":false},{"_id":"698bea506052d3bed96309e6","name":"Weiqiang Wang","hidden":false}],"publishedAt":"2026-02-09T18:43:40.000Z","submittedOnDailyAt":"2026-02-11T00:10:55.649Z","title":"UI-Venus-1.5 Technical Report","submittedOnDailyBy":{"_id":"60d2a2984956988b63753371","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg","isPro":false,"fullname":"Zhangxuan Gu","user":"zhangxgu","type":"user"},"summary":"GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus","upvotes":151,"discussionId":"698bea516052d3bed96309e7","projectPage":"https://ui-venus.github.io/UI-Venus-1.5/","githubRepo":"https://github.com/inclusionAI/UI-Venus","githubRepoAddedBy":"user","ai_summary":"UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.","ai_keywords":["GUI agents","Mid-Training stage","Online Reinforcement Learning","full-trajectory rollouts","Model Merging","dense variants","mixture-of-experts variant"],"githubStars":1104,"organization":{"_id":"67aea5c8f086ab0f70ed97c9","name":"inclusionAI","fullname":"inclusionAI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}},"publishedAt":"2026-02-09T13:43:40.000Z","title":"UI-Venus-1.5 Technical Report","summary":"GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09082.png","numComments":3,"submittedBy":{"_id":"60d2a2984956988b63753371","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg","fullname":"Zhangxuan Gu","name":"zhangxgu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"67aea5c8f086ab0f70ed97c9","name":"inclusionAI","fullname":"inclusionAI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.10063","authors":[{"_id":"698bf4ef6052d3bed9630a96","user":{"_id":"6895e7f146763431aea25ca4","avatarUrl":"/avatars/52e550c3f7e8da2e31b63413e2e71e6c.svg","isPro":false,"fullname":"Tianyi Jiang","user":"LumosJiang","type":"user"},"name":"Tianyi Jiang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:33.352Z","hidden":false},{"_id":"698bf4ef6052d3bed9630a97","name":"Arctanx An","hidden":false},{"_id":"698bf4ef6052d3bed9630a98","name":"Hengyi Feng","hidden":false},{"_id":"698bf4ef6052d3bed9630a99","name":"Naixin Zhai","hidden":false},{"_id":"698bf4ef6052d3bed9630a9a","name":"Haodong Li","hidden":false},{"_id":"698bf4ef6052d3bed9630a9b","user":{"_id":"64084fa192033c150738e4f2","avatarUrl":"/avatars/dfff2216eb235c635e5abe6fda3084f0.svg","isPro":false,"fullname":"Yu_xm","user":"Yu2020","type":"user"},"name":"Xiaomin Yu","status":"claimed_verified","statusLastChangedAt":"2026-02-11T12:34:26.745Z","hidden":false},{"_id":"698bf4ef6052d3bed9630a9c","name":"Jiahui Liu","hidden":false},{"_id":"698bf4ef6052d3bed9630a9d","name":"Hanwen Du","hidden":false},{"_id":"698bf4ef6052d3bed9630a9e","name":"Shuo Zhang","hidden":false},{"_id":"698bf4ef6052d3bed9630a9f","user":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","isPro":false,"fullname":"Zhi Yang","user":"yangzhi1","type":"user"},"name":"Zhi Yang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:35.722Z","hidden":false},{"_id":"698bf4ef6052d3bed9630aa0","name":"Jie Huang","hidden":false},{"_id":"698bf4ef6052d3bed9630aa1","name":"Yuhua Li","hidden":false},{"_id":"698bf4ef6052d3bed9630aa2","name":"Yongxin Ni","hidden":false},{"_id":"698bf4ef6052d3bed9630aa3","name":"Huacan Wang","hidden":false},{"_id":"698bf4ef6052d3bed9630aa4","name":"Ronghao Chen","hidden":false}],"publishedAt":"2026-02-10T18:31:47.000Z","submittedOnDailyAt":"2026-02-11T00:51:58.024Z","title":"Chain of Mindset: Reasoning with Adaptive Cognitive Modes","submittedOnDailyBy":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","isPro":false,"fullname":"Zhi Yang","user":"yangzhi1","type":"user"},"summary":"Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.","upvotes":71,"discussionId":"698bf4f06052d3bed9630aa5","githubRepo":"https://github.com/QuantaAlpha/chain-of-mindset","githubRepoAddedBy":"user","ai_summary":"A novel training-free framework called Chain of Mindset enables step-level adaptive mindset orchestration for large language models by integrating spatial, convergent, divergent, and algorithmic reasoning approaches.","ai_keywords":["Chain of Mindset","CoM","agentic framework","step-level adaptive mindset orchestration","Spatial mindset","Convergent mindset","Divergent mindset","Algorithmic mindset","Meta-Agent","bidirectional Context Gate","reasoning efficiency","large language models"],"githubStars":77,"organization":{"_id":"68b33ab6a9ed99140481cf44","name":"QuantaAlpha","fullname":"QuantaAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}},"publishedAt":"2026-02-10T13:31:47.000Z","title":"Chain of Mindset: Reasoning with Adaptive Cognitive Modes","summary":"Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset{https://github.com/QuantaAlpha/chain-of-mindset}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10063.png","numComments":2,"submittedBy":{"_id":"64aa645404e7b379feccc490","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png","fullname":"Zhi Yang","name":"yangzhi1","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"68b33ab6a9ed99140481cf44","name":"QuantaAlpha","fullname":"QuantaAlpha","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.08234","authors":[{"_id":"698aba731b2dc6b37d61b0e4","user":{"_id":"643e9ee6f6bb3c31a26e7bc4","avatarUrl":"/avatars/acfaa7d6a23dada24c86b954c3be116a.svg","isPro":false,"fullname":"Peng Xia","user":"richardxp888","type":"user"},"name":"Peng Xia","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:18:58.896Z","hidden":false},{"_id":"698aba731b2dc6b37d61b0e5","name":"Jianwen Chen","hidden":false},{"_id":"698aba731b2dc6b37d61b0e6","name":"Hanyang Wang","hidden":false},{"_id":"698aba731b2dc6b37d61b0e7","name":"Jiaqi Liu","hidden":false},{"_id":"698aba731b2dc6b37d61b0e8","name":"Kaide Zeng","hidden":false},{"_id":"698aba731b2dc6b37d61b0e9","user":{"_id":"63234809155b0e2c44f354d6","avatarUrl":"/avatars/60d38f8f0e12363f3f5e0388e635d7b6.svg","isPro":false,"fullname":"Yu Wang","user":"YuWangX","type":"user"},"name":"Yu Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:18:56.703Z","hidden":false},{"_id":"698aba731b2dc6b37d61b0ea","name":"Siwei Han","hidden":false},{"_id":"698aba731b2dc6b37d61b0eb","name":"Yiyang Zhou","hidden":false},{"_id":"698aba731b2dc6b37d61b0ec","name":"Xujiang Zhao","hidden":false},{"_id":"698aba731b2dc6b37d61b0ed","name":"Haifeng Chen","hidden":false},{"_id":"698aba731b2dc6b37d61b0ee","name":"Zeyu Zheng","hidden":false},{"_id":"698aba731b2dc6b37d61b0ef","name":"Cihang Xie","hidden":false},{"_id":"698aba731b2dc6b37d61b0f0","name":"Huaxiu Yao","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/L9mDRVV2qoifMcWtJ_1ib.jpeg"],"publishedAt":"2026-02-09T03:17:17.000Z","submittedOnDailyAt":"2026-02-11T00:14:23.139Z","title":"SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning","submittedOnDailyBy":{"_id":"643e9ee6f6bb3c31a26e7bc4","avatarUrl":"/avatars/acfaa7d6a23dada24c86b954c3be116a.svg","isPro":false,"fullname":"Peng Xia","user":"richardxp888","type":"user"},"summary":"Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.","upvotes":66,"discussionId":"698aba731b2dc6b37d61b0f1","githubRepo":"https://github.com/aiming-lab/SkillRL","githubRepoAddedBy":"user","ai_summary":"SkillRL enables LLM agents to improve through hierarchical skill discovery and recursive policy evolution, achieving superior performance on complex tasks while reducing computational overhead.","ai_keywords":["large language model agents","reinforcement learning","skill discovery","recursive evolution","skill library","SkillBank","experience-based distillation","adaptive retrieval strategy","policy improvement","token footprint"],"githubStars":491,"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}},"publishedAt":"2026-02-08T22:17:17.000Z","title":"SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning","summary":"Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/L9mDRVV2qoifMcWtJ_1ib.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08234.png","numComments":2,"submittedBy":{"_id":"643e9ee6f6bb3c31a26e7bc4","avatarUrl":"/avatars/acfaa7d6a23dada24c86b954c3be116a.svg","fullname":"Peng Xia","name":"richardxp888","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09443","authors":[{"_id":"698c019f6052d3bed9630b1c","name":"Yun Luo","hidden":false},{"_id":"698c019f6052d3bed9630b1d","user":{"_id":"6418228b83957c4eaaad4d01","avatarUrl":"/avatars/b6af01d09bba5d5ba7bf4a62914ca468.svg","isPro":false,"fullname":"wang","user":"astrid01052","type":"user"},"name":"Futing Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:44.168Z","hidden":false},{"_id":"698c019f6052d3bed9630b1e","name":"Qianjia Cheng","hidden":false},{"_id":"698c019f6052d3bed9630b1f","name":"Fangchen Yu","hidden":false},{"_id":"698c019f6052d3bed9630b20","name":"Haodi Lei","hidden":false},{"_id":"698c019f6052d3bed9630b21","name":"Jianhao Yan","hidden":false},{"_id":"698c019f6052d3bed9630b22","name":"Chenxi Li","hidden":false},{"_id":"698c019f6052d3bed9630b23","name":"Jiacheng Chen","hidden":false},{"_id":"698c019f6052d3bed9630b24","name":"Yufeng Zhao","hidden":false},{"_id":"698c019f6052d3bed9630b25","user":{"_id":"691b0f528411a45dc9ee9de8","avatarUrl":"/avatars/261c28f7e616a8482970f50c1f8919fd.svg","isPro":false,"fullname":"Haiyuan Wan","user":"HY-Wan","type":"user"},"name":"Haiyuan Wan","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:35.614Z","hidden":false},{"_id":"698c019f6052d3bed9630b26","name":"Yuchen Zhang","hidden":false},{"_id":"698c019f6052d3bed9630b27","name":"Shenghe Zheng","hidden":false},{"_id":"698c019f6052d3bed9630b28","name":"Junchi Yao","hidden":false},{"_id":"698c019f6052d3bed9630b29","name":"Qingyang Zhang","hidden":false},{"_id":"698c019f6052d3bed9630b2a","name":"Haonan He","hidden":false},{"_id":"698c019f6052d3bed9630b2b","name":"Wenxuan Zeng","hidden":false},{"_id":"698c019f6052d3bed9630b2c","name":"Li Sheng","hidden":false},{"_id":"698c019f6052d3bed9630b2d","name":"Chengxing Xie","hidden":false},{"_id":"698c019f6052d3bed9630b2e","name":"Yuxin Zuo","hidden":false},{"_id":"698c019f6052d3bed9630b2f","name":"Yizhuo Li","hidden":false},{"_id":"698c019f6052d3bed9630b30","name":"Yulun Wu","hidden":false},{"_id":"698c019f6052d3bed9630b31","name":"Rui Huang","hidden":false},{"_id":"698c019f6052d3bed9630b32","name":"Dongzhan Zhou","hidden":false},{"_id":"698c019f6052d3bed9630b33","name":"Kai Chen","hidden":false},{"_id":"698c019f6052d3bed9630b34","name":"Yu Qiao","hidden":false},{"_id":"698c019f6052d3bed9630b35","name":"Lei Bai","hidden":false},{"_id":"698c019f6052d3bed9630b36","name":"Yu Cheng","hidden":false},{"_id":"698c019f6052d3bed9630b37","name":"Ning Ding","hidden":false},{"_id":"698c019f6052d3bed9630b38","name":"Bowen Zhou","hidden":false},{"_id":"698c019f6052d3bed9630b39","name":"Peng Ye","hidden":false},{"_id":"698c019f6052d3bed9630b3a","name":"Ganqu Cui","hidden":false}],"publishedAt":"2026-02-10T06:28:08.000Z","submittedOnDailyAt":"2026-02-11T04:00:48.041Z","title":"P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads","submittedOnDailyBy":{"_id":"6086838b19137b3a6ba760e7","avatarUrl":"/avatars/d63eea3e39b22c6e65b82c28192696f1.svg","isPro":false,"fullname":"Jianhao Yan","user":"Elliott","type":"user"},"summary":"The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.","upvotes":57,"discussionId":"698c019f6052d3bed9630b3b","projectPage":"https://prime-rl.github.io/P1-VL","githubRepo":"https://github.com/PRIME-RL/P1-VL","githubRepoAddedBy":"user","ai_summary":"Physics-oriented vision-language models leverage curriculum reinforcement learning and agentic augmentation to achieve state-of-the-art scientific reasoning performance while maintaining physical consistency through multimodal perception.","ai_keywords":["vision-language models","curriculum reinforcement learning","agentic augmentation","multimodal perception","scientific reasoning","physical consistency","HiPhO benchmark","P1-VL-235B-A22B","Gemini-3-Pro"],"githubStars":14,"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "}},"publishedAt":"2026-02-10T01:28:08.000Z","title":"P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads","summary":"The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09443.png","numComments":2,"submittedBy":{"_id":"6086838b19137b3a6ba760e7","avatarUrl":"/avatars/d63eea3e39b22c6e65b82c28192696f1.svg","fullname":"Jianhao Yan","name":"Elliott","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"6747ee5decec679eafb90450","name":"ShanghaiAiLab","fullname":"shanghai ailab "},"isAuthorParticipating":false},{"paper":{"id":"2602.10090","authors":[{"_id":"698bf6fe6052d3bed9630ac0","name":"Zhaoyang Wang","hidden":false},{"_id":"698bf6fe6052d3bed9630ac1","name":"Canwen Xu","hidden":false},{"_id":"698bf6fe6052d3bed9630ac2","name":"Boyi Liu","hidden":false},{"_id":"698bf6fe6052d3bed9630ac3","name":"Yite Wang","hidden":false},{"_id":"698bf6fe6052d3bed9630ac4","name":"Siwei Han","hidden":false},{"_id":"698bf6fe6052d3bed9630ac5","name":"Zhewei Yao","hidden":false},{"_id":"698bf6fe6052d3bed9630ac6","name":"Huaxiu Yao","hidden":false},{"_id":"698bf6fe6052d3bed9630ac7","name":"Yuxiong He","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/at1Sens0OXJ4Yt8ne9kAE.png","https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/Hs08PrK-yZHZ5FBmPLoMV.png","https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/IIL00IFjA5UOIILbLKOY9.png","https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/o5mHd9GtSOBL8Ni_s_J8B.png","https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/jsnsJiwl4N10Px2LgZ9Ve.png"],"publishedAt":"2026-02-10T18:55:41.000Z","submittedOnDailyAt":"2026-02-11T02:28:53.819Z","title":"Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning","submittedOnDailyBy":{"_id":"633122d3f242a8532b7a928d","avatarUrl":"/avatars/2158ffff0882a8fb4588e273fd60dea7.svg","isPro":true,"fullname":"Chi","user":"ChilleD","type":"user"},"summary":"Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.","upvotes":49,"discussionId":"698bf6ff6052d3bed9630ac8","projectPage":"https://github.com/Snowflake-Labs/agent-world-model","githubRepo":"https://github.com/Snowflake-Labs/agent-world-model","githubRepoAddedBy":"user","ai_summary":"Large language model agents trained in synthetic environments with code-driven simulations and database-backed state transitions demonstrate superior out-of-distribution generalization compared to traditional benchmark-specific approaches.","ai_keywords":["large language model","autonomous agents","multi-turn interactions","tool-use agents","reinforcement learning","synthetic environment generation","code-driven environments","database-backed state transitions","out-of-distribution generalization"],"githubStars":230,"organization":{"_id":"62cece4aa3a23014aca72499","name":"Snowflake","fullname":"Snowflake","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"}},"publishedAt":"2026-02-10T13:55:41.000Z","title":"Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning","summary":"Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/at1Sens0OXJ4Yt8ne9kAE.png","https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/Hs08PrK-yZHZ5FBmPLoMV.png","https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/IIL00IFjA5UOIILbLKOY9.png","https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/o5mHd9GtSOBL8Ni_s_J8B.png","https://cdn-uploads.huggingface.co/production/uploads/633122d3f242a8532b7a928d/jsnsJiwl4N10Px2LgZ9Ve.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10090.png","numComments":2,"submittedBy":{"_id":"633122d3f242a8532b7a928d","avatarUrl":"/avatars/2158ffff0882a8fb4588e273fd60dea7.svg","fullname":"Chi","name":"ChilleD","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"62cece4aa3a23014aca72499","name":"Snowflake","fullname":"Snowflake","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64dc52cf858f8a41c12fc819/O9-MWzRjWzbNP_DQlMb-7.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08426","authors":[{"_id":"698bedd06052d3bed9630a13","user":{"_id":"6191f22d08a57f265f7f5266","avatarUrl":"/avatars/215164e7b6d025a3c32555ff541cdd62.svg","isPro":false,"fullname":"XinghaoWang","user":"Singhoo","type":"user"},"name":"Xinghao Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:09.483Z","hidden":false},{"_id":"698bedd06052d3bed9630a14","name":"Pengyu Wang","hidden":false},{"_id":"698bedd06052d3bed9630a15","user":{"_id":"64f033ef82c6eea604c4da8b","avatarUrl":"/avatars/51b93fea7fd68b4274ee03701245dcca.svg","isPro":false,"fullname":"Xiaoran Liu (SII)","user":"SII-xrliu","type":"user"},"name":"Xiaoran Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:02.192Z","hidden":false},{"_id":"698bedd06052d3bed9630a16","name":"Fangxu Liu","hidden":false},{"_id":"698bedd06052d3bed9630a17","name":"Jason Chu","hidden":false},{"_id":"698bedd06052d3bed9630a18","name":"Kai Song","hidden":false},{"_id":"698bedd06052d3bed9630a19","name":"Xipeng Qiu","hidden":false}],"publishedAt":"2026-02-09T09:31:06.000Z","submittedOnDailyAt":"2026-02-11T00:29:34.413Z","title":"Prism: Spectral-Aware Block-Sparse Attention","submittedOnDailyBy":{"_id":"6191f22d08a57f265f7f5266","avatarUrl":"/avatars/215164e7b6d025a3c32555ff541cdd62.svg","isPro":false,"fullname":"XinghaoWang","user":"Singhoo","type":"user"},"summary":"Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.","upvotes":35,"discussionId":"698bedd06052d3bed9630a1a","projectPage":"https://efficacious-citrus-7a0.notion.site/Prism-Spectral-Aware-Block-Sparse-Attention-304d97f5df9d80318802f9cb37d18c3e","githubRepo":"https://github.com/xinghaow99/prism","githubRepoAddedBy":"user","ai_summary":"Prism addresses inefficiencies in block-sparse attention for long-context LLM pre-filling by using a spectral-aware approach that improves block selection accuracy through energy-based temperature calibration.","ai_keywords":["block-sparse attention","long-context LLM","pre-filling","coarse-grained attention","Rotary Positional Embeddings","RoPE","mean pooling","low-pass filter","destructive interference","spectral-aware approach","energy-based temperature calibration","block selection","attention mechanisms"],"githubStars":21,"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}},"publishedAt":"2026-02-09T04:31:06.000Z","title":"Prism: Spectral-Aware Block-Sparse Attention","summary":"Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1times speedup.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08426.png","numComments":3,"submittedBy":{"_id":"6191f22d08a57f265f7f5266","avatarUrl":"/avatars/215164e7b6d025a3c32555ff541cdd62.svg","fullname":"XinghaoWang","name":"Singhoo","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"613b0dee83ec35d460684607","name":"OpenMOSS-Team","fullname":"OpenMOSS","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.07035","authors":[{"_id":"698b43df6052d3bed963079f","user":{"_id":"698b419584704fee74958520","avatarUrl":"/avatars/7e94be7a4396174e3546114c3e3af598.svg","isPro":false,"fullname":"Zhao Jiahao","user":"bubble65","type":"user"},"name":"Jiahao Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:16:42.263Z","hidden":false},{"_id":"698b43df6052d3bed96307a0","name":"Shaoxuan Xu","hidden":false},{"_id":"698b43df6052d3bed96307a1","user":{"_id":"6309bfdab8d7b3889319b588","avatarUrl":"/avatars/572acdad470f765ef2e058ead3741e24.svg","isPro":false,"fullname":"SunZX","user":"Jeryi","type":"user"},"name":"Zhongxiang Sun","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:16:40.143Z","hidden":false},{"_id":"698b43df6052d3bed96307a2","name":"Fengqi Zhu","hidden":false},{"_id":"698b43df6052d3bed96307a3","name":"Jingyang Ou","hidden":false},{"_id":"698b43df6052d3bed96307a4","user":{"_id":"645b0c3ec35da9c7afd95421","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg","isPro":false,"fullname":"Yuling","user":"YerbaPage","type":"user"},"name":"Yuling Shi","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:16:37.162Z","hidden":false},{"_id":"698b43df6052d3bed96307a5","name":"Chongxuan Li","hidden":false},{"_id":"698b43df6052d3bed96307a6","name":"Xiao Zhang","hidden":false},{"_id":"698b43df6052d3bed96307a7","name":"Jun Xu","hidden":false}],"publishedAt":"2026-02-03T09:12:08.000Z","submittedOnDailyAt":"2026-02-11T01:28:30.266Z","title":"DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents","submittedOnDailyBy":{"_id":"6309bfdab8d7b3889319b588","avatarUrl":"/avatars/572acdad470f765ef2e058ead3741e24.svg","isPro":false,"fullname":"SunZX","user":"Jeryi","type":"user"},"summary":"Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C","upvotes":30,"discussionId":"698b43df6052d3bed96307a8","projectPage":"https://bubble65.github.io/dllm-searcher-pub/","githubRepo":"https://github.com/bubble65/DLLM-Searcher","githubRepoAddedBy":"user","ai_summary":"Diffusion Large Language Models are optimized for search agents through enhanced reasoning capabilities and reduced latency via a parallel reasoning paradigm.","ai_keywords":["Diffusion Large Language Models","ReAct agent paradigm","Latency Challenge","Agentic Supervised Fine-Tuning","Agentic Variance-Reduced Preference Optimization","Parallel-Reasoning and Acting","P-ReAct"],"githubStars":13,"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"}},"publishedAt":"2026-02-03T04:12:08.000Z","title":"DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents","summary":"Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM's information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool's return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07035.png","numComments":2,"submittedBy":{"_id":"6309bfdab8d7b3889319b588","avatarUrl":"/avatars/572acdad470f765ef2e058ead3741e24.svg","fullname":"SunZX","name":"Jeryi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"622177ac43826d6f261f8208","name":"RUC","fullname":"Renmin University of China","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/670IAX9A2-BflqA5MiSBW.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.10104","authors":[{"_id":"698bfbeb6052d3bed9630ae1","user":{"_id":"64e84d40d50f3979be9afcbb","avatarUrl":"/avatars/6a706a4916132c1f1cda63d11dc46b87.svg","isPro":false,"fullname":"Jiang Yuxin","user":"YuxinJ","type":"user"},"name":"Yuxin Jiang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:13.438Z","hidden":false},{"_id":"698bfbeb6052d3bed9630ae2","name":"Yuchao Gu","hidden":false},{"_id":"698bfbeb6052d3bed9630ae3","name":"Ivor W. Tsang","hidden":false},{"_id":"698bfbeb6052d3bed9630ae4","name":"Mike Zheng Shou","hidden":false}],"publishedAt":"2026-02-10T18:58:41.000Z","submittedOnDailyAt":"2026-02-11T01:40:37.554Z","title":"Olaf-World: Orienting Latent Actions for Video World Modeling","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.","upvotes":27,"discussionId":"698bfbeb6052d3bed9630ae5","projectPage":"https://showlab.github.io/Olaf-World/","githubRepo":"https://github.com/showlab/Olaf-World","githubRepoAddedBy":"user","ai_summary":"Sequence-level control-effect alignment enables structured latent action space learning for zero-shot action transfer in video world models.","ai_keywords":["action-controllable world models","latent action learning","temporal feature differences","self-supervised video encoder","sequence-level control-effect alignment","action-conditioned video world models","zero-shot action transfer","data-efficient adaptation"],"githubStars":73},"publishedAt":"2026-02-10T13:58:41.000Z","title":"Olaf-World: Orienting Latent Actions for Video World Modeling","summary":"Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10104.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.09084","authors":[{"_id":"698be8996052d3bed96309ac","name":"Ruijie Ye","hidden":false},{"_id":"698be8996052d3bed96309ad","name":"Jiayi Zhang","hidden":false},{"_id":"698be8996052d3bed96309ae","name":"Zhuoxin Liu","hidden":false},{"_id":"698be8996052d3bed96309af","user":{"_id":"66dd321b41074b6a3df723d4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yeFbjcbmTT0U3c7i0eTdZ.jpeg","isPro":false,"fullname":"Zihao Zhu","user":"SingleBicycle","type":"user"},"name":"Zihao Zhu","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:16.736Z","hidden":false},{"_id":"698be8996052d3bed96309b0","name":"Siyuan Yang","hidden":false},{"_id":"698be8996052d3bed96309b1","name":"Li Li","hidden":false},{"_id":"698be8996052d3bed96309b2","name":"Tianfu Fu","hidden":false},{"_id":"698be8996052d3bed96309b3","user":{"_id":"62c5947524171688a9feb992","avatarUrl":"/avatars/5a151713b9eae8dc566f5957acee3475.svg","isPro":false,"fullname":"Franck Dernoncourt","user":"Franck-Dernoncourt","type":"user"},"name":"Franck Dernoncourt","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:50:37.345Z","hidden":false},{"_id":"698be8996052d3bed96309b4","name":"Yue Zhao","hidden":false},{"_id":"698be8996052d3bed96309b5","name":"Jiacheng Zhu","hidden":false},{"_id":"698be8996052d3bed96309b6","name":"Ryan Rossi","hidden":false},{"_id":"698be8996052d3bed96309b7","name":"Wenhao Chai","hidden":false},{"_id":"698be8996052d3bed96309b8","name":"Zhengzhong Tu","hidden":false}],"publishedAt":"2026-02-09T18:59:18.000Z","submittedOnDailyAt":"2026-02-11T00:04:02.116Z","title":"Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling","submittedOnDailyBy":{"_id":"62548d5fef3debb2ddf91217","avatarUrl":"/avatars/14975b45568f9c399c92c3986b6ce83e.svg","isPro":false,"fullname":"Zhengzhong Tu","user":"vztu","type":"user"},"summary":"We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.","upvotes":27,"discussionId":"698be89a6052d3bed96309b9","projectPage":"https://agent-banana.github.io/","githubRepo":"https://github.com/taco-group/agent-banana","githubRepoAddedBy":"user","ai_summary":"Agent Banana addresses challenges in instruction-based image editing through a hierarchical framework with context folding and image layer decomposition for high-fidelity, multi-turn editing at ultra-high resolution.","ai_keywords":["agentic planner-executor framework","context folding","image layer decomposition","multi-turn editing","high-fidelity editing","object-aware editing","deliberative editing","HDD-Bench","dialogue-based benchmark","native-resolution outputs","long-horizon control","stepwise targets"],"githubStars":39,"organization":{"_id":"693049768605dfa68334b46d","name":"TexasAMUniversity","fullname":"Texas A&M University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/uv9z1cu15X7vyo70DW0tH.png"}},"publishedAt":"2026-02-09T13:59:18.000Z","title":"Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling","summary":"We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09084.png","numComments":3,"submittedBy":{"_id":"62548d5fef3debb2ddf91217","avatarUrl":"/avatars/14975b45568f9c399c92c3986b6ce83e.svg","fullname":"Zhengzhong Tu","name":"vztu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"organization":{"_id":"693049768605dfa68334b46d","name":"TexasAMUniversity","fullname":"Texas A&M University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/uv9z1cu15X7vyo70DW0tH.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08847","authors":[{"_id":"698c133d6052d3bed9630bf2","user":{"_id":"66ba29dd59e8e7a957154c5f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png","isPro":false,"fullname":"Lang Feng","user":"langfeng01","type":"user"},"name":"Lang Feng","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:13:38.406Z","hidden":false},{"_id":"698c133d6052d3bed9630bf3","user":{"_id":"63db5dc49f2687298a1547bf","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63db5dc49f2687298a1547bf/xVFi0kRkYud191cQgma16.jpeg","isPro":false,"fullname":"Longtao Zheng","user":"ltzheng","type":"user"},"name":"Longtao Zheng","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:13:41.494Z","hidden":false},{"_id":"698c133d6052d3bed9630bf4","user":{"_id":"641d6099f9a3a9c532bd3954","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/641d6099f9a3a9c532bd3954/NBrTueSRKkxhdLvCdZ0lt.jpeg","isPro":false,"fullname":"Shuo He","user":"heshuo","type":"user"},"name":"Shuo He","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:44.843Z","hidden":false},{"_id":"698c133d6052d3bed9630bf5","user":{"_id":"64054d8a3d49e1e066bfa32b","avatarUrl":"/avatars/9044f937145cc5aa4bc3a5ffa751f724.svg","isPro":false,"fullname":"Fuxiang Zhang","user":"sicer","type":"user"},"name":"Fuxiang Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:13:54.465Z","hidden":false},{"_id":"698c133d6052d3bed9630bf6","name":"Bo An","hidden":false}],"publishedAt":"2026-02-09T16:13:39.000Z","submittedOnDailyAt":"2026-02-11T03:10:03.516Z","title":"Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems","submittedOnDailyBy":{"_id":"66ba29dd59e8e7a957154c5f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png","isPro":false,"fullname":"Lang Feng","user":"langfeng01","type":"user"},"summary":"Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.","upvotes":25,"discussionId":"698c133d6052d3bed9630bf7","githubRepo":"https://github.com/langfengQ/DrMAS","githubRepoAddedBy":"user","ai_summary":"Multi-agent large language model systems face training instability in reinforcement learning due to global normalization mismatches, which is addressed by Dr. MAS through agent-specific advantage normalization and enhanced training stability.","ai_keywords":["multi-agent LLM systems","reinforcement learning","GRPO-style optimization","gradient-norm instability","agent-wise remedy","advantage normalization","Qwen2.5","Qwen3 series"],"githubStars":82,"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"}},"publishedAt":"2026-02-09T11:13:39.000Z","title":"Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems","summary":"Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08847.png","numComments":2,"submittedBy":{"_id":"66ba29dd59e8e7a957154c5f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png","fullname":"Lang Feng","name":"langfeng01","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.07422","authors":[{"_id":"698c9b1b6c5152984e4f3c8c","name":"Tianyi Wu","hidden":false},{"_id":"698c9b1b6c5152984e4f3c8d","user":{"_id":"61711f02e0b1ddb56eb9b526","avatarUrl":"/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg","isPro":true,"fullname":"Mingzhe Du","user":"Elfsong","type":"user"},"name":"Mingzhe Du","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:50:46.234Z","hidden":false},{"_id":"698c9b1b6c5152984e4f3c8e","name":"Yue Liu","hidden":false},{"_id":"698c9b1b6c5152984e4f3c8f","name":"Chengran Yang","hidden":false},{"_id":"698c9b1b6c5152984e4f3c90","name":"Terry Yue Zhuo","hidden":false},{"_id":"698c9b1b6c5152984e4f3c91","name":"Jiaheng Zhang","hidden":false},{"_id":"698c9b1b6c5152984e4f3c92","name":"See-Kiong Ng","hidden":false}],"publishedAt":"2026-02-07T07:42:07.000Z","submittedOnDailyAt":"2026-02-11T12:39:27.357Z","title":"Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model","submittedOnDailyBy":{"_id":"685b9b62e896e7627649bd2f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YFxwvCmegiZHTaWif-xd1.png","isPro":false,"fullname":"Tianyi Wu","user":"tianyiwuhaha","type":"user"},"summary":"Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.","upvotes":21,"discussionId":"698c9b1b6c5152984e4f3c93","githubRepo":"https://github.com/AndrewWTY/SecCoderX","githubRepoAddedBy":"user","ai_summary":"SecCoderX uses online reinforcement learning to align large language models for secure code generation while preserving functionality, addressing the functionality-security trade-off through vulnerability detection integration and reward modeling.","ai_keywords":["large language models","secure code generation","online reinforcement learning","vulnerability detection","reward model","functionality-preserving","code alignment","security supervision"],"githubStars":60,"organization":{"_id":"6508ab2b349930913196378b","name":"NationalUniversityofSingapore","fullname":"National University of Singapore","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}},"publishedAt":"2026-02-07T02:42:07.000Z","title":"Secure Code Generation via Online Reinforcement Learning with Vulnerability Reward Model","summary":"Large language models (LLMs) are increasingly used in software development, yet their tendency to generate insecure code remains a major barrier to real-world deployment. Existing secure code alignment methods often suffer from a functionality--security paradox, improving security at the cost of substantial utility degradation. We propose SecCoderX, an online reinforcement learning framework for functionality-preserving secure code generation. SecCoderX first bridges vulnerability detection and secure code generation by repurposing mature detection resources in two ways: (i) synthesizing diverse, reality-grounded vulnerability-inducing coding tasks for online RL rollouts, and (ii) training a reasoning-based vulnerability reward model that provides scalable and reliable security supervision. Together, these components are unified in an online RL loop to align code LLMs to generate secure and functional code. Extensive experiments demonstrate that SecCoderX achieves state-of-the-art performance, improving Effective Safety Rate (ESR) by approximately 10% over unaligned models, whereas prior methods often degrade ESR by 14-54%. We release our code, dataset and model checkpoints at https://github.com/AndrewWTY/SecCoderX.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07422.png","numComments":2,"submittedBy":{"_id":"685b9b62e896e7627649bd2f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YFxwvCmegiZHTaWif-xd1.png","fullname":"Tianyi Wu","name":"tianyiwuhaha","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6508ab2b349930913196378b","name":"NationalUniversityofSingapore","fullname":"National University of Singapore","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.00268","authors":[{"_id":"698c6b52eb12ea7453916823","user":{"_id":"65d4985d4e358ce02a949f8c","avatarUrl":"/avatars/3eda6f50d17802b1ce94349c89637e3c.svg","isPro":false,"fullname":"Ariel Shaulov","user":"shaulov","type":"user"},"name":"Ariel Shaulov","status":"claimed_verified","statusLastChangedAt":"2026-02-11T12:34:28.330Z","hidden":false},{"_id":"698c6b52eb12ea7453916824","name":"Eitan Shaar","hidden":false},{"_id":"698c6b52eb12ea7453916825","name":"Amit Edenzon","hidden":false},{"_id":"698c6b52eb12ea7453916826","name":"Lior Wolf","hidden":false}],"publishedAt":"2026-01-30T19:44:16.000Z","submittedOnDailyAt":"2026-02-11T09:15:42.189Z","title":"TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation","submittedOnDailyBy":{"_id":"65d4985d4e358ce02a949f8c","avatarUrl":"/avatars/3eda6f50d17802b1ce94349c89637e3c.svg","isPro":false,"fullname":"Ariel Shaulov","user":"shaulov","type":"user"},"summary":"Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.","upvotes":21,"discussionId":"698c6b52eb12ea7453916827","projectPage":"https://arielshaulov.github.io/TokenTrim/","githubRepo":"https://github.com/arielshaulov/TokenTrim","githubRepoAddedBy":"user","ai_summary":"Auto-regressive video generation suffers from temporal drift due to error accumulation in latent conditioning tokens, which is addressed by identifying and removing unstable tokens during inference to improve long-horizon consistency.","ai_keywords":["auto-regressive video generation","temporal drift","latent conditioning tokens","inference-time error propagation","unstable tokens","latent space"],"githubStars":14},"publishedAt":"2026-01-30T14:44:16.000Z","title":"TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation","summary":"Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00268.png","numComments":3,"submittedBy":{"_id":"65d4985d4e358ce02a949f8c","avatarUrl":"/avatars/3eda6f50d17802b1ce94349c89637e3c.svg","fullname":"Ariel Shaulov","name":"shaulov","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.04208","authors":[{"_id":"69881f9dbeecc443208d245f","user":{"_id":"675288db99b478caa10a95e7","avatarUrl":"/avatars/e3c63a61b324a21b1853e1def2915910.svg","isPro":false,"fullname":"Hyeonbeom Choi","user":"violet-blue","type":"user"},"name":"Hyeonbeom Choi","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:19:31.704Z","hidden":false},{"_id":"69881f9dbeecc443208d2460","user":{"_id":"6384ac60b2906edaf8342ca5","avatarUrl":"/avatars/033d27d1ef43444300b8a5c97447000a.svg","isPro":false,"fullname":"Daechul Ahn","user":"Daechul","type":"user"},"name":"Daechul Ahn","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:30:40.330Z","hidden":false},{"_id":"69881f9dbeecc443208d2461","name":"Youhan Lee","hidden":false},{"_id":"69881f9dbeecc443208d2462","name":"Taewook Kang","hidden":false},{"_id":"69881f9dbeecc443208d2463","name":"Seongwon Cho","hidden":false},{"_id":"69881f9dbeecc443208d2464","name":"Jonghyun Choi","hidden":false}],"publishedAt":"2026-02-04T04:48:16.000Z","submittedOnDailyAt":"2026-02-11T00:14:40.246Z","title":"SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models","submittedOnDailyBy":{"_id":"6384ac60b2906edaf8342ca5","avatarUrl":"/avatars/033d27d1ef43444300b8a5c97447000a.svg","isPro":false,"fullname":"Daechul Ahn","user":"Daechul","type":"user"},"summary":"Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.","upvotes":19,"discussionId":"69881f9dbeecc443208d2465","projectPage":"https://dcahn12.github.io/projects/scale/","ai_summary":"SCALE is a novel inference strategy for Vision-Language-Action models that jointly modulates visual perception and action based on self-uncertainty, improving robustness without additional training or multiple forward passes.","ai_keywords":["Vision-Language-Action models","test-time scaling","active inference","self-uncertainty","visual perception","action decoding","perceptual ambiguity","uncertainty-driven exploration","single-pass inference","adaptive execution"],"organization":{"_id":"66d54dc8033492801db2bf5a","name":"SeoulNatlUniv","fullname":"Seoul National University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"}},"publishedAt":"2026-02-03T23:48:16.000Z","title":"SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models","summary":"Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic control, with test-time scaling (TTS) gaining attention to enhance robustness beyond training. However, existing TTS methods for VLAs require additional training, verifiers, and multiple forward passes, making them impractical for deployment. Moreover, they intervene only at action decoding while keeping visual representations fixed-insufficient under perceptual ambiguity, where reconsidering how to perceive is as important as deciding what to do. To address these limitations, we propose SCALE, a simple inference strategy that jointly modulates visual perception and action based on 'self-uncertainty', inspired by uncertainty-driven exploration in Active Inference theory-requiring no additional training, no verifier, and only a single forward pass. SCALE broadens exploration in both perception and action under high uncertainty, while focusing on exploitation when confident-enabling adaptive execution across varying conditions. Experiments on simulated and real-world benchmarks demonstrate that SCALE improves state-of-the-art VLAs and outperforms existing TTS methods while maintaining single-pass efficiency.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04208.png","numComments":2,"submittedBy":{"_id":"6384ac60b2906edaf8342ca5","avatarUrl":"/avatars/033d27d1ef43444300b8a5c97447000a.svg","fullname":"Daechul Ahn","name":"Daechul","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66d54dc8033492801db2bf5a","name":"SeoulNatlUniv","fullname":"Seoul National University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/659ccc9d18897eb6594e897f/_-0BM-1UyM-d-lRiahFnf.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.07022","authors":[{"_id":"698c2ee36052d3bed9630c8e","user":{"_id":"636f37fa93d9a0c987e092fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/636f37fa93d9a0c987e092fa/vdZgFPobSIUbBTC3jlfH5.jpeg","isPro":false,"fullname":"Yucheng Zhou","user":"YCZhou","type":"user"},"name":"Yucheng Zhou","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:13:43.761Z","hidden":false},{"_id":"698c2ee36052d3bed9630c8f","name":"Hao Li","hidden":false},{"_id":"698c2ee36052d3bed9630c90","name":"Jianbing Shen","hidden":false}],"publishedAt":"2026-02-02T07:48:04.000Z","submittedOnDailyAt":"2026-02-11T04:59:56.431Z","title":"Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss","submittedOnDailyBy":{"_id":"636f37fa93d9a0c987e092fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/636f37fa93d9a0c987e092fa/vdZgFPobSIUbBTC3jlfH5.jpeg","isPro":false,"fullname":"Yucheng Zhou","user":"YCZhou","type":"user"},"summary":"Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.","upvotes":19,"discussionId":"698c2ee46052d3bed9630c91","ai_summary":"Autoregressive models with diffusion loss outperform traditional diffusion models by effectively mitigating condition errors through patch denoising optimization and condition refinement using Optimal Transport theory.","ai_keywords":["autoregressive models","diffusion models","diffusion loss","patch denoising optimization","condition error","condition distribution","Optimal Transport (OT) theory","Wasserstein Gradient Flow","condition refinement","conditional diffusion"],"organization":{"_id":"66bf160955a1210c53a8dc1c","name":"JordanLee7677888","fullname":"University of Macau","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66bf1530a03b764ca9f80a54/WFe8vJJBZv9w3TFbIW7S3.png"}},"publishedAt":"2026-02-02T02:48:04.000Z","title":"Condition Errors Refinement in Autoregressive Image Generation with Diffusion Loss","summary":"Recent studies have explored autoregressive models for image generation, with promising results, and have combined diffusion models with autoregressive frameworks to optimize image generation via diffusion losses. In this study, we present a theoretical analysis of diffusion and autoregressive models with diffusion loss, highlighting the latter's advantages. We present a theoretical comparison of conditional diffusion and autoregressive diffusion with diffusion loss, demonstrating that patch denoising optimization in autoregressive models effectively mitigates condition errors and leads to a stable condition distribution. Our analysis also reveals that autoregressive condition generation refines the condition, causing the condition error influence to decay exponentially. In addition, we introduce a novel condition refinement approach based on Optimal Transport (OT) theory to address ``condition inconsistency''. We theoretically demonstrate that formulating condition refinement as a Wasserstein Gradient Flow ensures convergence toward the ideal condition distribution, effectively mitigating condition inconsistency. Experiments demonstrate the superiority of our method over diffusion and autoregressive models with diffusion loss methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07022.png","numComments":2,"submittedBy":{"_id":"636f37fa93d9a0c987e092fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/636f37fa93d9a0c987e092fa/vdZgFPobSIUbBTC3jlfH5.jpeg","fullname":"Yucheng Zhou","name":"YCZhou","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"66bf160955a1210c53a8dc1c","name":"JordanLee7677888","fullname":"University of Macau","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66bf1530a03b764ca9f80a54/WFe8vJJBZv9w3TFbIW7S3.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.10098","authors":[{"_id":"698bf4886052d3bed9630a8b","name":"Jingwen Sun","hidden":false},{"_id":"698bf4886052d3bed9630a8c","name":"Wenyao Zhang","hidden":false},{"_id":"698bf4886052d3bed9630a8d","name":"Zekun Qi","hidden":false},{"_id":"698bf4886052d3bed9630a8e","name":"Shaojie Ren","hidden":false},{"_id":"698bf4886052d3bed9630a8f","name":"Zezhi Liu","hidden":false},{"_id":"698bf4886052d3bed9630a90","name":"Hanxin Zhu","hidden":false},{"_id":"698bf4886052d3bed9630a91","name":"Guangzhong Sun","hidden":false},{"_id":"698bf4886052d3bed9630a92","name":"Xin Jin","hidden":false},{"_id":"698bf4886052d3bed9630a93","name":"Zhibo Chen","hidden":false}],"publishedAt":"2026-02-10T18:58:01.000Z","submittedOnDailyAt":"2026-02-11T01:04:16.621Z","title":"VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.","upvotes":18,"discussionId":"698bf4896052d3bed9630a94","projectPage":"https://ginwind.github.io/VLA-JEPA/","githubRepo":"https://github.com/ginwind/VLA-JEPA","githubRepoAddedBy":"user","ai_summary":"VLA-JEPA is a JEPA-style pretraining framework that improves vision-language-action policy learning by using leakage-free state prediction in latent space, enhancing generalization and robustness in manipulation tasks.","ai_keywords":["Vision-Language-Action","JEPA","latent-action objectives","pixel variation","action-relevant state transitions","appearance bias","nuisance motion","information leakage","target encoder","student pathway","latent representations","future frames","current observation","latent space","dynamics abstractions","camera motion","background changes","JEPA pretraining","action-head fine-tuning","generalization","robustness"],"githubStars":51},"publishedAt":"2026-02-10T13:58:01.000Z","title":"VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model","summary":"Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is leakage-free state prediction: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10098.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.00462","authors":[{"_id":"698b928c6052d3bed963087f","user":{"_id":"6270c58780d5f35f8dbe42be","avatarUrl":"/avatars/d4d6e5eadfe9b1f47bce1c66728b24fc.svg","isPro":true,"fullname":"Benno Krojer","user":"BennoKrojer","type":"user"},"name":"Benno Krojer","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:40.382Z","hidden":false},{"_id":"698b928c6052d3bed9630880","name":"Shravan Nayak","hidden":false},{"_id":"698b928c6052d3bed9630881","name":"Oscar Maas","hidden":false},{"_id":"698b928c6052d3bed9630882","name":"Vaibhav Adlakha","hidden":false},{"_id":"698b928c6052d3bed9630883","name":"Desmond Elliott","hidden":false},{"_id":"698b928c6052d3bed9630884","name":"Siva Reddy","hidden":false},{"_id":"698b928c6052d3bed9630885","name":"Marius Mosbach","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6270c58780d5f35f8dbe42be/a4l8v5GTpZ83wLjtr1HAl.png"],"publishedAt":"2026-01-31T02:33:07.000Z","submittedOnDailyAt":"2026-02-11T11:32:25.809Z","title":"LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs","submittedOnDailyBy":{"_id":"6270c58780d5f35f8dbe42be","avatarUrl":"/avatars/d4d6e5eadfe9b1f47bce1c66728b24fc.svg","isPro":true,"fullname":"Benno Krojer","user":"BennoKrojer","type":"user"},"summary":"Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.","upvotes":18,"discussionId":"698b928c6052d3bed9630886","githubRepo":"https://github.com/McGill-NLP/latentlens","githubRepoAddedBy":"user","ai_summary":"LatentLens enables interpretation of visual token representations in vision-language models by comparing them to contextualized textual representations, revealing that visual tokens are more interpretable than previously thought.","ai_keywords":["Vision-Language Model","visual tokens","embedding space","LLM","MLP transformation","interpretability methods","latent representations","contextualized token representations","nearest neighbor representations"],"githubStars":20,"organization":{"_id":"618cd1bfb8de35a67a79d266","name":"McGill-NLP","fullname":"McGill NLP Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1651301909677-5fa9ff3ea13e063b8b2b60cb.png"}},"publishedAt":"2026-01-30T21:33:07.000Z","title":"LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs","summary":"Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6270c58780d5f35f8dbe42be/a4l8v5GTpZ83wLjtr1HAl.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00462.png","numComments":2,"submittedBy":{"_id":"6270c58780d5f35f8dbe42be","avatarUrl":"/avatars/d4d6e5eadfe9b1f47bce1c66728b24fc.svg","fullname":"Benno Krojer","name":"BennoKrojer","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"618cd1bfb8de35a67a79d266","name":"McGill-NLP","fullname":"McGill NLP Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1651301909677-5fa9ff3ea13e063b8b2b60cb.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09849","authors":[{"_id":"698bf64c6052d3bed9630ab2","name":"Yucheng Hu","hidden":false},{"_id":"698bf64c6052d3bed9630ab3","name":"Jianke Zhang","hidden":false},{"_id":"698bf64c6052d3bed9630ab4","name":"Yuanfei Luo","hidden":false},{"_id":"698bf64c6052d3bed9630ab5","name":"Yanjiang Guo","hidden":false},{"_id":"698bf64c6052d3bed9630ab6","name":"Xiaoyu Chen","hidden":false},{"_id":"698bf64c6052d3bed9630ab7","name":"Xinshu Sun","hidden":false},{"_id":"698bf64c6052d3bed9630ab8","name":"Kun Feng","hidden":false},{"_id":"698bf64c6052d3bed9630ab9","name":"Qingzhou Lu","hidden":false},{"_id":"698bf64c6052d3bed9630aba","name":"Sheng Chen","hidden":false},{"_id":"698bf64c6052d3bed9630abb","name":"Yangang Zhang","hidden":false},{"_id":"698bf64c6052d3bed9630abc","name":"Wei Li","hidden":false},{"_id":"698bf64c6052d3bed9630abd","name":"Jianyu Chen","hidden":false}],"publishedAt":"2026-02-10T14:54:01.000Z","submittedOnDailyAt":"2026-02-11T01:08:02.876Z","title":"BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation","submittedOnDailyBy":{"_id":"63044e025c70c21d0eaf08bc","avatarUrl":"/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg","isPro":false,"fullname":"Wei Li","user":"Wiley085","type":"user"},"summary":"Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.","upvotes":16,"discussionId":"698bf64c6052d3bed9630abe","projectPage":"https://cladernyjorn.github.io/BagelVLA.github.io/","ai_summary":"BagelVLA is a unified Vision-Language-Action model that integrates linguistic planning, visual forecasting, and action generation through residual flow guidance for improved manipulation tasks.","ai_keywords":["Vision-Language-Action models","linguistic planning","visual forecasting","action generation","pretrained unified understanding","residual flow guidance","denoising","multi-stage reasoning"]},"publishedAt":"2026-02-10T09:54:01.000Z","title":"BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation","summary":"Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09849.png","numComments":2,"submittedBy":{"_id":"63044e025c70c21d0eaf08bc","avatarUrl":"/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg","fullname":"Wei Li","name":"Wiley085","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.09000","authors":[{"_id":"698c7b50eb12ea74539168dd","name":"Ali Hatamizadeh","hidden":false},{"_id":"698c7b50eb12ea74539168de","name":"Shrimai Prabhumoye","hidden":false},{"_id":"698c7b50eb12ea74539168df","name":"Igor Gitman","hidden":false},{"_id":"698c7b50eb12ea74539168e0","name":"Ximing Lu","hidden":false},{"_id":"698c7b50eb12ea74539168e1","name":"Seungju Han","hidden":false},{"_id":"698c7b50eb12ea74539168e2","name":"Wei Ping","hidden":false},{"_id":"698c7b50eb12ea74539168e3","name":"Yejin Choi","hidden":false},{"_id":"698c7b50eb12ea74539168e4","name":"Jan Kautz","hidden":false}],"publishedAt":"2026-02-09T18:45:11.000Z","submittedOnDailyAt":"2026-02-11T14:22:03.863Z","title":"iGRPO: Self-Feedback-Driven LLM Reasoning","submittedOnDailyBy":{"_id":"64414b62603214724ebd2636","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg","isPro":false,"fullname":"Ali","user":"ahatamiz","type":"user"},"summary":"Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.","upvotes":15,"discussionId":"698c7b50eb12ea74539168e5","ai_summary":"Iterative Group Relative Policy Optimization enhances mathematical reasoning in large language models through a two-stage process combining exploratory drafting and refined iterations, achieving state-of-the-art results on competitive benchmarks.","ai_keywords":["Reinforcement Learning","Proximal Policy Optimization","Group Relative Policy Optimization","iterative policy optimization","mathematical reasoning","reward normalization","exploratory drafting","draft-conditioned refinements","entropy collapse"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-09T13:45:11.000Z","title":"iGRPO: Self-Feedback-Driven LLM Reasoning","summary":"Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09000.png","numComments":2,"submittedBy":{"_id":"64414b62603214724ebd2636","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg","fullname":"Ali","name":"ahatamiz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.01244","authors":[{"_id":"698c7937eb12ea7453916853","name":"Siwei Wu","hidden":false},{"_id":"698c7937eb12ea7453916854","name":"Yizhi Li","hidden":false},{"_id":"698c7937eb12ea7453916855","name":"Yuyang Song","hidden":false},{"_id":"698c7937eb12ea7453916856","name":"Wei Zhang","hidden":false},{"_id":"698c7937eb12ea7453916857","name":"Yang Wang","hidden":false},{"_id":"698c7937eb12ea7453916858","name":"Riza Batista-Navarro","hidden":false},{"_id":"698c7937eb12ea7453916859","name":"Xian Yang","hidden":false},{"_id":"698c7937eb12ea745391685a","name":"Mingjie Tang","hidden":false},{"_id":"698c7937eb12ea745391685b","name":"Bryan Dai","hidden":false},{"_id":"698c7937eb12ea745391685c","name":"Jian Yang","hidden":false},{"_id":"698c7937eb12ea745391685d","name":"Chenghua Lin","hidden":false}],"publishedAt":"2026-02-01T14:09:23.000Z","submittedOnDailyAt":"2026-02-11T11:05:49.131Z","title":"Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments","submittedOnDailyBy":{"_id":"656d97b10bbc114fe64a96c5","avatarUrl":"/avatars/fd23bae1d85c5b96c42064a5ddcfad41.svg","isPro":false,"fullname":"SiweiWu","user":"SiweiWu","type":"user"},"summary":"Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\emph{Executability}, since each instance requires a suitable and often distinct Docker environment; and \\emph{Verifiability}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.","upvotes":15,"discussionId":"698c7937eb12ea745391685e","githubRepo":"https://github.com/multimodal-art-projection/TerminalTraj","githubRepoAddedBy":"user","ai_summary":"A scalable pipeline called TerminalTraj addresses challenges in creating high-quality terminal trajectories for training agentic models by filtering repositories, generating Docker-aligned task instances, and synthesizing executable agent trajectories across multiple domains.","ai_keywords":["terminal trajectories","Docker environments","executability","verifiability","TerminalBench","Qwen2.5-Coder","terminal-based tasks","agent trajectories","Docker-aligned task instances"],"githubStars":34},"publishedAt":"2026-02-01T09:09:23.000Z","title":"Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments","summary":"Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \\emph{Executability}, since each instance requires a suitable and often distinct Docker environment; and \\emph{Verifiability}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose TerminalTraj, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\\% on TB~1.0 and 10\\% on TB~2.0 over their respective backbones. Notably, TerminalTraj-32B achieves strong performance among models with fewer than 100B parameters, reaching 35.30\\% on TB~1.0 and 22.00\\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.01244.png","numComments":2,"submittedBy":{"_id":"656d97b10bbc114fe64a96c5","avatarUrl":"/avatars/fd23bae1d85c5b96c42064a5ddcfad41.svg","fullname":"SiweiWu","name":"SiweiWu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":27,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.10102","authors":[{"_id":"698bf2046052d3bed9630a4d","name":"Zhongwei Ren","hidden":false},{"_id":"698bf2046052d3bed9630a4e","name":"Yunchao Wei","hidden":false},{"_id":"698bf2046052d3bed9630a4f","name":"Xiao Yu","hidden":false},{"_id":"698bf2046052d3bed9630a50","name":"Guixun Luo","hidden":false},{"_id":"698bf2046052d3bed9630a51","name":"Yao Zhao","hidden":false},{"_id":"698bf2046052d3bed9630a52","name":"Bingyi Kang","hidden":false},{"_id":"698bf2046052d3bed9630a53","name":"Jiashi Feng","hidden":false},{"_id":"698bf2046052d3bed9630a54","name":"Xiaojie Jin","hidden":false}],"publishedAt":"2026-02-10T18:58:19.000Z","submittedOnDailyAt":"2026-02-11T00:41:09.836Z","title":"VideoWorld 2: Learning Transferable Knowledge from Real-world Videos","submittedOnDailyBy":{"_id":"64e1cabf12a5504dda7e4948","avatarUrl":"/avatars/53851eddb4e1cae773f3e3607181094b.svg","isPro":true,"fullname":"rzw","user":"maverickrzw","type":"user"},"summary":"Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.","upvotes":14,"discussionId":"698bf2046052d3bed9630a55","projectPage":"https://maverickren.github.io/VideoWorld2.github.io/","githubRepo":"https://github.com/ByteDance-Seed/VideoWorld","githubRepoAddedBy":"user","ai_summary":"VideoWorld 2 enables transferable knowledge learning from raw videos through a dynamic-enhanced Latent Dynamics Model that decouples action dynamics from visual appearance, achieving improved task performance and long-horizon reasoning in real-world applications.","ai_keywords":["latent dynamics model","video diffusion model","action dynamics","visual appearance","latent codes","autoregressive modeling","task policies","long-horizon reasoning","Open-X dataset","CALVIN"],"githubStars":703,"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-02-10T13:58:19.000Z","title":"VideoWorld 2: Learning Transferable Knowledge from Real-world Videos","summary":"Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10102.png","numComments":2,"submittedBy":{"_id":"64e1cabf12a5504dda7e4948","avatarUrl":"/avatars/53851eddb4e1cae773f3e3607181094b.svg","fullname":"rzw","name":"maverickrzw","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.09439","authors":[{"_id":"698c12636052d3bed9630bec","name":"Xu Ma","hidden":false},{"_id":"698c12636052d3bed9630bed","name":"Yitian Zhang","hidden":false},{"_id":"698c12636052d3bed9630bee","name":"Qihua Dong","hidden":false},{"_id":"698c12636052d3bed9630bef","name":"Yun Fu","hidden":false}],"publishedAt":"2026-02-10T06:06:54.000Z","submittedOnDailyAt":"2026-02-11T02:56:41.984Z","title":"Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning","submittedOnDailyBy":{"_id":"62434e02ffb7778797651d50","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1654982689149-62434e02ffb7778797651d50.jpeg","isPro":true,"fullname":"Xu Ma","user":"ma-xu","type":"user"},"summary":"High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.","upvotes":13,"discussionId":"698c12636052d3bed9630bf0","projectPage":"https://huggingface.co/spaces/ma-xu/fine-t2i-explore","ai_summary":"A large-scale, high-quality, and fully open dataset for text-to-image fine-tuning is presented, featuring over 6 million text-image pairs with rigorous filtering for alignment and quality across multiple task combinations and visual styles.","ai_keywords":["text-to-image","fine-tuning","diffusion models","autoregressive models","text-image alignment","visual fidelity","prompt quality","dataset scaling"],"organization":{"_id":"68aa797c53b0c4b2e53dc742","name":"Northeastern","fullname":"Northeastern University ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68aa7916a2c0dfa90ecea840/rMbsVWRw5ypjq_Gls_X1O.png"}},"publishedAt":"2026-02-10T01:06:54.000Z","title":"Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning","summary":"High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09439.png","numComments":2,"submittedBy":{"_id":"62434e02ffb7778797651d50","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1654982689149-62434e02ffb7778797651d50.jpeg","fullname":"Xu Ma","name":"ma-xu","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"68aa797c53b0c4b2e53dc742","name":"Northeastern","fullname":"Northeastern University ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68aa7916a2c0dfa90ecea840/rMbsVWRw5ypjq_Gls_X1O.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06820","authors":[{"_id":"69894bebbeecc443208d25cc","name":"Dunwei Tu","hidden":false},{"_id":"69894bebbeecc443208d25cd","name":"Hongyan Hao","hidden":false},{"_id":"69894bebbeecc443208d25ce","user":{"_id":"65ced159d82f8d722c78e0cf","avatarUrl":"/avatars/a7121348e4fe77055533f14cda7f90b8.svg","isPro":false,"fullname":"Hansi Yang","user":"animawang","type":"user"},"name":"Hansi Yang","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:29:52.498Z","hidden":false},{"_id":"69894bebbeecc443208d25cf","name":"Yihao Chen","hidden":false},{"_id":"69894bebbeecc443208d25d0","name":"Yi-Kai Zhang","hidden":false},{"_id":"69894bebbeecc443208d25d1","name":"Zhikang Xia","hidden":false},{"_id":"69894bebbeecc443208d25d2","name":"Yu Yang","hidden":false},{"_id":"69894bebbeecc443208d25d3","name":"Yueqing Sun","hidden":false},{"_id":"69894bebbeecc443208d25d4","name":"Xingchen Liu","hidden":false},{"_id":"69894bebbeecc443208d25d5","name":"Furao Shen","hidden":false},{"_id":"69894bebbeecc443208d25d6","name":"Qi Gu","hidden":false},{"_id":"69894bebbeecc443208d25d7","name":"Hui Su","hidden":false},{"_id":"69894bebbeecc443208d25d8","name":"Xunliang Cai","hidden":false}],"publishedAt":"2026-02-06T16:05:55.000Z","submittedOnDailyAt":"2026-02-11T04:01:10.656Z","title":"ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training","submittedOnDailyBy":{"_id":"65ced159d82f8d722c78e0cf","avatarUrl":"/avatars/a7121348e4fe77055533f14cda7f90b8.svg","isPro":false,"fullname":"Hansi Yang","user":"animawang","type":"user"},"summary":"Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as ^2-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.","upvotes":13,"discussionId":"69894bebbeecc443208d25d9","ai_summary":"ScaleEnv framework generates interactive environments from scratch to improve agent generalization through diverse domain scaling and verified task completion.","ai_keywords":["generalist agents","interactive environments","self-exploration","procedural testing","tool dependency graph expansion","executable action verification","multi-turn tool-use benchmarks","-Bench","VitaBench","model generalization"],"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}},"publishedAt":"2026-02-06T11:05:55.000Z","title":"ScaleEnv: Scaling Environment Synthesis from Scratch for Generalist Interactive Tool-Use Agent Training","summary":"Training generalist agents capable of adapting to diverse scenarios requires interactive environments for self-exploration. However, interactive environments remain critically scarce, and existing synthesis methods suffer from significant limitations regarding environmental diversity and scalability. To address these challenges, we introduce ScaleEnv, a framework that constructs fully interactive environments and verifiable tasks entirely from scratch. Specifically, ScaleEnv ensures environment reliability through procedural testing, and guarantees task completeness and solvability via tool dependency graph expansion and executable action verification. By enabling agents to learn through exploration within ScaleEnv, we demonstrate significant performance improvements on unseen, multi-turn tool-use benchmarks such as ^2-Bench and VitaBench, highlighting strong generalization capabilities. Furthermore, we investigate the relationship between increasing number of domains and model generalization performance, providing empirical evidence that scaling environmental diversity is critical for robust agent learning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06820.png","numComments":2,"submittedBy":{"_id":"65ced159d82f8d722c78e0cf","avatarUrl":"/avatars/a7121348e4fe77055533f14cda7f90b8.svg","fullname":"Hansi Yang","name":"animawang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"68b28d79a176a9beb30d2049","name":"meituan-longcat","fullname":"LongCat","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09017","authors":[{"_id":"698bb5cf6052d3bed9630903","name":"Zichen Jeff Cui","hidden":false},{"_id":"698bb5cf6052d3bed9630904","name":"Omar Rayyan","hidden":false},{"_id":"698bb5cf6052d3bed9630905","name":"Haritheja Etukuru","hidden":false},{"_id":"698bb5cf6052d3bed9630906","user":{"_id":"698cb7952bc6b6467f2f6365","avatarUrl":"/avatars/b33bd302defdb440f3abd3d25a2c2982.svg","isPro":false,"fullname":"Bowen Tan","user":"bowenT","type":"user"},"name":"Bowen Tan","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:48.701Z","hidden":false},{"_id":"698bb5cf6052d3bed9630907","name":"Zavier Andrianarivo","hidden":false},{"_id":"698bb5cf6052d3bed9630908","name":"Zicheng Teng","hidden":false},{"_id":"698bb5cf6052d3bed9630909","name":"Yihang Zhou","hidden":false},{"_id":"698bb5cf6052d3bed963090a","name":"Krish Mehta","hidden":false},{"_id":"698bb5cf6052d3bed963090b","name":"Nicholas Wojno","hidden":false},{"_id":"698bb5cf6052d3bed963090c","user":{"_id":"64c883c38b1d0044b90e2b20","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/L9D-q2YpvTuk0j6d4OcCC.png","isPro":false,"fullname":"Kevin Wu","user":"kevinywu","type":"user"},"name":"Kevin Yuanbo Wu","status":"claimed_verified","statusLastChangedAt":"2026-02-11T22:16:54.234Z","hidden":false},{"_id":"698bb5cf6052d3bed963090d","name":"Manan H Anjaria","hidden":false},{"_id":"698bb5cf6052d3bed963090e","name":"Ziyuan Wu","hidden":false},{"_id":"698bb5cf6052d3bed963090f","name":"Manrong Mao","hidden":false},{"_id":"698bb5cf6052d3bed9630910","name":"Guangxun Zhang","hidden":false},{"_id":"698bb5cf6052d3bed9630911","name":"Binit Shah","hidden":false},{"_id":"698bb5cf6052d3bed9630912","name":"Yejin Kim","hidden":false},{"_id":"698bb5cf6052d3bed9630913","name":"Soumith Chintala","hidden":false},{"_id":"698bb5cf6052d3bed9630914","name":"Lerrel Pinto","hidden":false},{"_id":"698bb5cf6052d3bed9630915","user":{"_id":"630e567f8df86f1e5bf0d837","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/630e567f8df86f1e5bf0d837/Mje-2ahgmMET2jzqLafhy.jpeg","isPro":false,"fullname":"Mahi Shafiullah","user":"notmahi","type":"user"},"name":"Nur Muhammad Mahi Shafiullah","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:29.812Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/630e567f8df86f1e5bf0d837/VohfT0ukOlS81o1xJ74xt.mp4","https://cdn-uploads.huggingface.co/production/uploads/630e567f8df86f1e5bf0d837/auh6YbxfTv7ZgZDtoP4mu.mp4","https://cdn-uploads.huggingface.co/production/uploads/630e567f8df86f1e5bf0d837/675SyMnFONBi2w6TLQ20d.mp4","https://cdn-uploads.huggingface.co/production/uploads/630e567f8df86f1e5bf0d837/K8Qvu2IMmsaCPGAnQ850T.mp4"],"publishedAt":"2026-02-09T18:58:50.000Z","submittedOnDailyAt":"2026-02-11T12:49:34.599Z","title":"Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models","submittedOnDailyBy":{"_id":"630e567f8df86f1e5bf0d837","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/630e567f8df86f1e5bf0d837/Mje-2ahgmMET2jzqLafhy.jpeg","isPro":false,"fullname":"Mahi Shafiullah","user":"notmahi","type":"user"},"summary":"The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/","upvotes":12,"discussionId":"698bb5d06052d3bed9630916","projectPage":"https://cap-policy.github.io/","githubRepo":"https://github.com/jeffacce/cap-policy","githubRepoAddedBy":"user","ai_summary":"Contact-Anchored Policies replace language conditioning with physical contact points and use modular utility models for robust manipulation, achieving superior zero-shot performance with minimal demonstration data through real-to-sim iteration cycles.","ai_keywords":["Contact-Anchored Policies","language conditioning","physical contact","modular utility models","real-to-sim iteration","EgoGym","manipulation skills","zero-shot evaluation","demonstration data"],"githubStars":12,"organization":{"_id":"691d8e884bbe8df0d99462e2","name":"newyorkuniversity","fullname":"New York University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/orNHmPzOQf2_F5UgXPsu5.png"}},"publishedAt":"2026-02-09T13:58:50.000Z","title":"Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models","summary":"The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with points of physical contact in space. Simultaneously, we structure CAP as a library of modular utility models rather than a monolithic generalist policy. This factorization allows us to implement a real-to-sim iteration cycle: we build EgoGym, a lightweight simulation benchmark, to rapidly identify failure modes and refine our models and datasets prior to real-world deployment. We show that by conditioning on contact and iterating via simulation, CAP generalizes to novel environments and embodiments out of the box on three fundamental manipulation skills while using only 23 hours of demonstration data, and outperforms large, state-of-the-art VLAs in zero-shot evaluations by 56%. All model checkpoints, codebase, hardware, simulation, and datasets will be open-sourced. Project page: https://cap-policy.github.io/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/630e567f8df86f1e5bf0d837/VohfT0ukOlS81o1xJ74xt.mp4","https://cdn-uploads.huggingface.co/production/uploads/630e567f8df86f1e5bf0d837/auh6YbxfTv7ZgZDtoP4mu.mp4","https://cdn-uploads.huggingface.co/production/uploads/630e567f8df86f1e5bf0d837/675SyMnFONBi2w6TLQ20d.mp4","https://cdn-uploads.huggingface.co/production/uploads/630e567f8df86f1e5bf0d837/K8Qvu2IMmsaCPGAnQ850T.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09017.png","numComments":2,"submittedBy":{"_id":"630e567f8df86f1e5bf0d837","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/630e567f8df86f1e5bf0d837/Mje-2ahgmMET2jzqLafhy.jpeg","fullname":"Mahi Shafiullah","name":"notmahi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"organization":{"_id":"691d8e884bbe8df0d99462e2","name":"newyorkuniversity","fullname":"New York University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/orNHmPzOQf2_F5UgXPsu5.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08025","authors":[{"_id":"698c7597eb12ea7453916837","user":{"_id":"68fc3ddcdc9e5cbf49cbc716","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68fc3ddcdc9e5cbf49cbc716/gzvksq-XgWnekB6Xl25pw.jpeg","isPro":false,"fullname":"EasonYe","user":"EasonUwU","type":"user"},"name":"Yixuan Ye","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:38.731Z","hidden":false},{"_id":"698c7597eb12ea7453916838","name":"Xuanyu Lu","hidden":false},{"_id":"698c7597eb12ea7453916839","name":"Yuxin Jiang","hidden":false},{"_id":"698c7597eb12ea745391683a","name":"Yuchao Gu","hidden":false},{"_id":"698c7597eb12ea745391683b","name":"Rui Zhao","hidden":false},{"_id":"698c7597eb12ea745391683c","name":"Qiwei Liang","hidden":false},{"_id":"698c7597eb12ea745391683d","name":"Jiachun Pan","hidden":false},{"_id":"698c7597eb12ea745391683e","name":"Fengda Zhang","hidden":false},{"_id":"698c7597eb12ea745391683f","name":"Weijia Wu","hidden":false},{"_id":"698c7597eb12ea7453916840","name":"Alex Jinpeng Wang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6345a93afe134dfd7a0cfabd/bwNXUxQqslgNNPGQj_FzL.mp4"],"publishedAt":"2026-02-08T15:57:23.000Z","submittedOnDailyAt":"2026-02-11T23:27:34.924Z","title":"MIND: Benchmarking Memory Consistency and Action Control in World Models","submittedOnDailyBy":{"_id":"6345a93afe134dfd7a0cfabd","avatarUrl":"/avatars/65130ce06b1c72ab1066678419731d88.svg","isPro":false,"fullname":"wu weijia","user":"weijiawu","type":"user"},"summary":"World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/","upvotes":12,"discussionId":"698c7597eb12ea7453916841","projectPage":"https://csu-jpg.github.io/MIND.github.io/","githubRepo":"https://github.com/CSU-JPG/MIND","githubRepoAddedBy":"user","ai_summary":"MIND is introduced as the first open-domain closed-loop benchmark for evaluating memory consistency and action control in world models, featuring high-quality videos and diverse action spaces to assess temporal stability and contextual coherence.","ai_keywords":["world models","memory consistency","action control","closed-loop benchmark","interactive Video-to-World baseline","temporal stability","contextual coherence","action generalization"],"githubStars":36,"organization":{"_id":"63f63303b29015adc33aeaa8","name":"NUS-CS3213","fullname":"National University of Singapore"}},"publishedAt":"2026-02-08T10:57:23.000Z","title":"MIND: Benchmarking Memory Consistency and Action Control in World Models","summary":"World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6345a93afe134dfd7a0cfabd/bwNXUxQqslgNNPGQj_FzL.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08025.png","numComments":2,"submittedBy":{"_id":"6345a93afe134dfd7a0cfabd","avatarUrl":"/avatars/65130ce06b1c72ab1066678419731d88.svg","fullname":"wu weijia","name":"weijiawu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"63f63303b29015adc33aeaa8","name":"NUS-CS3213","fullname":"National University of Singapore"},"isAuthorParticipating":false},{"paper":{"id":"2602.09276","authors":[{"_id":"698c97f66c5152984e4f3c7f","name":"Archiki Prasad","hidden":false},{"_id":"698c97f66c5152984e4f3c80","name":"Mandar Joshi","hidden":false},{"_id":"698c97f66c5152984e4f3c81","name":"Kenton Lee","hidden":false},{"_id":"698c97f66c5152984e4f3c82","name":"Mohit Bansal","hidden":false},{"_id":"698c97f66c5152984e4f3c83","name":"Peter Shaw","hidden":false}],"publishedAt":"2026-02-09T23:32:12.000Z","submittedOnDailyAt":"2026-02-11T12:28:49.902Z","title":"Effective Reasoning Chains Reduce Intrinsic Dimensionality","submittedOnDailyBy":{"_id":"607aeae5d2cd8c150e6ae074","avatarUrl":"/avatars/a087743b98b6fe2181283a9610db4ec4.svg","isPro":false,"fullname":"Archiki Prasad","user":"archiki","type":"user"},"summary":"Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.","upvotes":11,"discussionId":"698c97f66c5152984e4f3c84","ai_summary":"Effective chain-of-thought reasoning strategies reduce intrinsic dimensionality, leading to better generalization by requiring fewer model parameters to achieve given accuracy thresholds.","ai_keywords":["chain-of-thought","language models","reasoning strategies","intrinsic dimensionality","generalization","task formulation","model architecture","GSM8K","Gemma-3"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-09T18:32:12.000Z","title":"Effective Reasoning Chains Reduce Intrinsic Dimensionality","summary":"Chain-of-thought (CoT) reasoning and its variants have substantially improved the performance of language models on complex reasoning tasks, yet the precise mechanisms by which different strategies facilitate generalization remain poorly understood. While current explanations often point to increased test-time computation or structural guidance, establishing a consistent, quantifiable link between these factors and generalization remains challenging. In this work, we identify intrinsic dimensionality as a quantitative measure for characterizing the effectiveness of reasoning chains. Intrinsic dimensionality quantifies the minimum number of model dimensions needed to reach a given accuracy threshold on a given task. By keeping the model architecture fixed and varying the task formulation through different reasoning strategies, we demonstrate that effective reasoning strategies consistently reduce the intrinsic dimensionality of the task. Validating this on GSM8K with Gemma-3 1B and 4B, we observe a strong inverse correlation between the intrinsic dimensionality of a reasoning strategy and its generalization performance on both in-distribution and out-of-distribution data. Our findings suggest that effective reasoning chains facilitate learning by better compressing the task using fewer parameters, offering a new quantitative metric for analyzing reasoning processes.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09276.png","numComments":2,"submittedBy":{"_id":"607aeae5d2cd8c150e6ae074","avatarUrl":"/avatars/a087743b98b6fe2181283a9610db4ec4.svg","fullname":"Archiki Prasad","name":"archiki","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08382","authors":[{"_id":"698c01df6052d3bed9630b55","name":"Zhuoen Chen","hidden":false},{"_id":"698c01df6052d3bed9630b56","name":"Dongfang Li","hidden":false},{"_id":"698c01df6052d3bed9630b57","name":"Meishan Zhang","hidden":false},{"_id":"698c01df6052d3bed9630b58","name":"Baotian Hu","hidden":false},{"_id":"698c01df6052d3bed9630b59","name":"Min Zhang","hidden":false}],"publishedAt":"2026-02-09T08:33:11.000Z","submittedOnDailyAt":"2026-02-11T05:17:58.176Z","title":"Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning","submittedOnDailyBy":{"_id":"62e38a3ef3b208e2aecf2c84","avatarUrl":"/avatars/eee9d7d53f3f9b7a18a21d289abd3c64.svg","isPro":false,"fullname":"Dongfang Li","user":"crazyofapple","type":"user"},"summary":"Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.","upvotes":10,"discussionId":"698c01df6052d3bed9630b5a","ai_summary":"A cognitive-inspired framework for long-context language modeling uses chunk-wise compression and selective memory recall to improve efficiency and performance over traditional approaches.","ai_keywords":["large language models","long-context processing","retrieval-augmented generation","chunk-wise compression","selective memory recall","learned compressor","gating module","reasoning module","working memory","end-to-end reinforcement learning","multi-hop reasoning","RULER-HQA","MemAgent"],"organization":{"_id":"629867d7f2bf8bd3e468706e","name":"HIT-TMG","fullname":"Lychee Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64b7679a08e2452d18db9a9e/uk4QHGGYqcrEHmiweTFWy.png"}},"publishedAt":"2026-02-09T03:33:11.000Z","title":"Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning","summary":"Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08382.png","numComments":2,"submittedBy":{"_id":"62e38a3ef3b208e2aecf2c84","avatarUrl":"/avatars/eee9d7d53f3f9b7a18a21d289abd3c64.svg","fullname":"Dongfang Li","name":"crazyofapple","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"629867d7f2bf8bd3e468706e","name":"HIT-TMG","fullname":"Lychee Team","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64b7679a08e2452d18db9a9e/uk4QHGGYqcrEHmiweTFWy.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.07276","authors":[{"_id":"698adc4e1b2dc6b37d61b29f","user":{"_id":"664263ea8b2d38e53f04079c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/664263ea8b2d38e53f04079c/_7og0ggbPpEHcH6g__DF0.jpeg","isPro":false,"fullname":"Pengrui Han","user":"barryhpr","type":"user"},"name":"Pengrui Han","status":"claimed_verified","statusLastChangedAt":"2026-02-11T22:17:01.630Z","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a0","user":{"_id":"66a3f1c4c38ce500371fd8d4","avatarUrl":"/avatars/381de938091f1a5c179eef72aa247bbf.svg","isPro":false,"fullname":"Xueqiang Xu","user":"XueqiangXu","type":"user"},"name":"Xueqiang Xu","status":"claimed_verified","statusLastChangedAt":"2026-02-11T22:16:59.753Z","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a1","user":{"_id":"687ebb87416fcff56959d817","avatarUrl":"/avatars/8f13090dd6179bb18e8c8d205fd20131.svg","isPro":true,"fullname":"Keyang Xuan","user":"keyangx3","type":"user"},"name":"Keyang Xuan","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:19:22.360Z","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a2","user":{"_id":"649c5cf5c1ae48cf4d7dda34","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg","isPro":false,"fullname":"Peiyang Song","user":"p-song1","type":"user"},"name":"Peiyang Song","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:18:00.362Z","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a3","name":"Siru Ouyang","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a4","name":"Runchu Tian","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a5","name":"Yuqing Jiang","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a6","name":"Cheng Qian","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a7","name":"Pengcheng Jiang","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a8","name":"Jiashuo Sun","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2a9","name":"Junxia Cui","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2aa","name":"Ming Zhong","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2ab","name":"Ge Liu","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2ac","name":"Jiawei Han","hidden":false},{"_id":"698adc4e1b2dc6b37d61b2ad","name":"Jiaxuan You","hidden":false}],"publishedAt":"2026-02-07T00:00:50.000Z","submittedOnDailyAt":"2026-02-11T05:30:55.286Z","title":"Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs","submittedOnDailyBy":{"_id":"649c5cf5c1ae48cf4d7dda34","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg","isPro":false,"fullname":"Peiyang Song","user":"p-song1","type":"user"},"summary":"Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.","upvotes":10,"discussionId":"698adc4e1b2dc6b37d61b2ae","ai_summary":"STEER2ADAPT is a lightweight framework that adapts large language models by composing steering vectors from reusable semantic prior subspaces, enabling efficient and flexible task adaptation through linear combinations of basis vectors.","ai_keywords":["activation steering","large language models","steering vectors","semantic prior subspace","linear combination","inference-time adaptation","data efficiency","stability","transparency"],"organization":{"_id":"65448bef5b5d9185ba3202b9","name":"UIUC-CS","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}},"publishedAt":"2026-02-06T19:00:50.000Z","title":"Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs","summary":"Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07276.png","numComments":2,"submittedBy":{"_id":"649c5cf5c1ae48cf4d7dda34","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649c5cf5c1ae48cf4d7dda34/bSJXATqkqBn8ypUy0OezY.jpeg","fullname":"Peiyang Song","name":"p-song1","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"65448bef5b5d9185ba3202b9","name":"UIUC-CS","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.10116","authors":[{"_id":"698bf1aa6052d3bed9630a2d","name":"Hongchi Xia","hidden":false},{"_id":"698bf1aa6052d3bed9630a2e","name":"Xuan Li","hidden":false},{"_id":"698bf1aa6052d3bed9630a2f","name":"Zhaoshuo Li","hidden":false},{"_id":"698bf1aa6052d3bed9630a30","name":"Qianli Ma","hidden":false},{"_id":"698bf1aa6052d3bed9630a31","name":"Jiashu Xu","hidden":false},{"_id":"698bf1aa6052d3bed9630a32","name":"Ming-Yu Liu","hidden":false},{"_id":"698bf1aa6052d3bed9630a33","name":"Yin Cui","hidden":false},{"_id":"698bf1aa6052d3bed9630a34","name":"Tsung-Yi Lin","hidden":false},{"_id":"698bf1aa6052d3bed9630a35","name":"Wei-Chiu Ma","hidden":false},{"_id":"698bf1aa6052d3bed9630a36","name":"Shenlong Wang","hidden":false},{"_id":"698bf1aa6052d3bed9630a37","name":"Shuran Song","hidden":false},{"_id":"698bf1aa6052d3bed9630a38","name":"Fangyin Wei","hidden":false}],"publishedAt":"2026-02-10T18:59:55.000Z","submittedOnDailyAt":"2026-02-11T00:45:34.378Z","title":"SAGE: Scalable Agentic 3D Scene Generation for Embodied AI","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.","upvotes":9,"discussionId":"698bf1ab6052d3bed9630a39","projectPage":"https://nvlabs.github.io/sage","ai_summary":"SAGE is an agentic framework that automatically generates simulation-ready 3D environments for embodied AI by combining layout and object composition generators with evaluative critics for semantic plausibility and physical stability.","ai_keywords":["embodied agents","simulation-ready environments","scene-generation systems","agentic framework","layout generation","object composition","semantic plausibility","visual realism","physical stability","iterative reasoning","adaptive tool selection","policy training","embodied AI"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-10T13:59:55.000Z","title":"SAGE: Scalable Agentic 3D Scene Generation for Embodied AI","summary":"Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., \"pick up a bowl and place it on the table\"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10116.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.09823","authors":[{"_id":"698bf2d76052d3bed9630a6f","name":"Wenfu Wang","hidden":false},{"_id":"698bf2d76052d3bed9630a70","name":"Chenxing Li","hidden":false},{"_id":"698bf2d76052d3bed9630a71","name":"Liqiang Zhang","hidden":false},{"_id":"698bf2d76052d3bed9630a72","name":"Yiyang Zhao","hidden":false},{"_id":"698bf2d76052d3bed9630a73","name":"Yuxiang Zou","hidden":false},{"_id":"698bf2d76052d3bed9630a74","name":"Hanzhao Li","hidden":false},{"_id":"698bf2d76052d3bed9630a75","name":"Mingyu Cui","hidden":false},{"_id":"698bf2d76052d3bed9630a76","name":"Hao Zhang","hidden":false},{"_id":"698bf2d76052d3bed9630a77","name":"Kun Wei","hidden":false},{"_id":"698bf2d76052d3bed9630a78","name":"Le Xu","hidden":false},{"_id":"698bf2d76052d3bed9630a79","name":"Zikang Huang","hidden":false},{"_id":"698bf2d76052d3bed9630a7a","name":"Jiajun Xu","hidden":false},{"_id":"698bf2d76052d3bed9630a7b","name":"Jiliang Hu","hidden":false},{"_id":"698bf2d76052d3bed9630a7c","name":"Xiang He","hidden":false},{"_id":"698bf2d76052d3bed9630a7d","name":"Zeyu Xie","hidden":false},{"_id":"698bf2d76052d3bed9630a7e","name":"Jiawen Kang","hidden":false},{"_id":"698bf2d76052d3bed9630a7f","name":"Youjun Chen","hidden":false},{"_id":"698bf2d76052d3bed9630a80","name":"Meng Yu","hidden":false},{"_id":"698bf2d76052d3bed9630a81","name":"Dong Yu","hidden":false},{"_id":"698bf2d76052d3bed9630a82","name":"Rilin Chen","hidden":false},{"_id":"698bf2d76052d3bed9630a83","name":"Linlin Di","hidden":false},{"_id":"698bf2d76052d3bed9630a84","name":"Shulin Feng","hidden":false},{"_id":"698bf2d76052d3bed9630a85","name":"Na Hu","hidden":false},{"_id":"698bf2d76052d3bed9630a86","name":"Yang Liu","hidden":false},{"_id":"698bf2d76052d3bed9630a87","name":"Bang Wang","hidden":false},{"_id":"698bf2d76052d3bed9630a88","name":"Shan Yang","hidden":false}],"publishedAt":"2026-02-10T14:31:11.000Z","submittedOnDailyAt":"2026-02-11T00:39:19.748Z","title":"Covo-Audio Technical Report","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant, demonstrates strong spoken conversational abilities, including understanding, contextual reasoning, instruction following, and generating contextually appropriate and empathetic responses, validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model, achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALMs for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering, enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning, and suggest a scalable path toward more capable and versatile LALMs.","upvotes":8,"discussionId":"698bf2d76052d3bed9630a89","ai_summary":"Covo-Audio is a 7B-parameter end-to-end large audio language model that processes continuous audio inputs and generates audio outputs, achieving state-of-the-art performance across speech-text modeling, spoken dialogue, and full-duplex voice interaction tasks through large-scale pretraining and post-training techniques.","ai_keywords":["end-to-end LALM","continuous audio inputs","audio outputs","large-scale curated pretraining","targeted post-training","speech-text modeling","spoken dialogue","speech understanding","audio understanding","full-duplex voice interaction","speech-text comprehension","semantic reasoning","dialogue-oriented variant","conversational abilities","contextual reasoning","instruction following","empathetic responses","full-duplex model","intelligent-speaker decoupling strategy","voice rendering","text-to-speech"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-10T09:31:11.000Z","title":"Covo-Audio Technical Report","summary":"In this work, we present Covo-Audio, a 7B-parameter end-to-end LALM that directly processes continuous audio inputs and generates audio outputs within a single unified architecture. Through large-scale curated pretraining and targeted post-training, Covo-Audio achieves state-of-the-art or competitive performance among models of comparable scale across a broad spectrum of tasks, including speech-text modeling, spoken dialogue, speech understanding, audio understanding, and full-duplex voice interaction. Extensive evaluations demonstrate that the pretrained foundation model exhibits strong speech-text comprehension and semantic reasoning capabilities on multiple benchmarks, outperforming representative open-source models of comparable scale. Furthermore, Covo-Audio-Chat, the dialogue-oriented variant, demonstrates strong spoken conversational abilities, including understanding, contextual reasoning, instruction following, and generating contextually appropriate and empathetic responses, validating its applicability to real-world conversational assistant scenarios. Covo-Audio-Chat-FD, the evolved full-duplex model, achieves substantially superior performance on both spoken dialogue capabilities and full-duplex interaction behaviors, demonstrating its competence in practical robustness. To mitigate the high cost of deploying end-to-end LALMs for natural conversational systems, we propose an intelligence-speaker decoupling strategy that separates dialogue intelligence from voice rendering, enabling flexible voice customization with minimal text-to-speech (TTS) data while preserving dialogue performance. Overall, our results highlight the strong potential of 7B-scale models to integrate sophisticated audio intelligence with high-level semantic reasoning, and suggest a scalable path toward more capable and versatile LALMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09823.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.09268","authors":[{"_id":"698c48066052d3bed9630d0f","name":"Nikita Starodubcev","hidden":false},{"_id":"698c48066052d3bed9630d10","name":"Daniil Pakhomov","hidden":false},{"_id":"698c48066052d3bed9630d11","name":"Zongze Wu","hidden":false},{"_id":"698c48066052d3bed9630d12","name":"Ilya Drobyshevskiy","hidden":false},{"_id":"698c48066052d3bed9630d13","name":"Yuchen Liu","hidden":false},{"_id":"698c48066052d3bed9630d14","name":"Zhonghao Wang","hidden":false},{"_id":"698c48066052d3bed9630d15","name":"Yuqian Zhou","hidden":false},{"_id":"698c48066052d3bed9630d16","name":"Zhe Lin","hidden":false},{"_id":"698c48066052d3bed9630d17","name":"Dmitry Baranchuk","hidden":false}],"publishedAt":"2026-02-09T23:06:58.000Z","submittedOnDailyAt":"2026-02-11T06:44:20.608Z","title":"Rethinking Global Text Conditioning in Diffusion Transformers","submittedOnDailyBy":{"_id":"6410d3a4cfbe9c4400233d1e","avatarUrl":"/avatars/9e1c4e0ea5f6964c90013f6e19c41db1.svg","isPro":false,"fullname":"nikita","user":"quickjkee","type":"user"},"summary":"Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.","upvotes":8,"discussionId":"698c48066052d3bed9630d18","githubRepo":"https://github.com/quickjkee/modulation-guidance","githubRepoAddedBy":"user","ai_summary":"Modulation-based text conditioning in diffusion transformers provides performance benefits when used as guidance for controllable generation rather than just as attention mechanisms.","ai_keywords":["diffusion transformers","attention layers","modulation mechanism","pooled text embedding","text-to-image generation","image editing","controllable generation"],"githubStars":19},"publishedAt":"2026-02-09T18:06:58.000Z","title":"Rethinking Global Text Conditioning in Diffusion Transformers","summary":"Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09268.png","numComments":2,"submittedBy":{"_id":"6410d3a4cfbe9c4400233d1e","avatarUrl":"/avatars/9e1c4e0ea5f6964c90013f6e19c41db1.svg","fullname":"nikita","name":"quickjkee","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.09662","authors":[{"_id":"698c455c6052d3bed9630cf8","name":"Deyang Jiang","hidden":false},{"_id":"698c455c6052d3bed9630cf9","name":"Jing Huang","hidden":false},{"_id":"698c455c6052d3bed9630cfa","name":"Xuanle Zhao","hidden":false},{"_id":"698c455c6052d3bed9630cfb","name":"Lei Chen","hidden":false},{"_id":"698c455c6052d3bed9630cfc","name":"Liming Zheng","hidden":false},{"_id":"698c455c6052d3bed9630cfd","name":"Fanfan Liu","hidden":false},{"_id":"698c455c6052d3bed9630cfe","name":"Haibo Qiu","hidden":false},{"_id":"698c455c6052d3bed9630cff","name":"Peng Shi","hidden":false},{"_id":"698c455c6052d3bed9630d00","name":"Zhixiong Zeng","hidden":false}],"publishedAt":"2026-02-10T11:16:57.000Z","submittedOnDailyAt":"2026-02-11T06:33:59.311Z","title":"TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution","submittedOnDailyBy":{"_id":"6572cbc42bb242937c0a1101","avatarUrl":"/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg","isPro":false,"fullname":"Xuanle Zhao","user":"xxxllz","type":"user"},"summary":"Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (i.e., trajectory difficulty) and breadth (i.e., trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.","upvotes":6,"discussionId":"698c455d6052d3bed9630d01","githubRepo":"https://github.com/UITron-hub/TreeCUA","githubRepoAddedBy":"user","ai_summary":"TreeCUA enables efficient GUI automation scaling through tree-structured trajectory organization and multi-agent collaboration, improving GUI planning capabilities via adaptive exploration and trajectory verification.","ai_keywords":["GUI automation","GUI planning","tree-structured trajectories","multi-agent collaborative framework","adaptive exploration algorithm","world knowledge guidance","global memory backtracking","TreeCUA-DPO"],"githubStars":4},"publishedAt":"2026-02-10T06:16:57.000Z","title":"TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution","summary":"Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (i.e., trajectory difficulty) and breadth (i.e., trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09662.png","numComments":2,"submittedBy":{"_id":"6572cbc42bb242937c0a1101","avatarUrl":"/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg","fullname":"Xuanle Zhao","name":"xxxllz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07839","authors":[{"_id":"698b1fc81b2dc6b37d61b42b","name":"Jiaxi Liu","hidden":false},{"_id":"698b1fc81b2dc6b37d61b42c","name":"Yanzuo Jiang","hidden":false},{"_id":"698b1fc81b2dc6b37d61b42d","name":"Guibin Zhang","hidden":false},{"_id":"698b1fc81b2dc6b37d61b42e","name":"Zihan Zhang","hidden":false},{"_id":"698b1fc81b2dc6b37d61b42f","name":"Heng Chang","hidden":false},{"_id":"698b1fc81b2dc6b37d61b430","name":"Zhenfei Yin","hidden":false},{"_id":"698b1fc81b2dc6b37d61b431","name":"Qibing Ren","hidden":false},{"_id":"698b1fc81b2dc6b37d61b432","name":"Junchi Yan","hidden":false}],"publishedAt":"2026-02-08T06:37:01.000Z","submittedOnDailyAt":"2026-02-11T05:25:11.480Z","title":"TodoEvolve: Learning to Architect Agent Planning Systems","submittedOnDailyBy":{"_id":"6363a1fa123a5d5cd4a800e2","avatarUrl":"/avatars/a0961ca5463aae05de0b1574c0064fae.svg","isPro":false,"fullname":"guibin zhang","user":"greeky","type":"user"},"summary":"Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.","upvotes":6,"discussionId":"698b1fc81b2dc6b37d61b433","githubRepo":"https://github.com/EcthelionLiu/TodoEvolve","githubRepoAddedBy":"user","ai_summary":"TodoEvolve enables autonomous synthesis and revision of task-specific planning architectures through a modular design space and multi-objective reinforcement learning optimization.","ai_keywords":["meta-planning","planning architectures","PlanFactory","Impedance-Guided Preference Optimization","IGPO","reinforcement learning","task-specific planning","modular design space"],"githubStars":7},"publishedAt":"2026-02-08T01:37:01.000Z","title":"TodoEvolve: Learning to Architect Agent Planning Systems","summary":"Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via Impedance-Guided Preference Optimization (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07839.png","numComments":2,"submittedBy":{"_id":"6363a1fa123a5d5cd4a800e2","avatarUrl":"/avatars/a0961ca5463aae05de0b1574c0064fae.svg","fullname":"guibin zhang","name":"greeky","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07755","authors":[{"_id":"698c5623eb12ea74539167c8","name":"Yiming Xiong","hidden":false},{"_id":"698c5623eb12ea74539167c9","name":"Shengran Hu","hidden":false},{"_id":"698c5623eb12ea74539167ca","name":"Jeff Clune","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/640530fd0ab5e22719fb199a/ZjwTGGL6Vakh4QRRq222F.gif"],"publishedAt":"2026-02-08T01:20:49.000Z","submittedOnDailyAt":"2026-02-11T07:44:00.282Z","title":"Learning to Continually Learn via Meta-learning Agentic Memory Designs","submittedOnDailyBy":{"_id":"640530fd0ab5e22719fb199a","avatarUrl":"/avatars/6b023f496a2f8cb5f37ec617d84179b7.svg","isPro":false,"fullname":"Shengran HU","user":"Shengran","type":"user"},"summary":"The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.","upvotes":6,"discussionId":"698c5624eb12ea74539167cb","projectPage":"https://yimingxiong.me/alma","githubRepo":"https://github.com/zksha/alma","githubRepoAddedBy":"user","ai_summary":"ALMA is a framework that uses meta-learning to automatically discover memory designs for agentic systems, enabling continual learning without human engineering across diverse domains.","ai_keywords":["foundation models","agentic systems","continual learning","memory modules","meta-learning","Meta Agent","executable code","database schemas","retrieval mechanisms","update mechanisms"],"githubStars":140},"publishedAt":"2026-02-07T20:20:49.000Z","title":"Learning to Continually Learn via Meta-learning Agentic Memory Designs","summary":"The statelessness of foundation models bottlenecks agentic systems' ability to continually learn, a core capability for long-horizon reasoning and adaptation. To address this limitation, agentic systems commonly incorporate memory modules to retain and reuse past experience, aiming for continual learning during test time. However, most existing memory designs are human-crafted and fixed, which limits their ability to adapt to the diversity and non-stationarity of real-world tasks. In this paper, we introduce ALMA (Automated meta-Learning of Memory designs for Agentic systems), a framework that meta-learns memory designs to replace hand-engineered memory designs, therefore minimizing human effort and enabling agentic systems to be continual learners across diverse domains. Our approach employs a Meta Agent that searches over memory designs expressed as executable code in an open-ended manner, theoretically allowing the discovery of arbitrary memory designs, including database schemas as well as their retrieval and update mechanisms. Extensive experiments across four sequential decision-making domains demonstrate that the learned memory designs enable more effective and efficient learning from experience than state-of-the-art human-crafted memory designs on all benchmarks. When developed and deployed safely, ALMA represents a step toward self-improving AI systems that learn to be adaptive, continual learners.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/640530fd0ab5e22719fb199a/ZjwTGGL6Vakh4QRRq222F.gif"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07755.png","numComments":2,"submittedBy":{"_id":"640530fd0ab5e22719fb199a","avatarUrl":"/avatars/6b023f496a2f8cb5f37ec617d84179b7.svg","fullname":"Shengran HU","name":"Shengran","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.09591","authors":[{"_id":"698c7521eb12ea7453916832","user":{"_id":"67f7ed6ae64695f24a724be1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67f7ed6ae64695f24a724be1/DTeiPbxey03v4kBmbJkaC.png","isPro":false,"fullname":"Daisuke Nohara","user":"neodymium6","type":"user"},"name":"Daisuke Nohara","status":"claimed_verified","statusLastChangedAt":"2026-02-11T12:43:42.483Z","hidden":false},{"_id":"698c7521eb12ea7453916833","user":{"_id":"6308c49c454dc257521bc7f9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6308c49c454dc257521bc7f9/UWUS6OPa6OpVu1T0gd-wJ.jpeg","isPro":false,"fullname":"Taishi","user":"Taishi-N324","type":"user"},"name":"Taishi Nakamura","status":"claimed_verified","statusLastChangedAt":"2026-02-11T22:16:31.194Z","hidden":false},{"_id":"698c7521eb12ea7453916834","name":"Rio Yokota","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67f7ed6ae64695f24a724be1/IZ-6mQkAvIQwUdcXgQjb7.png"],"publishedAt":"2026-02-10T09:45:42.000Z","submittedOnDailyAt":"2026-02-11T10:30:14.868Z","title":"On the Optimal Reasoning Length for RL-Trained Language Models","submittedOnDailyBy":{"_id":"67f7ed6ae64695f24a724be1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67f7ed6ae64695f24a724be1/DTeiPbxey03v4kBmbJkaC.png","isPro":false,"fullname":"Daisuke Nohara","user":"neodymium6","type":"user"},"summary":"Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.","upvotes":5,"discussionId":"698c7521eb12ea7453916835","ai_summary":"Length control methods in reinforcement learning-trained language models affect reasoning performance and computational efficiency, with optimal output lengths balancing these factors.","ai_keywords":["reinforcement learning","chain of thought","length control methods","reasoning acquisition","output length","computational cost","efficiency","performance"]},"publishedAt":"2026-02-10T04:45:42.000Z","title":"On the Optimal Reasoning Length for RL-Trained Language Models","summary":"Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/67f7ed6ae64695f24a724be1/IZ-6mQkAvIQwUdcXgQjb7.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09591.png","numComments":2,"submittedBy":{"_id":"67f7ed6ae64695f24a724be1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67f7ed6ae64695f24a724be1/DTeiPbxey03v4kBmbJkaC.png","fullname":"Daisuke Nohara","name":"neodymium6","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.09153","authors":[{"_id":"698bfc136052d3bed9630ae7","user":{"_id":"663d126ee6257fa86ac793eb","avatarUrl":"/avatars/954f47804a9149dfc1c1d5de4dfefb63.svg","isPro":false,"fullname":"Nicholas Pfaff","user":"nepfaff","type":"user"},"name":"Nicholas Pfaff","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:17:17.749Z","hidden":false},{"_id":"698bfc136052d3bed9630ae8","name":"Thomas Cohn","hidden":false},{"_id":"698bfc136052d3bed9630ae9","name":"Sergey Zakharov","hidden":false},{"_id":"698bfc136052d3bed9630aea","name":"Rick Cory","hidden":false},{"_id":"698bfc136052d3bed9630aeb","name":"Russ Tedrake","hidden":false}],"publishedAt":"2026-02-09T19:56:04.000Z","submittedOnDailyAt":"2026-02-11T20:25:55.910Z","title":"SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes","submittedOnDailyBy":{"_id":"663d126ee6257fa86ac793eb","avatarUrl":"/avatars/954f47804a9149dfc1c1d5de4dfefb63.svg","isPro":false,"fullname":"Nicholas Pfaff","user":"nepfaff","type":"user"},"summary":"Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stagesx2013from architectural layout to furniture placement to small object populationx2013each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.","upvotes":5,"discussionId":"698bfc136052d3bed9630aec","projectPage":"https://scenesmith.github.io/","githubRepo":"https://github.com/nepfaff/scenesmith","githubRepoAddedBy":"user","ai_summary":"SceneSmith is a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts through multiple stages involving VLM agents and integrated asset generation techniques.","ai_keywords":["SceneSmith","hierarchical agentic framework","natural language prompts","VLM agents","text-to-3D synthesis","dataset retrieval","physical property estimation","simulation-ready environments","robotic manipulation","end-to-end pipeline"],"githubStars":239,"organization":{"_id":"63360dd88a858c0785bd5b14","name":"TRI-ML","fullname":"Toyota Research Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6150b090d84cf0532aa1764b/PRbgRfhJ-8xWF4GteD1Tf.png"}},"publishedAt":"2026-02-09T14:56:04.000Z","title":"SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes","summary":"Simulation has become a key tool for training and evaluating home robots at scale, yet existing environments fail to capture the diversity and physical complexity of real indoor spaces. Current scene synthesis methods produce sparsely furnished rooms that lack the dense clutter, articulated furniture, and physical properties essential for robotic manipulation. We introduce SceneSmith, a hierarchical agentic framework that generates simulation-ready indoor environments from natural language prompts. SceneSmith constructs scenes through successive stagesx2013from architectural layout to furniture placement to small object populationx2013each implemented as an interaction among VLM agents: designer, critic, and orchestrator. The framework tightly integrates asset generation through text-to-3D synthesis for static objects, dataset retrieval for articulated objects, and physical property estimation. SceneSmith generates 3-6x more objects than prior methods, with <2% inter-object collisions and 96% of objects remaining stable under physics simulation. In a user study with 205 participants, it achieves 92% average realism and 91% average prompt faithfulness win rates against baselines. We further demonstrate that these environments can be used in an end-to-end pipeline for automatic robot policy evaluation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09153.png","numComments":2,"submittedBy":{"_id":"663d126ee6257fa86ac793eb","avatarUrl":"/avatars/954f47804a9149dfc1c1d5de4dfefb63.svg","fullname":"Nicholas Pfaff","name":"nepfaff","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"63360dd88a858c0785bd5b14","name":"TRI-ML","fullname":"Toyota Research Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6150b090d84cf0532aa1764b/PRbgRfhJ-8xWF4GteD1Tf.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09024","authors":[{"_id":"698ab8ae1b2dc6b37d61b089","name":"Qihang Yu","hidden":false},{"_id":"698ab8ae1b2dc6b37d61b08a","name":"Qihao Liu","hidden":false},{"_id":"698ab8ae1b2dc6b37d61b08b","user":{"_id":"661c9059bcd78151e5c06ea1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/661c9059bcd78151e5c06ea1/27bfNo1LZeZQ77vWuAa10.png","isPro":false,"fullname":"Ju He","user":"turkeyju","type":"user"},"name":"Ju He","status":"claimed_verified","statusLastChangedAt":"2026-02-12T20:28:47.491Z","hidden":false},{"_id":"698ab8ae1b2dc6b37d61b08c","name":"Xinyang Zhang","hidden":false},{"_id":"698ab8ae1b2dc6b37d61b08d","name":"Yang Liu","hidden":false},{"_id":"698ab8ae1b2dc6b37d61b08e","name":"Liang-Chieh Chen","hidden":false},{"_id":"698ab8ae1b2dc6b37d61b08f","name":"Xi Chen","hidden":false}],"publishedAt":"2026-02-09T18:59:58.000Z","submittedOnDailyAt":"2026-02-11T00:43:47.623Z","title":"Autoregressive Image Generation with Masked Bit Modeling","submittedOnDailyBy":{"_id":"648010b00b9d0f49849adb19","avatarUrl":"/avatars/54419bb6e6ee788aba10cd64cd921204.svg","isPro":false,"fullname":"Qihang Yu","user":"yucornetto","type":"user"},"summary":"This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/","upvotes":5,"discussionId":"698ab8ae1b2dc6b37d61b090","projectPage":"https://bar-gen.github.io/","githubRepo":"https://github.com/amazon-far/BAR","githubRepoAddedBy":"user","ai_summary":"Discrete tokenizers can match or exceed continuous methods when properly scaled, and a new masked Bit AutoRegressive modeling approach achieves state-of-the-art results with reduced computational costs.","ai_keywords":["discrete tokenizers","continuous pipelines","latent space","codebook size","autoregressive transformer","masked bit modeling","Bit AutoRegressive modeling","gFID","ImageNet-256"],"githubStars":30,"organization":{"_id":"68d5924a76c6b4bfe8b4ab60","name":"Amazon-FAR","fullname":"Amazon Frontier AI & Robotics","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68251d059667f9f347003874/YPxVjC8rCyexwo42cznvE.png"}},"publishedAt":"2026-02-09T13:59:58.000Z","title":"Autoregressive Image Generation with Masked Bit Modeling","summary":"This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09024.png","numComments":2,"submittedBy":{"_id":"648010b00b9d0f49849adb19","avatarUrl":"/avatars/54419bb6e6ee788aba10cd64cd921204.svg","fullname":"Qihang Yu","name":"yucornetto","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"68d5924a76c6b4bfe8b4ab60","name":"Amazon-FAR","fullname":"Amazon Frontier AI & Robotics","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68251d059667f9f347003874/YPxVjC8rCyexwo42cznvE.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08344","authors":[{"_id":"698bf2606052d3bed9630a57","name":"Qi Guo","hidden":false},{"_id":"698bf2606052d3bed9630a58","name":"Jianing Wang","hidden":false},{"_id":"698bf2606052d3bed9630a59","user":{"_id":"65a0aade5fafc248c2156e95","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg","isPro":false,"fullname":"DeyangKong","user":"DeyangKong","type":"user"},"name":"Deyang Kong","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:45.220Z","hidden":false},{"_id":"698bf2606052d3bed9630a5a","name":"Xiangyu Xi","hidden":false},{"_id":"698bf2606052d3bed9630a5b","name":"Jianfei Zhang","hidden":false},{"_id":"698bf2606052d3bed9630a5c","name":"Yi Lu","hidden":false},{"_id":"698bf2606052d3bed9630a5d","name":"Jingang Wang","hidden":false},{"_id":"698bf2606052d3bed9630a5e","name":"Wei Wang","hidden":false},{"_id":"698bf2606052d3bed9630a5f","name":"Shikun Zhang","hidden":false},{"_id":"698bf2606052d3bed9630a60","name":"Wei Ye","hidden":false}],"publishedAt":"2026-02-09T07:29:13.000Z","submittedOnDailyAt":"2026-02-11T00:39:26.160Z","title":"OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration","submittedOnDailyBy":{"_id":"65a0aade5fafc248c2156e95","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg","isPro":false,"fullname":"DeyangKong","user":"DeyangKong","type":"user"},"summary":"Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.","upvotes":5,"discussionId":"698bf2606052d3bed9630a61","ai_summary":"Reinforcement learning with verifiable rewards is used to enhance parallel thinking in large reasoning models through outline-guided path exploration that reduces information redundancy and improves solution discovery.","ai_keywords":["parallel thinking","large reasoning models","Reinforcement Learning","RLVR","mutual information bottleneck","outline-guided path exploration","reasoning outlines","iterative RL strategy","aggregation phase","path exploration stage"]},"publishedAt":"2026-02-09T02:29:13.000Z","title":"OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration","summary":"Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08344.png","numComments":2,"submittedBy":{"_id":"65a0aade5fafc248c2156e95","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65a0aade5fafc248c2156e95/S9YjJMTuKc-U1cFizqUMA.jpeg","fullname":"DeyangKong","name":"DeyangKong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.07153","authors":[{"_id":"698abd921b2dc6b37d61b120","user":{"_id":"683c642b02c1a474a867964e","avatarUrl":"/avatars/63e44a9cf788ee7b3ad236407700ceca.svg","isPro":false,"fullname":"Jinbiao Wei","user":"mikeweii","type":"user"},"name":"Jinbiao Wei","status":"admin_assigned","statusLastChangedAt":"2026-02-10T19:44:23.938Z","hidden":false},{"_id":"698abd921b2dc6b37d61b121","name":"Yilun Zhao","hidden":false},{"_id":"698abd921b2dc6b37d61b122","name":"Kangqi Ni","hidden":false},{"_id":"698abd921b2dc6b37d61b123","name":"Arman Cohan","hidden":false}],"publishedAt":"2026-02-06T19:55:26.000Z","submittedOnDailyAt":"2026-02-11T00:20:18.700Z","title":"ANCHOR: Branch-Point Data Generation for GUI Agents","submittedOnDailyBy":{"_id":"683c642b02c1a474a867964e","avatarUrl":"/avatars/63e44a9cf788ee7b3ad236407700ceca.svg","isPro":false,"fullname":"Jinbiao Wei","user":"mikeweii","type":"user"},"summary":"End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.","upvotes":5,"discussionId":"698abd921b2dc6b37d61b124","ai_summary":"A trajectory expansion framework called Anchor bootstraps scalable desktop supervision from seed demonstrations by identifying branch points and generating new trajectories through state-grounded task variants.","ai_keywords":["trajectory expansion","GUI agents","desktop environments","seed demonstrations","branch points","state-grounded task variants","trajectory generation","verifier","task completion","step-level filtering","denoising"],"organization":{"_id":"6532df27d690f3012efde84c","name":"yale-nlp","fullname":"Yale NLP Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65204db5b0e0d57453cb1809/9OAeiZ-BrN2g1h1yd6-1W.png"}},"publishedAt":"2026-02-06T14:55:26.000Z","title":"ANCHOR: Branch-Point Data Generation for GUI Agents","summary":"End-to-end GUI agents for real desktop environments require large amounts of high-quality interaction data, yet collecting human demonstrations is expensive and existing synthetic pipelines often suffer from limited task diversity or noisy, goal-drifting trajectories. We present a trajectory expansion framework Anchor that bootstraps scalable desktop supervision from a small set of verified seed demonstrations. Starting from each seed, we identify branch points that correspond to meaningful state changes and propose new, state-grounded task variants conditioned on the current GUI context. An executing agent then follows the proposed instructions to generate new trajectories, while a verifier enforces task completion via state-aware checks and trajectory-level consistency. To improve supervision quality, we further apply task-conditioned step-level filtering to remove ungrounded actions and denoise post-branch segments to maintain coherent intent. Experiments on standard desktop benchmarks, OSWorld and WindowsAgentArena, show that models fine-tuned on our expanded corpus achieve consistent improvements over zero-shot agents and representative synthesis baselines, and generalize across applications and operating systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07153.png","numComments":3,"submittedBy":{"_id":"683c642b02c1a474a867964e","avatarUrl":"/avatars/63e44a9cf788ee7b3ad236407700ceca.svg","fullname":"Jinbiao Wei","name":"mikeweii","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6532df27d690f3012efde84c","name":"yale-nlp","fullname":"Yale NLP Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65204db5b0e0d57453cb1809/9OAeiZ-BrN2g1h1yd6-1W.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.06161","authors":[{"_id":"6989a4a2beecc443208d27be","user":{"_id":"6605389c80bc77d583bc1bfb","avatarUrl":"/avatars/63d70bfc7cf299635ee9e0cf529a2975.svg","isPro":false,"fullname":"Yanzheng Xiang","user":"YanzhengXiang","type":"user"},"name":"Yanzheng Xiang","status":"claimed_verified","statusLastChangedAt":"2026-02-09T14:31:31.793Z","hidden":false},{"_id":"6989a4a2beecc443208d27bf","name":"Lan Wei","hidden":false},{"_id":"6989a4a2beecc443208d27c0","name":"Yizhen Yao","hidden":false},{"_id":"6989a4a2beecc443208d27c1","name":"Qinglin Zhu","hidden":false},{"_id":"6989a4a2beecc443208d27c2","name":"Hanqi Yan","hidden":false},{"_id":"6989a4a2beecc443208d27c3","name":"Chen Jin","hidden":false},{"_id":"6989a4a2beecc443208d27c4","name":"Philip Alexander Teare","hidden":false},{"_id":"6989a4a2beecc443208d27c5","name":"Dandan Zhang","hidden":false},{"_id":"6989a4a2beecc443208d27c6","name":"Lin Gui","hidden":false},{"_id":"6989a4a2beecc443208d27c7","name":"Amrutha Saseendran","hidden":false},{"_id":"6989a4a2beecc443208d27c8","name":"Yulan He","hidden":false}],"publishedAt":"2026-02-05T19:58:48.000Z","submittedOnDailyAt":"2026-02-11T06:38:13.562Z","title":"Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding","submittedOnDailyBy":{"_id":"6605389c80bc77d583bc1bfb","avatarUrl":"/avatars/63d70bfc7cf299635ee9e0cf529a2975.svg","isPro":false,"fullname":"Yanzheng Xiang","user":"YanzhengXiang","type":"user"},"summary":"Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.","upvotes":4,"discussionId":"6989a4a3beecc443208d27c9","ai_summary":"COVER enables efficient parallel decoding for diffusion language models by implementing cache override verification that reduces unnecessary revisions and maintains output quality through stable drafting and attention view construction.","ai_keywords":["diffusion language model","parallel decoding","token masking","verification schemes","flip-flop oscillations","KV cache override","attention views","diagonal correction","stability aware score","cache drift"],"organization":{"_id":"602d1122374a0dbe5856eca3","name":"KingsCollegeLondon","fullname":"King's College London","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613566183325-5e54fab537cb5b49818287e5.jpeg"}},"publishedAt":"2026-02-05T14:58:48.000Z","title":"Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding","summary":"Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06161.png","numComments":3,"submittedBy":{"_id":"6605389c80bc77d583bc1bfb","avatarUrl":"/avatars/63d70bfc7cf299635ee9e0cf529a2975.svg","fullname":"Yanzheng Xiang","name":"YanzhengXiang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"602d1122374a0dbe5856eca3","name":"KingsCollegeLondon","fullname":"King's College London","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613566183325-5e54fab537cb5b49818287e5.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.05085","authors":[{"_id":"698d062b6c5152984e4f3e27","user":{"_id":"65f2a015f79caa96bcde9de4","avatarUrl":"/avatars/b989383d43162a42e264001ff940ea73.svg","isPro":false,"fullname":"Sidi Lu","user":"desire2020","type":"user"},"name":"Sidi Lu","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:18.928Z","hidden":false},{"_id":"698d062b6c5152984e4f3e28","name":"Zhenwen Liang","hidden":false},{"_id":"698d062b6c5152984e4f3e29","name":"Dongyang Ma","hidden":false},{"_id":"698d062b6c5152984e4f3e2a","user":{"_id":"64674375946476c5d215382d","avatarUrl":"/avatars/0f0b98080c064a17d003da8a22a047cc.svg","isPro":false,"fullname":"Yan Wang","user":"libertywing","type":"user"},"name":"Yan Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:16.938Z","hidden":false},{"_id":"698d062b6c5152984e4f3e2b","name":"Haitao Mi","hidden":false},{"_id":"698d062b6c5152984e4f3e2c","name":"Dong Yu","hidden":false}],"publishedAt":"2026-02-04T22:09:40.000Z","submittedOnDailyAt":"2026-02-11T20:20:40.073Z","title":"Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories","submittedOnDailyBy":{"_id":"65f2a015f79caa96bcde9de4","avatarUrl":"/avatars/b989383d43162a42e264001ff940ea73.svg","isPro":false,"fullname":"Sidi Lu","user":"desire2020","type":"user"},"summary":"In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.","upvotes":4,"discussionId":"698d062b6c5152984e4f3e2d","ai_summary":"Locas, a locally-supported parametric memory mechanism, enables flexible integration with transformer models for continual learning while minimizing catastrophic forgetting through principled initialization techniques.","ai_keywords":["parametric memory","transformer","FFN blocks","continual learning","low-rank sideway-FFN-style memories","GLU-FFN","parameter-efficient learning","computation-efficient learning","catastrophic forgetting","MMLU evaluation"],"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-04T17:09:40.000Z","title":"Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories","summary":"In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05085.png","numComments":2,"submittedBy":{"_id":"65f2a015f79caa96bcde9de4","avatarUrl":"/avatars/b989383d43162a42e264001ff940ea73.svg","fullname":"Sidi Lu","name":"desire2020","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.10099","authors":[{"_id":"698c5686eb12ea74539167cd","name":"Amandeep Kumar","hidden":false},{"_id":"698c5686eb12ea74539167ce","name":"Vishal M. Patel","hidden":false}],"publishedAt":"2026-02-10T18:58:04.000Z","submittedOnDailyAt":"2026-02-11T07:49:26.845Z","title":"Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders","submittedOnDailyBy":{"_id":"646bbce4628e5b50b2dfdc1c","avatarUrl":"/avatars/451361495c67be90a07698c3ad358dc8.svg","isPro":false,"fullname":"Amandeep","user":"Aman015","type":"user"},"summary":"Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF","upvotes":3,"discussionId":"698c5686eb12ea74539167cf","githubRepo":"https://github.com/amandpkr/RJF","githubRepoAddedBy":"user","ai_summary":"Geometric interference in standard diffusion transformers prevents convergence on representation encoders, which is resolved through Riemannian flow matching with jacobi regularization enabling effective training without width scaling.","ai_keywords":["diffusion transformers","representation encoders","geometric interference","Euclidean flow matching","hyperspherical feature space","Riemannian flow matching","jacobi regularization","manifold geodesics","curvature-induced error propagation","Diffusion Transformer architectures","DiT-B"],"githubStars":15,"organization":{"_id":"653945b47ba797097a7b4eab","name":"JohnsHopkins","fullname":"Johns Hopkins University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/653944e58e687a41625a4694/qqHzBOarppVrUuZbbjqwh.png"}},"publishedAt":"2026-02-10T13:58:04.000Z","title":"Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders","summary":"Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10099.png","numComments":2,"submittedBy":{"_id":"646bbce4628e5b50b2dfdc1c","avatarUrl":"/avatars/451361495c67be90a07698c3ad358dc8.svg","fullname":"Amandeep","name":"Aman015","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"653945b47ba797097a7b4eab","name":"JohnsHopkins","fullname":"Johns Hopkins University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/653944e58e687a41625a4694/qqHzBOarppVrUuZbbjqwh.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08503","authors":[{"_id":"698bb52d6052d3bed96308fd","name":"Yi Ding","hidden":false},{"_id":"698bb52d6052d3bed96308fe","name":"Ziliang Qiu","hidden":false},{"_id":"698bb52d6052d3bed96308ff","name":"Bolian Li","hidden":false},{"_id":"698bb52d6052d3bed9630900","name":"Ruqi Zhang","hidden":false}],"publishedAt":"2026-02-09T10:55:13.000Z","submittedOnDailyAt":"2026-02-11T04:01:57.995Z","title":"Learning Self-Correction in Vision-Language Models via Rollout Augmentation","submittedOnDailyBy":{"_id":"662678dfdd43e904ef1dcd03","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg","isPro":false,"fullname":"Yi Ding","user":"Tuwhy","type":"user"},"summary":"Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72times training time per step.","upvotes":3,"discussionId":"698bb52e6052d3bed9630901","projectPage":"https://dripnowhy.github.io/Octopus/","ai_summary":"Octopus, an RL rollout augmentation framework, enables efficient self-correction learning in vision-language models through synthetic example generation and response masking strategies.","ai_keywords":["reinforcement learning","vision-language models","self-correction","rollout augmentation","response-masking strategy","sample efficiency","RL optimization","controllable self-correction"]},"publishedAt":"2026-02-09T05:55:13.000Z","title":"Learning Self-Correction in Vision-Language Models via Rollout Augmentation","summary":"Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only 0.72times training time per step.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08503.png","numComments":2,"submittedBy":{"_id":"662678dfdd43e904ef1dcd03","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg","fullname":"Yi Ding","name":"Tuwhy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.05892","authors":[{"_id":"698c93c1e40091ab7f8e9191","name":"Han Li","hidden":false},{"_id":"698c93c1e40091ab7f8e9192","name":"Letian Zhu","hidden":false},{"_id":"698c93c1e40091ab7f8e9193","name":"Bohan Zhang","hidden":false},{"_id":"698c93c1e40091ab7f8e9194","name":"Rili Feng","hidden":false},{"_id":"698c93c1e40091ab7f8e9195","name":"Jiaming Wang","hidden":false},{"_id":"698c93c1e40091ab7f8e9196","name":"Yue Pan","hidden":false},{"_id":"698c93c1e40091ab7f8e9197","name":"Earl T. Barr","hidden":false},{"_id":"698c93c1e40091ab7f8e9198","name":"Sarro Federica","hidden":false},{"_id":"698c93c1e40091ab7f8e9199","name":"Zhaoyang Chu","hidden":false},{"_id":"698c93c1e40091ab7f8e919a","name":"He Ye","hidden":false}],"publishedAt":"2026-02-05T17:10:26.000Z","submittedOnDailyAt":"2026-02-11T12:11:56.881Z","title":"ContextBench: A Benchmark for Context Retrieval in Coding Agents","submittedOnDailyBy":{"_id":"64fb128552e82dd432682b06","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64fb128552e82dd432682b06/GYcOiwa4R3RrgcM2tSuV_.png","isPro":false,"fullname":"Zhaoyang Chu","user":"chuzy","type":"user"},"summary":"LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.","upvotes":3,"discussionId":"698c93c1e40091ab7f8e919b","projectPage":"https://contextbench.github.io/","githubRepo":"https://github.com/EuniAI/ContextBench","githubRepoAddedBy":"user","ai_summary":"ContextBench evaluates context retrieval in coding agents through detailed process analysis, revealing that advanced agent designs provide limited improvements in context usage while highlighting gaps between explored and utilized information.","ai_keywords":["context retrieval","issue resolution","LLMs","coding agents","process-oriented evaluation","gold contexts","context recall","context precision","context efficiency","agent trajectories"],"githubStars":6},"publishedAt":"2026-02-05T12:10:26.000Z","title":"ContextBench: A Benchmark for Context Retrieval in Coding Agents","summary":"LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05892.png","numComments":2,"submittedBy":{"_id":"64fb128552e82dd432682b06","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64fb128552e82dd432682b06/GYcOiwa4R3RrgcM2tSuV_.png","fullname":"Zhaoyang Chu","name":"chuzy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.05435","authors":[{"_id":"698c30536052d3bed9630c93","name":"Donglin Yang","hidden":false},{"_id":"698c30536052d3bed9630c94","name":"Yongxing Zhang","hidden":false},{"_id":"698c30536052d3bed9630c95","name":"Xin Yu","hidden":false},{"_id":"698c30536052d3bed9630c96","name":"Liang Hou","hidden":false},{"_id":"698c30536052d3bed9630c97","name":"Xin Tao","hidden":false},{"_id":"698c30536052d3bed9630c98","name":"Pengfei Wan","hidden":false},{"_id":"698c30536052d3bed9630c99","name":"Xiaojuan Qi","hidden":false},{"_id":"698c30536052d3bed9630c9a","name":"Renjie Liao","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66de6bd4174e9c6971dedb70/wGg_dE2ypVy7ZoMOHHUZr.png"],"publishedAt":"2026-02-05T08:25:05.000Z","submittedOnDailyAt":"2026-02-11T05:09:44.300Z","title":"Stable Velocity: A Variance Perspective on Flow Matching","submittedOnDailyBy":{"_id":"66de6bd4174e9c6971dedb70","avatarUrl":"/avatars/40f35d8d87ecc543bc1debdb7790133f.svg","isPro":false,"fullname":"Yang","user":"linYD0718","type":"user"},"summary":"While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet 256times256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2times faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.","upvotes":3,"discussionId":"698c30536052d3bed9630c9b","projectPage":"https://linydthu.github.io/StableVelocity/","githubRepo":"https://github.com/linYDTHU/StableVelocity","githubRepoAddedBy":"user","ai_summary":"Stable Velocity framework addresses high-variance training in flow matching by identifying low-variance regimes and proposing variance-reduction techniques for improved training efficiency and sampling speed.","ai_keywords":["flow matching","conditional velocities","variance reduction","Stable Velocity Matching","Variance-Aware Representation Alignment","Stable Velocity Sampling","training efficiency","sampling speed"],"githubStars":18},"publishedAt":"2026-02-05T03:25:05.000Z","title":"Stable Velocity: A Variance Perspective on Flow Matching","summary":"While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet 256times256 and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than 2times faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66de6bd4174e9c6971dedb70/wGg_dE2ypVy7ZoMOHHUZr.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05435.png","numComments":2,"submittedBy":{"_id":"66de6bd4174e9c6971dedb70","avatarUrl":"/avatars/40f35d8d87ecc543bc1debdb7790133f.svg","fullname":"Yang","name":"linYD0718","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.02464","authors":[{"_id":"698a0c131b2dc6b37d61adeb","name":"Or Shafran","hidden":false},{"_id":"698a0c131b2dc6b37d61adec","name":"Shaked Ronen","hidden":false},{"_id":"698a0c131b2dc6b37d61aded","name":"Omri Fahn","hidden":false},{"_id":"698a0c131b2dc6b37d61adee","name":"Shauli Ravfogel","hidden":false},{"_id":"698a0c131b2dc6b37d61adef","name":"Atticus Geiger","hidden":false},{"_id":"698a0c131b2dc6b37d61adf0","name":"Mor Geva","hidden":false}],"publishedAt":"2026-02-02T18:49:05.000Z","submittedOnDailyAt":"2026-02-11T05:12:22.421Z","title":"From Directions to Regions: Decomposing Activations in Language Models via Local Geometry","submittedOnDailyBy":{"_id":"67b738790d11464dcbc8adf5","avatarUrl":"/avatars/f59109ff562d7b037be2f81bb11df270.svg","isPro":false,"fullname":"Or Shafran","user":"ordavids1","type":"user"},"summary":"Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.","upvotes":3,"discussionId":"698a0c131b2dc6b37d61adf1","githubRepo":"https://github.com/ordavid-s/decomposing-activations-local-geometry","githubRepoAddedBy":"user","ai_summary":"Mixture of Factor Analyzers (MFA) provides a scalable, unsupervised approach for discovering complex, nonlinear structures in language model activation spaces by modeling local geometric structures through Gaussian regions and their covariance properties.","ai_keywords":["Mixture of Factor Analyzers","activation space","Gaussian regions","local covariance structure","concept discovery","model control","sparse autoencoders"],"githubStars":16},"publishedAt":"2026-02-02T13:49:05.000Z","title":"From Directions to Regions: Decomposing Activations in Language Models via Local Geometry","summary":"Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02464.png","numComments":2,"submittedBy":{"_id":"67b738790d11464dcbc8adf5","avatarUrl":"/avatars/f59109ff562d7b037be2f81bb11df270.svg","fullname":"Or Shafran","name":"ordavids1","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07398","authors":[{"_id":"698b88a16052d3bed9630867","user":{"_id":"65492ac93ce45eb764f723d7","avatarUrl":"/avatars/a2eac1e964335d6d1e0820c4282a6ebe.svg","isPro":false,"fullname":"Ruoyao Wen","user":"Ruoyao","type":"user"},"name":"Ruoyao Wen","status":"claimed_verified","statusLastChangedAt":"2026-02-11T22:16:56.110Z","hidden":false},{"_id":"698b88a16052d3bed9630868","name":"Hao Li","hidden":false},{"_id":"698b88a16052d3bed9630869","name":"Chaowei Xiao","hidden":false},{"_id":"698b88a16052d3bed963086a","name":"Ning Zhang","hidden":false}],"publishedAt":"2026-02-07T06:28:51.000Z","submittedOnDailyAt":"2026-02-11T19:57:58.922Z","title":"AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management","submittedOnDailyBy":{"_id":"65492ac93ce45eb764f723d7","avatarUrl":"/avatars/a2eac1e964335d6d1e0820c4282a6ebe.svg","isPro":false,"fullname":"Ruoyao Wen","user":"Ruoyao","type":"user"},"summary":"Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.\n  We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.\n  On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.","upvotes":2,"discussionId":"698b88a16052d3bed963086b","githubRepo":"https://github.com/ruoyaow/agentsys-memory","githubRepoAddedBy":"user","ai_summary":"AgentSys defends against indirect prompt injection in LLM agents through hierarchical memory isolation and controlled data flow, significantly reducing attack success rates while maintaining performance.","ai_keywords":["LLM agents","prompt injection","working memory","context window","tool calls","memory isolation","agent hierarchy","schema validation","JSON parsing","defensive mechanisms","attack success rate"],"githubStars":6},"publishedAt":"2026-02-07T01:28:51.000Z","title":"AgentSys: Secure and Dynamic LLM Agents Through Explicit Hierarchical Memory Management","summary":"Indirect prompt injection threatens LLM agents by embedding malicious instructions in external content, enabling unauthorized actions and data theft. LLM agents maintain working memory through their context window, which stores interaction history for decision-making. Conventional agents indiscriminately accumulate all tool outputs and reasoning traces in this memory, creating two critical vulnerabilities: (1) injected instructions persist throughout the workflow, granting attackers multiple opportunities to manipulate behavior, and (2) verbose, non-essential content degrades decision-making capabilities. Existing defenses treat bloated memory as given and focus on remaining resilient, rather than reducing unnecessary accumulation to prevent the attack.\n  We present AgentSys, a framework that defends against indirect prompt injection through explicit memory management. Inspired by process memory isolation in operating systems, AgentSys organizes agents hierarchically: a main agent spawns worker agents for tool calls, each running in an isolated context and able to spawn nested workers for subtasks. External data and subtask traces never enter the main agent's memory; only schema-validated return values can cross boundaries through deterministic JSON parsing. Ablations show isolation alone cuts attack success to 2.19%, and adding a validator/sanitizer further improves defense with event-triggered checks whose overhead scales with operations rather than context length.\n  On AgentDojo and ASB, AgentSys achieves 0.78% and 4.25% attack success while slightly improving benign utility over undefended baselines. It remains robust to adaptive attackers and across multiple foundation models, showing that explicit memory management enables secure, dynamic LLM agent architectures. Our code is available at: https://github.com/ruoyaow/agentsys-memory.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07398.png","numComments":2,"submittedBy":{"_id":"65492ac93ce45eb764f723d7","avatarUrl":"/avatars/a2eac1e964335d6d1e0820c4282a6ebe.svg","fullname":"Ruoyao Wen","name":"Ruoyao","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.09924","authors":[{"_id":"698c449a6052d3bed9630cf2","user":{"_id":"65cb41d20b3bd8f5ce0fcdba","avatarUrl":"/avatars/351f1afbfba9f71f0c3760b046e4fccd.svg","isPro":false,"fullname":"William Gitta Lugoloobi","user":"CoffeeGitta","type":"user"},"name":"William Lugoloobi","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:40.866Z","hidden":false},{"_id":"698c449a6052d3bed9630cf3","name":"Thomas Foster","hidden":false},{"_id":"698c449a6052d3bed9630cf4","name":"William Bankes","hidden":false},{"_id":"698c449a6052d3bed9630cf5","name":"Chris Russell","hidden":false}],"publishedAt":"2026-02-10T15:57:00.000Z","submittedOnDailyAt":"2026-02-11T06:37:21.213Z","title":"LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations","submittedOnDailyBy":{"_id":"65cb41d20b3bd8f5ce0fcdba","avatarUrl":"/avatars/351f1afbfba9f71f0c3760b046e4fccd.svg","isPro":false,"fullname":"William Gitta Lugoloobi","user":"CoffeeGitta","type":"user"},"summary":"Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty","upvotes":1,"discussionId":"698c449a6052d3bed9630cf6","githubRepo":"https://github.com/KabakaWilliam/llms_know_difficulty","githubRepoAddedBy":"user","ai_summary":"LLMs' internal representations can predict problem difficulty and enable efficient inference routing that reduces costs while maintaining performance.","ai_keywords":["linear probes","pre-generation activations","policy-specific success","math tasks","coding tasks","E2H-AMC","model-specific difficulty","extended reasoning","inference cost","efficiency gains"],"githubStars":0},"publishedAt":"2026-02-10T10:57:00.000Z","title":"LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations","summary":"Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09924.png","numComments":2,"submittedBy":{"_id":"65cb41d20b3bd8f5ce0fcdba","avatarUrl":"/avatars/351f1afbfba9f71f0c3760b046e4fccd.svg","fullname":"William Gitta Lugoloobi","name":"CoffeeGitta","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.08519","authors":[{"_id":"698bd6a96052d3bed963097a","user":{"_id":"63b91ec5f270ad02f61327b0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1673076377130-noauth.jpeg","isPro":false,"fullname":"Yunhui Liu","user":"Cloudy1225","type":"user"},"name":"Yunhui Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:22.049Z","hidden":false},{"_id":"698bd6a96052d3bed963097b","name":"Pengyu Qiu","hidden":false},{"_id":"698bd6a96052d3bed963097c","name":"Yu Xing","hidden":false},{"_id":"698bd6a96052d3bed963097d","name":"Yongchao Liu","hidden":false},{"_id":"698bd6a96052d3bed963097e","name":"Peng Du","hidden":false},{"_id":"698bd6a96052d3bed963097f","name":"Chuntao Hong","hidden":false},{"_id":"698bd6a96052d3bed9630980","name":"Jiajun Zheng","hidden":false},{"_id":"698bd6a96052d3bed9630981","name":"Tao Zheng","hidden":false},{"_id":"698bd6a96052d3bed9630982","name":"Tieke He","hidden":false}],"publishedAt":"2026-02-09T11:07:24.000Z","submittedOnDailyAt":"2026-02-11T08:55:27.000Z","title":"Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering","submittedOnDailyBy":{"_id":"63b91ec5f270ad02f61327b0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1673076377130-noauth.jpeg","isPro":false,"fullname":"Yunhui Liu","user":"Cloudy1225","type":"user"},"summary":"Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).","upvotes":1,"discussionId":"698bd6aa6052d3bed9630983","projectPage":"https://pyagc.readthedocs.io","githubRepo":"https://github.com/Cloudy1225/PyAGC","githubRepoAddedBy":"user","ai_summary":"PyAGC presents a production-ready benchmark and library for attributed graph clustering that addresses limitations of current research through scalable, memory-efficient implementations and comprehensive evaluation protocols.","ai_keywords":["attributed graph clustering","graph-structured data","structural topology","node attributes","unsupervised task","fraud detection","user segmentation","homophily","full-batch training","mini-batch implementations","Encode-Cluster-Optimize framework","unsupervised structural metrics","efficiency profiling","high-stakes industrial workflows"],"githubStars":23,"organization":{"_id":"67c1d682826160b28f778510","name":"antgroup","fullname":"Ant Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}},"publishedAt":"2026-02-09T06:07:24.000Z","title":"Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering","summary":"Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08519.png","numComments":2,"submittedBy":{"_id":"63b91ec5f270ad02f61327b0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1673076377130-noauth.jpeg","fullname":"Yunhui Liu","name":"Cloudy1225","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"67c1d682826160b28f778510","name":"antgroup","fullname":"Ant Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"},"isAuthorParticipating":true}]