[{"paper":{"id":"2602.10604","authors":[{"_id":"698d417065c0d15a6d162026","name":"Ailin Huang","hidden":false},{"_id":"698d417065c0d15a6d162027","name":"Ang Li","hidden":false},{"_id":"698d417065c0d15a6d162028","name":"Aobo Kong","hidden":false},{"_id":"698d417065c0d15a6d162029","name":"Bin Wang","hidden":false},{"_id":"698d417065c0d15a6d16202a","name":"Binxing Jiao","hidden":false},{"_id":"698d417065c0d15a6d16202b","name":"Bo Dong","hidden":false},{"_id":"698d417065c0d15a6d16202c","name":"Bojun Wang","hidden":false},{"_id":"698d417065c0d15a6d16202d","name":"Boyu Chen","hidden":false},{"_id":"698d417065c0d15a6d16202e","name":"Brian Li","hidden":false},{"_id":"698d417065c0d15a6d16202f","name":"Buyun Ma","hidden":false},{"_id":"698d417065c0d15a6d162030","name":"Chang Su","hidden":false},{"_id":"698d417065c0d15a6d162031","name":"Changxin Miao","hidden":false},{"_id":"698d417065c0d15a6d162032","name":"Changyi Wan","hidden":false},{"_id":"698d417065c0d15a6d162033","name":"Chao Lou","hidden":false},{"_id":"698d417065c0d15a6d162034","name":"Chen Hu","hidden":false},{"_id":"698d417065c0d15a6d162035","name":"Chen Xu","hidden":false},{"_id":"698d417065c0d15a6d162036","name":"Chenfeng Yu","hidden":false},{"_id":"698d417065c0d15a6d162037","name":"Chengting Feng","hidden":false},{"_id":"698d417065c0d15a6d162038","name":"Chengyuan Yao","hidden":false},{"_id":"698d417065c0d15a6d162039","name":"Chunrui Han","hidden":false},{"_id":"698d417065c0d15a6d16203a","name":"Dan Ma","hidden":false},{"_id":"698d417065c0d15a6d16203b","name":"Dapeng Shi","hidden":false},{"_id":"698d417065c0d15a6d16203c","name":"Daxin Jiang","hidden":false},{"_id":"698d417065c0d15a6d16203d","name":"Dehua Ma","hidden":false},{"_id":"698d417065c0d15a6d16203e","name":"Deshan Sun","hidden":false},{"_id":"698d417065c0d15a6d16203f","name":"Di Qi","hidden":false},{"_id":"698d417065c0d15a6d162040","name":"Enle Liu","hidden":false},{"_id":"698d417065c0d15a6d162041","name":"Fajie Zhang","hidden":false},{"_id":"698d417065c0d15a6d162042","name":"Fanqi Wan","hidden":false},{"_id":"698d417065c0d15a6d162043","name":"Guanzhe Huang","hidden":false},{"_id":"698d417065c0d15a6d162044","name":"Gulin Yan","hidden":false},{"_id":"698d417065c0d15a6d162045","name":"Guoliang Cao","hidden":false},{"_id":"698d417065c0d15a6d162046","name":"Guopeng Li","hidden":false},{"_id":"698d417065c0d15a6d162047","name":"Han Cheng","hidden":false},{"_id":"698d417065c0d15a6d162048","name":"Hangyu Guo","hidden":false},{"_id":"698d417065c0d15a6d162049","user":{"_id":"64b7874b9f5987572ca28461","avatarUrl":"/avatars/d24ee0a6329ff93936aa7829481e2046.svg","isPro":false,"fullname":"hanshanzhang","user":"brain-zhang","type":"user"},"name":"Hanshan Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:56:52.602Z","hidden":false},{"_id":"698d417065c0d15a6d16204a","name":"Hao Nie","hidden":false},{"_id":"698d417065c0d15a6d16204b","name":"Haonan Jia","hidden":false},{"_id":"698d417065c0d15a6d16204c","name":"Haoran Lv","hidden":false},{"_id":"698d417065c0d15a6d16204d","name":"Hebin Zhou","hidden":false},{"_id":"698d417065c0d15a6d16204e","name":"Hekun Lv","hidden":false},{"_id":"698d417065c0d15a6d16204f","name":"Heng Wang","hidden":false},{"_id":"698d417065c0d15a6d162050","name":"Heung-Yeung Shum","hidden":false},{"_id":"698d417065c0d15a6d162051","name":"Hongbo Huang","hidden":false},{"_id":"698d417065c0d15a6d162052","name":"Hongbo Peng","hidden":false},{"_id":"698d417065c0d15a6d162053","name":"Hongyu Zhou","hidden":false},{"_id":"698d417065c0d15a6d162054","name":"Hongyuan Wang","hidden":false},{"_id":"698d417065c0d15a6d162055","name":"Houyong Chen","hidden":false},{"_id":"698d417065c0d15a6d162056","name":"Huangxi Zhu","hidden":false},{"_id":"698d417065c0d15a6d162057","name":"Huimin Wu","hidden":false},{"_id":"698d417065c0d15a6d162058","name":"Huiyong Guo","hidden":false},{"_id":"698d417065c0d15a6d162059","name":"Jia Wang","hidden":false},{"_id":"698d417065c0d15a6d16205a","name":"Jian Zhou","hidden":false},{"_id":"698d417065c0d15a6d16205b","name":"Jianjian Sun","hidden":false},{"_id":"698d417065c0d15a6d16205c","name":"Jiaoren Wu","hidden":false},{"_id":"698d417065c0d15a6d16205d","name":"Jiaran Zhang","hidden":false},{"_id":"698d417065c0d15a6d16205e","name":"Jiashu Lv","hidden":false},{"_id":"698d417065c0d15a6d16205f","name":"Jiashuo Liu","hidden":false},{"_id":"698d417065c0d15a6d162060","name":"Jiayi Fu","hidden":false},{"_id":"698d417065c0d15a6d162061","name":"Jiayu Liu","hidden":false},{"_id":"698d417065c0d15a6d162062","name":"Jie Cheng","hidden":false},{"_id":"698d417065c0d15a6d162063","name":"Jie Luo","hidden":false},{"_id":"698d417065c0d15a6d162064","name":"Jie Yang","hidden":false},{"_id":"698d417065c0d15a6d162065","name":"Jie Zhou","hidden":false},{"_id":"698d417065c0d15a6d162066","name":"Jieyi Hou","hidden":false},{"_id":"698d417065c0d15a6d162067","name":"Jing Bai","hidden":false},{"_id":"698d417065c0d15a6d162068","user":{"_id":"625026b7d2d191ac43320c5e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg","isPro":false,"fullname":"Jingcheng Hu","user":"reign12","type":"user"},"name":"Jingcheng Hu","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:37.335Z","hidden":false},{"_id":"698d417065c0d15a6d162069","name":"Jingjing Xie","hidden":false},{"_id":"698d417065c0d15a6d16206a","name":"Jingwei Wu","hidden":false},{"_id":"698d417065c0d15a6d16206b","name":"Jingyang Zhang","hidden":false},{"_id":"698d417065c0d15a6d16206c","name":"Jishi Zhou","hidden":false},{"_id":"698d417065c0d15a6d16206d","name":"Junfeng Liu","hidden":false},{"_id":"698d417065c0d15a6d16206e","name":"Junzhe Lin","hidden":false},{"_id":"698d417065c0d15a6d16206f","name":"Ka Man Lo","hidden":false},{"_id":"698d417065c0d15a6d162070","name":"Kai Liang","hidden":false},{"_id":"698d417065c0d15a6d162071","name":"Kaibo Liu","hidden":false},{"_id":"698d417065c0d15a6d162072","name":"Kaijun Tan","hidden":false},{"_id":"698d417065c0d15a6d162073","user":{"_id":"66668c591964b6188ee310c2","avatarUrl":"/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg","isPro":false,"fullname":"Kaiwen Yan","user":"linrany","type":"user"},"name":"Kaiwen Yan","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:56:58.524Z","hidden":false},{"_id":"698d417065c0d15a6d162074","name":"Kaixiang Li","hidden":false},{"_id":"698d417065c0d15a6d162075","name":"Kang An","hidden":false},{"_id":"698d417065c0d15a6d162076","user":{"_id":"658a810665df457a55ffcd04","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg","isPro":false,"fullname":"Linkangheng","user":"Kangheng","type":"user"},"name":"Kangheng Lin","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:56:56.339Z","hidden":false},{"_id":"698d417065c0d15a6d162077","name":"Lei Yang","hidden":false},{"_id":"698d417065c0d15a6d162078","name":"Liang Lv","hidden":false},{"_id":"698d417065c0d15a6d162079","name":"Liang Zhao","hidden":false},{"_id":"698d417065c0d15a6d16207a","name":"Liangyu Chen","hidden":false},{"_id":"698d417065c0d15a6d16207b","name":"Lieyu Shi","hidden":false},{"_id":"698d417065c0d15a6d16207c","name":"Liguo Tan","hidden":false},{"_id":"698d417065c0d15a6d16207d","name":"Lin Lin","hidden":false},{"_id":"698d417065c0d15a6d16207e","name":"Lina Chen","hidden":false},{"_id":"698d417065c0d15a6d16207f","name":"Luck Ma","hidden":false},{"_id":"698d417065c0d15a6d162080","name":"Mengqiang Ren","hidden":false},{"_id":"698d417065c0d15a6d162081","name":"Michael Li","hidden":false},{"_id":"698d417065c0d15a6d162082","name":"Ming Li","hidden":false},{"_id":"698d417065c0d15a6d162083","name":"Mingliang Li","hidden":false},{"_id":"698d417065c0d15a6d162084","name":"Mingming Zhang","hidden":false},{"_id":"698d417065c0d15a6d162085","name":"Mingrui Chen","hidden":false},{"_id":"698d417065c0d15a6d162086","name":"Mitt Huang","hidden":false},{"_id":"698d417065c0d15a6d162087","name":"Na Wang","hidden":false},{"_id":"698d417065c0d15a6d162088","name":"Peng Liu","hidden":false},{"_id":"698d417065c0d15a6d162089","name":"Qi Han","hidden":false},{"_id":"698d417065c0d15a6d16208a","name":"Qian Zhao","hidden":false},{"_id":"698d417065c0d15a6d16208b","name":"Qinglin He","hidden":false},{"_id":"698d417065c0d15a6d16208c","name":"Qinxin Du","hidden":false},{"_id":"698d417065c0d15a6d16208d","name":"Qiuping Wu","hidden":false},{"_id":"698d417065c0d15a6d16208e","name":"Quan Sun","hidden":false},{"_id":"698d417065c0d15a6d16208f","name":"Rongqiu Yang","hidden":false},{"_id":"698d417065c0d15a6d162090","name":"Ruihang Miao","hidden":false},{"_id":"698d417065c0d15a6d162091","name":"Ruixin Han","hidden":false},{"_id":"698d417065c0d15a6d162092","name":"Ruosi Wan","hidden":false},{"_id":"698d417065c0d15a6d162093","name":"Ruyan Guo","hidden":false},{"_id":"698d417065c0d15a6d162094","name":"Shan Wang","hidden":false},{"_id":"698d417065c0d15a6d162095","name":"Shaoliang Pang","hidden":false},{"_id":"698d417065c0d15a6d162096","name":"Shaowen Yang","hidden":false},{"_id":"698d417065c0d15a6d162097","name":"Shengjie Fan","hidden":false},{"_id":"698d417065c0d15a6d162098","name":"Shijie Shang","hidden":false},{"_id":"698d417065c0d15a6d162099","name":"Shiliang Yang","hidden":false},{"_id":"698d417065c0d15a6d16209a","name":"Shiwei Li","hidden":false},{"_id":"698d417065c0d15a6d16209b","name":"Shuangshuang Tian","hidden":false},{"_id":"698d417065c0d15a6d16209c","name":"Siqi Liu","hidden":false},{"_id":"698d417065c0d15a6d16209d","name":"Siye Wu","hidden":false},{"_id":"698d417065c0d15a6d16209e","name":"Siyu Chen","hidden":false},{"_id":"698d417065c0d15a6d16209f","name":"Song Yuan","hidden":false},{"_id":"698d417065c0d15a6d1620a0","name":"Tiancheng Cao","hidden":false},{"_id":"698d417065c0d15a6d1620a1","name":"Tianchi Yue","hidden":false},{"_id":"698d417065c0d15a6d1620a2","name":"Tianhao Cheng","hidden":false},{"_id":"698d417065c0d15a6d1620a3","name":"Tianning Li","hidden":false},{"_id":"698d417065c0d15a6d1620a4","name":"Tingdan Luo","hidden":false},{"_id":"698d417065c0d15a6d1620a5","name":"Wang You","hidden":false},{"_id":"698d417065c0d15a6d1620a6","name":"Wei Ji","hidden":false},{"_id":"698d417065c0d15a6d1620a7","name":"Wei Yuan","hidden":false},{"_id":"698d417065c0d15a6d1620a8","name":"Wei Zhang","hidden":false},{"_id":"698d417065c0d15a6d1620a9","name":"Weibo Wu","hidden":false},{"_id":"698d417065c0d15a6d1620aa","user":{"_id":"6657620ea496f7fcb67c3871","avatarUrl":"/avatars/54fef1c835e6f6b478652d438a140d45.svg","isPro":false,"fullname":"xieweihao","user":"chalengr","type":"user"},"name":"Weihao Xie","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:56:48.216Z","hidden":false},{"_id":"698d417065c0d15a6d1620ab","name":"Wen Sun","hidden":false},{"_id":"698d417065c0d15a6d1620ac","name":"Wenjin Deng","hidden":false},{"_id":"698d417065c0d15a6d1620ad","user":{"_id":"650c04795510464e85b47470","avatarUrl":"/avatars/98c194e77826b928c49659849f466dad.svg","isPro":false,"fullname":"wen","user":"zhengwenzhen","type":"user"},"name":"Wenzhen Zheng","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:56:45.930Z","hidden":false},{"_id":"698d417065c0d15a6d1620ae","name":"Wuxun Xie","hidden":false},{"_id":"698d417065c0d15a6d1620af","name":"Xiangfeng Wang","hidden":false},{"_id":"698d417065c0d15a6d1620b0","name":"Xiangwen Kong","hidden":false},{"_id":"698d417065c0d15a6d1620b1","name":"Xiangyu Liu","hidden":false},{"_id":"698d417065c0d15a6d1620b2","name":"Xiangyu Zhang","hidden":false},{"_id":"698d417065c0d15a6d1620b3","name":"Xiaobo Yang","hidden":false},{"_id":"698d417065c0d15a6d1620b4","name":"Xiaojia Liu","hidden":false},{"_id":"698d417065c0d15a6d1620b5","name":"Xiaolan Yuan","hidden":false},{"_id":"698d417065c0d15a6d1620b6","name":"Xiaoran Jiao","hidden":false},{"_id":"698d417065c0d15a6d1620b7","name":"Xiaoxiao Ren","hidden":false},{"_id":"698d417065c0d15a6d1620b8","name":"Xiaoyun Zhang","hidden":false},{"_id":"698d417065c0d15a6d1620b9","name":"Xin Li","hidden":false},{"_id":"698d417065c0d15a6d1620ba","name":"Xin Liu","hidden":false},{"_id":"698d417065c0d15a6d1620bb","name":"Xin Wu","hidden":false},{"_id":"698d417065c0d15a6d1620bc","name":"Xing Chen","hidden":false},{"_id":"698d417065c0d15a6d1620bd","name":"Xingping Yang","hidden":false},{"_id":"698d417065c0d15a6d1620be","name":"Xinran Wang","hidden":false},{"_id":"698d417065c0d15a6d1620bf","name":"Xu Zhao","hidden":false},{"_id":"698d417065c0d15a6d1620c0","user":{"_id":"64ec5b64bfb2aa06a46ff2d6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg","isPro":false,"fullname":"xuan he","user":"tpa115k31","type":"user"},"name":"Xuan He","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:56:36.240Z","hidden":false},{"_id":"698d417065c0d15a6d1620c1","name":"Xuanti Feng","hidden":false},{"_id":"698d417065c0d15a6d1620c2","name":"Xuedan Cai","hidden":false},{"_id":"698d417065c0d15a6d1620c3","name":"Xuqiang Zhou","hidden":false},{"_id":"698d417065c0d15a6d1620c4","name":"Yanbo Yu","hidden":false},{"_id":"698d417065c0d15a6d1620c5","name":"Yang Li","hidden":false},{"_id":"698d417065c0d15a6d1620c6","name":"Yang Xu","hidden":false},{"_id":"698d417065c0d15a6d1620c7","name":"Yanlin Lai","hidden":false},{"_id":"698d417065c0d15a6d1620c8","name":"Yanming Xu","hidden":false},{"_id":"698d417065c0d15a6d1620c9","name":"Yaoyu Wang","hidden":false},{"_id":"698d417065c0d15a6d1620ca","name":"Yeqing Shen","hidden":false},{"_id":"698d417065c0d15a6d1620cb","name":"Yibo Zhu","hidden":false},{"_id":"698d417065c0d15a6d1620cc","name":"Yichen Lv","hidden":false},{"_id":"698d417065c0d15a6d1620cd","name":"Yicheng Cao","hidden":false},{"_id":"698d417065c0d15a6d1620ce","name":"Yifeng Gong","hidden":false},{"_id":"698d417065c0d15a6d1620cf","name":"Yijing Yang","hidden":false},{"_id":"698d417065c0d15a6d1620d0","name":"Yikun Yang","hidden":false},{"_id":"698d417065c0d15a6d1620d1","name":"Yin Zhao","hidden":false},{"_id":"698d417065c0d15a6d1620d2","name":"Yingxiu Zhao","hidden":false},{"_id":"698d417065c0d15a6d1620d3","name":"Yinmin Zhang","hidden":false},{"_id":"698d417065c0d15a6d1620d4","name":"Yitong Zhang","hidden":false},{"_id":"698d417065c0d15a6d1620d5","name":"Yixuan Zhang","hidden":false},{"_id":"698d417065c0d15a6d1620d6","name":"Yiyang Chen","hidden":false},{"_id":"698d417065c0d15a6d1620d7","name":"Yongchi Zhao","hidden":false},{"_id":"698d417065c0d15a6d1620d8","name":"Yongshen Long","hidden":false},{"_id":"698d417065c0d15a6d1620d9","name":"Yongyao Wang","hidden":false},{"_id":"698d417065c0d15a6d1620da","name":"Yousong Guan","hidden":false},{"_id":"698d417065c0d15a6d1620db","name":"Yu Zhou","hidden":false},{"_id":"698d417065c0d15a6d1620dc","name":"Yuang Peng","hidden":false},{"_id":"698d417065c0d15a6d1620dd","name":"Yuanhao Ding","hidden":false},{"_id":"698d417065c0d15a6d1620de","name":"Yuantao Fan","hidden":false},{"_id":"698d417065c0d15a6d1620df","name":"Yuanzhen Yang","hidden":false},{"_id":"698d417065c0d15a6d1620e0","name":"Yuchu Luo","hidden":false},{"_id":"698d417065c0d15a6d1620e1","name":"Yudi Zhao","hidden":false},{"_id":"698d417065c0d15a6d1620e2","name":"Yue Peng","hidden":false},{"_id":"698d417065c0d15a6d1620e3","name":"Yueqiang Lin","hidden":false},{"_id":"698d417065c0d15a6d1620e4","name":"Yufan Lu","hidden":false},{"_id":"698d417065c0d15a6d1620e5","name":"Yuling Zhao","hidden":false},{"_id":"698d417065c0d15a6d1620e6","name":"Yunzhou Ju","hidden":false},{"_id":"698d417065c0d15a6d1620e7","name":"Yurong Zhang","hidden":false},{"_id":"698d417065c0d15a6d1620e8","name":"Yusheng Li","hidden":false},{"_id":"698d417065c0d15a6d1620e9","name":"Yuxiang Yang","hidden":false},{"_id":"698d417065c0d15a6d1620ea","name":"Yuyang Chen","hidden":false},{"_id":"698d417065c0d15a6d1620eb","name":"Yuzhu Cai","hidden":false},{"_id":"698d417065c0d15a6d1620ec","name":"Zejia Weng","hidden":false},{"_id":"698d417065c0d15a6d1620ed","name":"Zetao Hong","hidden":false},{"_id":"698d417065c0d15a6d1620ee","name":"Zexi Li","hidden":false},{"_id":"698d417065c0d15a6d1620ef","name":"Zhe Xie","hidden":false},{"_id":"698d417065c0d15a6d1620f0","name":"Zheng Ge","hidden":false},{"_id":"698d417065c0d15a6d1620f1","name":"Zheng Gong","hidden":false},{"_id":"698d417065c0d15a6d1620f2","name":"Zheng Zeng","hidden":false},{"_id":"698d417065c0d15a6d1620f3","user":{"_id":"63607ace9ddc44e710e13f0f","avatarUrl":"/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg","isPro":false,"fullname":"zy","user":"lu-vae","type":"user"},"name":"Zhenyi Lu","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:56:50.532Z","hidden":false},{"_id":"698d417065c0d15a6d1620f4","name":"Zhewei Huang","hidden":false},{"_id":"698d417065c0d15a6d1620f5","name":"Zhichao Chang","hidden":false},{"_id":"698d417065c0d15a6d1620f6","name":"Zhiguo Huang","hidden":false},{"_id":"698d417065c0d15a6d1620f7","name":"Zhiheng Hu","hidden":false},{"_id":"698d417065c0d15a6d1620f8","name":"Zidong Yang","hidden":false},{"_id":"698d417065c0d15a6d1620f9","name":"Zili Wang","hidden":false},{"_id":"698d417065c0d15a6d1620fa","name":"Ziqi Ren","hidden":false},{"_id":"698d417065c0d15a6d1620fb","name":"Zixin Zhang","hidden":false},{"_id":"698d417065c0d15a6d1620fc","name":"Zixuan Wang","hidden":false}],"publishedAt":"2026-02-11T07:53:51.000Z","submittedOnDailyAt":"2026-02-12T00:26:49.880Z","title":"Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.","upvotes":179,"discussionId":"698d417165c0d15a6d1620fd","githubRepo":"https://github.com/stepfun-ai/Step-3.5-Flash","githubRepoAddedBy":"user","ai_summary":"Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.","ai_keywords":["Mixture-of-Experts","sparse MoE","foundation model","active parameters","interleaved attention","sliding-window attention","full attention","Multi-Token Prediction","reinforcement learning","verifiable signals","preference feedback","off-policy training","self-improvement","IMO-AnswerBench","LiveCodeBench","tau2-Bench","BrowseComp","Terminal-Bench"],"githubStars":1363,"organization":{"_id":"66e43eae9d477f566f937935","name":"stepfun-ai","fullname":"StepFun","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}},"publishedAt":"2026-02-11T02:53:51.000Z","title":"Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters","summary":"We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png","numComments":5,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"66e43eae9d477f566f937935","name":"stepfun-ai","fullname":"StepFun","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08099","authors":[{"_id":"698b10fb1b2dc6b37d61b3f7","user":{"_id":"66c704d5c797952bc2360ecb","avatarUrl":"/avatars/11e999a17c043c100571d8df0d966fdf.svg","isPro":false,"fullname":"issart","user":"issart12345","type":"user"},"name":"Issar Tzachor","status":"claimed_verified","statusLastChangedAt":"2026-02-12T20:23:27.323Z","hidden":false},{"_id":"698b10fb1b2dc6b37d61b3f8","name":"Dvir Samuel","hidden":false},{"_id":"698b10fb1b2dc6b37d61b3f9","name":"Rami Ben-Ari","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66c704d5c797952bc2360ecb/a1yLdiaoRaoLULO5VW1sE.mp4"],"publishedAt":"2026-02-08T19:39:32.000Z","submittedOnDailyAt":"2026-02-12T12:21:25.358Z","title":"VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval","submittedOnDailyBy":{"_id":"66c704d5c797952bc2360ecb","avatarUrl":"/avatars/11e999a17c043c100571d8df0d966fdf.svg","isPro":false,"fullname":"issart","user":"issart12345","type":"user"},"summary":"Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.","upvotes":121,"discussionId":"698b10fc1b2dc6b37d61b3fa","projectPage":"https://iyttor.github.io/VidVec","ai_summary":"Generative multimodal large language models are adapted for video-text embedding and retrieval through intermediate-layer analysis and text-based alignment without visual supervision.","ai_keywords":["generative Multimodal Large Language Models","Video Foundation Models","zero-shot retrieval","intermediate-layer embeddings","text-based alignment","video-text embedding","video retrieval benchmarks"]},"publishedAt":"2026-02-08T14:39:32.000Z","title":"VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval","summary":"Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66c704d5c797952bc2360ecb/a1yLdiaoRaoLULO5VW1sE.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08099.png","numComments":2,"submittedBy":{"_id":"66c704d5c797952bc2360ecb","avatarUrl":"/avatars/11e999a17c043c100571d8df0d966fdf.svg","fullname":"issart","name":"issart12345","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.11144","authors":[{"_id":"698d4ad765c0d15a6d162188","name":"Ruichuan An","hidden":false},{"_id":"698d4ad765c0d15a6d162189","name":"Sihan Yang","hidden":false},{"_id":"698d4ad765c0d15a6d16218a","name":"Ziyu Guo","hidden":false},{"_id":"698d4ad765c0d15a6d16218b","name":"Wei Dai","hidden":false},{"_id":"698d4ad765c0d15a6d16218c","user":{"_id":"67153c67fc14a252609746c9","avatarUrl":"/avatars/03f31f0f1c61a1241f128a3cce39bb6a.svg","isPro":false,"fullname":"zijun shen","user":"chawuciren","type":"user"},"name":"Zijun Shen","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:48:58.570Z","hidden":false},{"_id":"698d4ad765c0d15a6d16218d","user":{"_id":"65ddea8b2d26e59a5a33330f","avatarUrl":"/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg","isPro":false,"fullname":"li haodong","user":"mickyhimself","type":"user"},"name":"Haodong Li","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:48:50.139Z","hidden":false},{"_id":"698d4ad765c0d15a6d16218e","name":"Renrui Zhang","hidden":false},{"_id":"698d4ad765c0d15a6d16218f","name":"Xinyu Wei","hidden":false},{"_id":"698d4ad765c0d15a6d162190","user":{"_id":"675bf9e80f2c2a510df55fc6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ScFdLKNOBP-tp0t6bYxps.png","isPro":false,"fullname":"GuopengLI","user":"GuopengLi","type":"user"},"name":"Guopeng Li","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:49:15.009Z","hidden":false},{"_id":"698d4ad765c0d15a6d162191","name":"Wenshan Wu","hidden":false},{"_id":"698d4ad765c0d15a6d162192","name":"Wentao Zhang","hidden":false}],"publishedAt":"2026-02-11T18:55:54.000Z","submittedOnDailyAt":"2026-02-12T01:06:56.691Z","title":"GENIUS: Generative Fluid Intelligence Evaluation Suite","submittedOnDailyBy":{"_id":"65ddea8b2d26e59a5a33330f","avatarUrl":"/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg","isPro":false,"fullname":"li haodong","user":"mickyhimself","type":"user"},"summary":"Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.","upvotes":53,"discussionId":"698d4ad865c0d15a6d162193","ai_summary":"GENIUS evaluates multimodal models' generative fluid intelligence through pattern induction, constraint execution, and contextual adaptation tasks, revealing deficiencies in context comprehension rather than generative capability.","ai_keywords":["Unified Multimodal Models","Generative Fluid Intelligence","Inducing Implicit Patterns","Executing Ad-hoc Constraints","Adapting to Contextual Knowledge","attention intervention strategy"]},"publishedAt":"2026-02-11T13:55:54.000Z","title":"GENIUS: Generative Fluid Intelligence Evaluation Suite","summary":"Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GEN Fluid Intelligence EvalUation Suite). We formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: https://github.com/arctanxarc/GENIUS{https://github.com/arctanxarc/GENIUS}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11144.png","numComments":2,"submittedBy":{"_id":"65ddea8b2d26e59a5a33330f","avatarUrl":"/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg","fullname":"li haodong","name":"mickyhimself","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":0,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.11124","authors":[{"_id":"698d486865c0d15a6d162162","user":{"_id":"6570977f87a92b76922c9950","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg","isPro":false,"fullname":"Tianyi Xiong","user":"txiong23","type":"user"},"name":"Tianyi Xiong","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:23.141Z","hidden":false},{"_id":"698d486865c0d15a6d162163","name":"Shihao Wang","hidden":false},{"_id":"698d486865c0d15a6d162164","name":"Guilin Liu","hidden":false},{"_id":"698d486865c0d15a6d162165","name":"Yi Dong","hidden":false},{"_id":"698d486865c0d15a6d162166","name":"Ming Li","hidden":false},{"_id":"698d486865c0d15a6d162167","name":"Heng Huang","hidden":false},{"_id":"698d486865c0d15a6d162168","name":"Jan Kautz","hidden":false},{"_id":"698d486865c0d15a6d162169","user":{"_id":"66c8037c737ba92ae3fe0322","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c8037c737ba92ae3fe0322/WR_Yh5DWOVVh7IFlF24NM.jpeg","isPro":true,"fullname":"Zhiding Yu","user":"Zhiding","type":"user"},"name":"Zhiding Yu","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:25.160Z","hidden":false}],"publishedAt":"2026-02-11T18:35:39.000Z","submittedOnDailyAt":"2026-02-12T02:07:20.427Z","title":"PhyCritic: Multimodal Critic Models for Physical AI","submittedOnDailyBy":{"_id":"6570977f87a92b76922c9950","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg","isPro":false,"fullname":"Tianyi Xiong","user":"txiong23","type":"user"},"summary":"With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.","upvotes":51,"discussionId":"698d486865c0d15a6d16216a","projectPage":"https://research.nvidia.com/labs/lpr/phycritic","ai_summary":"PhyCritic is a multimodal critic model designed for physical AI tasks through a two-stage RLVR pipeline that enhances perception and reasoning capabilities.","ai_keywords":["multimodal models","physical AI","RLVR pipeline","physical skill warmup stage","self-referential critic finetuning","perception","reasoning","policy model"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2026-02-11T13:35:39.000Z","title":"PhyCritic: Multimodal Critic Models for Physical AI","summary":"With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11124.png","numComments":2,"submittedBy":{"_id":"6570977f87a92b76922c9950","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg","fullname":"Tianyi Xiong","name":"txiong23","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":10,"isUserFollowing":false},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.04935","authors":[{"_id":"698c7b53eb12ea74539168e7","user":{"_id":"68e8b2658b33409d96ca711c","avatarUrl":"/avatars/abea0be180fc111989185e3d7aeaeb88.svg","isPro":false,"fullname":"wyj","user":"wangyoujin","type":"user"},"name":"Youjin Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:24.472Z","hidden":false},{"_id":"698c7b53eb12ea74539168e8","name":"Run Zhou","hidden":false},{"_id":"698c7b53eb12ea74539168e9","name":"Rong Fu","hidden":false},{"_id":"698c7b53eb12ea74539168ea","name":"Shuaishuai Cao","hidden":false},{"_id":"698c7b53eb12ea74539168eb","name":"Hongwei Zeng","hidden":false},{"_id":"698c7b53eb12ea74539168ec","name":"Jiaxuan Lu","hidden":false},{"_id":"698c7b53eb12ea74539168ed","name":"Sicheng Fan","hidden":false},{"_id":"698c7b53eb12ea74539168ee","name":"Jiaqiao Zhao","hidden":false},{"_id":"698c7b53eb12ea74539168ef","name":"Liangming Pan","hidden":false}],"publishedAt":"2026-02-04T14:20:02.000Z","submittedOnDailyAt":"2026-02-12T01:32:55.176Z","title":"ASA: Training-Free Representation Engineering for Tool-Calling Agents","submittedOnDailyBy":{"_id":"68e8b2658b33409d96ca711c","avatarUrl":"/avatars/abea0be180fc111989185e3d7aeaeb88.svg","isPro":false,"fullname":"wyj","user":"wangyoujin","type":"user"},"summary":"Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations, yet the model remains conservative in entering tool mode, revealing a representation-behavior gap. We propose Activation Steering Adapter (ASA), a training-free, inference-time controller that performs a single-shot mid-layer intervention and targets tool domains via a router-conditioned mixture of steering vectors with a probe-guided signed gate to amplify true intent while suppressing spurious triggers. On MTU-Bench with Qwen2.5-1.5B, ASA improves strict tool-use F1 from 0.18 to 0.50 while reducing the false positive rate from 0.15 to 0.05, using only about 20KB of portable assets and no weight updates.","upvotes":40,"discussionId":"698c7b53eb12ea74539168f0","ai_summary":"A training-free method called Activation Steering Adapter corrects tool calling behavior in language models by using mid-layer activation interventions guided by a probe and router-conditioned steering vectors.","ai_keywords":["tool calling","activation steering adapter","mid-layer activations","representation-behavior gap","steering vectors","probe-guided signed gate","continual parameter-efficient fine-tuning","distribution shift","strict parsers","false positive rate","Qwen2.5-1.5B"]},"publishedAt":"2026-02-04T09:20:02.000Z","title":"ASA: Training-Free Representation Engineering for Tool-Calling Agents","summary":"Adapting LLM agents to domain-specific tool calling remains notably brittle under evolving interfaces. Prompt and schema engineering is easy to deploy but often fragile under distribution shift and strict parsers, while continual parameter-efficient fine-tuning improves reliability at the cost of training, maintenance, and potential forgetting. We identify a critical Lazy Agent failure mode where tool necessity is nearly perfectly decodable from mid-layer activations, yet the model remains conservative in entering tool mode, revealing a representation-behavior gap. We propose Activation Steering Adapter (ASA), a training-free, inference-time controller that performs a single-shot mid-layer intervention and targets tool domains via a router-conditioned mixture of steering vectors with a probe-guided signed gate to amplify true intent while suppressing spurious triggers. On MTU-Bench with Qwen2.5-1.5B, ASA improves strict tool-use F1 from 0.18 to 0.50 while reducing the false positive rate from 0.15 to 0.05, using only about 20KB of portable assets and no weight updates.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04935.png","numComments":2,"submittedBy":{"_id":"68e8b2658b33409d96ca711c","avatarUrl":"/avatars/abea0be180fc111989185e3d7aeaeb88.svg","fullname":"wyj","name":"wangyoujin","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.10177","authors":[{"_id":"698d426265c0d15a6d162113","name":"Tony Feng","hidden":false},{"_id":"698d426265c0d15a6d162114","name":"Trieu H. Trinh","hidden":false},{"_id":"698d426265c0d15a6d162115","name":"Garrett Bingham","hidden":false},{"_id":"698d426265c0d15a6d162116","name":"Dawsen Hwang","hidden":false},{"_id":"698d426265c0d15a6d162117","name":"Yuri Chervonyi","hidden":false},{"_id":"698d426265c0d15a6d162118","name":"Junehyuk Jung","hidden":false},{"_id":"698d426265c0d15a6d162119","name":"Joonkyung Lee","hidden":false},{"_id":"698d426265c0d15a6d16211a","name":"Carlo Pagano","hidden":false},{"_id":"698d426265c0d15a6d16211b","name":"Sang-hyun Kim","hidden":false},{"_id":"698d426265c0d15a6d16211c","name":"Federico Pasqualotto","hidden":false},{"_id":"698d426265c0d15a6d16211d","name":"Sergei Gukov","hidden":false},{"_id":"698d426265c0d15a6d16211e","name":"Jonathan N. Lee","hidden":false},{"_id":"698d426265c0d15a6d16211f","name":"Junsu Kim","hidden":false},{"_id":"698d426265c0d15a6d162120","name":"Kaiying Hou","hidden":false},{"_id":"698d426265c0d15a6d162121","name":"Golnaz Ghiasi","hidden":false},{"_id":"698d426265c0d15a6d162122","name":"Yi Tay","hidden":false},{"_id":"698d426265c0d15a6d162123","name":"YaGuang Li","hidden":false},{"_id":"698d426265c0d15a6d162124","name":"Chenkai Kuang","hidden":false},{"_id":"698d426265c0d15a6d162125","name":"Yuan Liu","hidden":false},{"_id":"698d426265c0d15a6d162126","name":"Hanzhao","hidden":false},{"_id":"698d426265c0d15a6d162127","name":"Lin","hidden":false},{"_id":"698d426265c0d15a6d162128","name":"Evan Zheran Liu","hidden":false},{"_id":"698d426265c0d15a6d162129","name":"Nigamaa Nayakanti","hidden":false},{"_id":"698d426265c0d15a6d16212a","name":"Xiaomeng Yang","hidden":false},{"_id":"698d426265c0d15a6d16212b","name":"Heng-tze Cheng","hidden":false},{"_id":"698d426265c0d15a6d16212c","name":"Demis Hassabis","hidden":false},{"_id":"698d426265c0d15a6d16212d","name":"Koray Kavukcuoglu","hidden":false},{"_id":"698d426265c0d15a6d16212e","name":"Quoc V. Le","hidden":false},{"_id":"698d426265c0d15a6d16212f","name":"Thang Luong","hidden":false}],"publishedAt":"2026-02-10T18:50:15.000Z","submittedOnDailyAt":"2026-02-12T00:30:55.790Z","title":"Towards Autonomous Mathematics Research","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.","upvotes":35,"discussionId":"698d426365c0d15a6d162130","ai_summary":"Aletheia, a math research agent, demonstrates advanced reasoning capabilities by generating and verifying solutions end-to-end in natural language, achieving autonomous research outcomes from Olympiad problems to PhD-level exercises and contributing to AI-assisted mathematical research.","ai_keywords":["foundational models","reasoning systems","International Mathematical Olympiad","mathematical research","AI-assisted mathematics","autonomous research","human-AI collaboration","proof construction","inference-time scaling law","tool use","natural language processing","mathematical verification"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-10T13:50:15.000Z","title":"Towards Autonomous Mathematics Research","summary":"Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom's Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest codifying standard levels quantifying autonomy and novelty of AI-assisted results. We conclude with reflections on human-AI collaboration in mathematics.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10177.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.10560","authors":[{"_id":"698d420e65c0d15a6d162108","name":"Leheng Sheng","hidden":false},{"_id":"698d420e65c0d15a6d162109","name":"Yongtao Zhang","hidden":false},{"_id":"698d420e65c0d15a6d16210a","name":"Wenchang Ma","hidden":false},{"_id":"698d420e65c0d15a6d16210b","user":{"_id":"63edd2d1f765928ceeb49057","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png","isPro":false,"fullname":"Yaorui SHI","user":"yrshi","type":"user"},"name":"Yaorui Shi","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:35.189Z","hidden":false},{"_id":"698d420e65c0d15a6d16210c","name":"Ting Huang","hidden":false},{"_id":"698d420e65c0d15a6d16210d","name":"Xiang Wang","hidden":false},{"_id":"698d420e65c0d15a6d16210e","name":"An Zhang","hidden":false},{"_id":"698d420e65c0d15a6d16210f","name":"Ke Shen","hidden":false},{"_id":"698d420e65c0d15a6d162110","name":"Tat-Seng Chua","hidden":false}],"publishedAt":"2026-02-11T06:14:53.000Z","submittedOnDailyAt":"2026-02-12T00:29:30.545Z","title":"When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals r^{update} and r^{exit} within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\\% times inference speed acceleration.","upvotes":28,"discussionId":"698d420f65c0d15a6d162111","projectPage":"https://alphalab-ustc.github.io/grumem-alphalab/","ai_summary":"GRU-Mem addresses long-context reasoning challenges in LLMs by incorporating text-controlled gates and reinforcement learning rewards to stabilize memory updates and improve computational efficiency.","ai_keywords":["long-context reasoning","large language models","recurrent memory update","text-controlled gates","reward signals","reinforcement learning","memory explosion","exit mechanism","inference speed acceleration"],"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}},"publishedAt":"2026-02-11T01:14:53.000Z","title":"When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning","summary":"While reasoning over long context is crucial for various real-world applications, it remains challenging for large language models (LLMs) as they suffer from performance degradation as the context length grows. Recent work MemAgent has tried to tackle this by processing context chunk-by-chunk in an RNN-like loop and updating a textual memory for final answering. However, this naive recurrent memory update faces two crucial drawbacks: (i) memory can quickly explode because it can update indiscriminately, even on evidence-free chunks; and (ii) the loop lacks an exit mechanism, leading to unnecessary computation after even sufficient evidence is collected. To address these issues, we propose GRU-Mem, which incorporates two text-controlled gates for more stable and efficient long-context reasoning. Specifically, in GRU-Mem, the memory only updates when the update gate is open and the recurrent loop will exit immediately once the exit gate is open. To endow the model with such capabilities, we introduce two reward signals r^{update} and r^{exit} within end-to-end RL, rewarding the correct updating and exiting behaviors respectively. Experiments on various long-context reasoning tasks demonstrate the effectiveness and efficiency of GRU-Mem, which generally outperforms the vanilla MemAgent with up to 400\\% times inference speed acceleration.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10560.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"67d1140985ea0644e2f14b99","name":"ByteDance-Seed","fullname":"ByteDance Seed","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.10622","authors":[{"_id":"698d5d9565c0d15a6d162214","user":{"_id":"67acb56faee29e2e5c1c41ec","avatarUrl":"/avatars/b47094266fe70d10786da845ae2ade2c.svg","isPro":false,"fullname":"Jiahao Yuan","user":"Jhcircle","type":"user"},"name":"Jiahao Yuan","status":"admin_assigned","statusLastChangedAt":"2026-02-12T16:47:44.410Z","hidden":false},{"_id":"698d5d9565c0d15a6d162215","name":"Yike Xu","hidden":false},{"_id":"698d5d9565c0d15a6d162216","name":"Jinyong Wen","hidden":false},{"_id":"698d5d9565c0d15a6d162217","name":"Baokun Wang","hidden":false},{"_id":"698d5d9565c0d15a6d162218","name":"Yang Chen","hidden":false},{"_id":"698d5d9565c0d15a6d162219","name":"Xiaotong Lin","hidden":false},{"_id":"698d5d9565c0d15a6d16221a","name":"Wuliang Huang","hidden":false},{"_id":"698d5d9565c0d15a6d16221b","name":"Ziyi Gao","hidden":false},{"_id":"698d5d9565c0d15a6d16221c","name":"Xing Fu","hidden":false},{"_id":"698d5d9565c0d15a6d16221d","name":"Yu Cheng","hidden":false},{"_id":"698d5d9565c0d15a6d16221e","name":"Weiqiang Wang","hidden":false}],"publishedAt":"2026-02-11T08:12:43.000Z","submittedOnDailyAt":"2026-02-12T12:32:34.675Z","title":"How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning","submittedOnDailyBy":{"_id":"67acb56faee29e2e5c1c41ec","avatarUrl":"/avatars/b47094266fe70d10786da845ae2ade2c.svg","isPro":false,"fullname":"Jiahao Yuan","user":"Jhcircle","type":"user"},"summary":"Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.","upvotes":27,"discussionId":"698d5d9665c0d15a6d16221f","githubRepo":"https://github.com/JhCircle/Deepfind-GGSM","githubRepoAddedBy":"user","ai_summary":"Research investigates the impact of different attention masking strategies on user embedding quality in decoder-only language models, proposing a gradient-guided soft masking technique to improve training stability and representation quality for user behavior analysis.","ai_keywords":["decoder-only large language models","attention masking","causal attention","hybrid attention","bidirectional attention","contrastive learning","gradient-guided soft masking","pre-warmup","linear scheduler","user representation learning","user cognition benchmarks"],"githubStars":2,"organization":{"_id":"67c1d682826160b28f778510","name":"antgroup","fullname":"Ant Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}},"publishedAt":"2026-02-11T03:12:43.000Z","title":"How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning","summary":"Decoder-only large language models are increasingly used as behavioral encoders for user representation learning, yet the impact of attention masking on the quality of user embeddings remains underexplored. In this work, we conduct a systematic study of causal, hybrid, and bidirectional attention masks within a unified contrastive learning framework trained on large-scale real-world Alipay data that integrates long-horizon heterogeneous user behaviors. To improve training dynamics when transitioning from causal to bidirectional attention, we propose Gradient-Guided Soft Masking, a gradient-based pre-warmup applied before a linear scheduler that gradually opens future attention during optimization. Evaluated on 9 industrial user cognition benchmarks covering prediction, preference, and marketing sensitivity tasks, our approach consistently yields more stable training and higher-quality bidirectional representations compared with causal, hybrid, and scheduler-only baselines, while remaining compatible with decoder pretraining. Overall, our findings highlight the importance of masking design and training transition in adapting decoder-only LLMs for effective user representation learning. Our code is available at https://github.com/JhCircle/Deepfind-GGSM.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10622.png","numComments":3,"submittedBy":{"_id":"67acb56faee29e2e5c1c41ec","avatarUrl":"/avatars/b47094266fe70d10786da845ae2ade2c.svg","fullname":"Jiahao Yuan","name":"Jhcircle","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"67c1d682826160b28f778510","name":"antgroup","fullname":"Ant Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.08711","authors":[{"_id":"698ab9841b2dc6b37d61b092","name":"Linli Yao","hidden":false},{"_id":"698ab9841b2dc6b37d61b093","name":"Yuancheng Wei","hidden":false},{"_id":"698ab9841b2dc6b37d61b094","name":"Yaojie Zhang","hidden":false},{"_id":"698ab9841b2dc6b37d61b095","name":"Lei Li","hidden":false},{"_id":"698ab9841b2dc6b37d61b096","name":"Xinlong Chen","hidden":false},{"_id":"698ab9841b2dc6b37d61b097","name":"Feifan Song","hidden":false},{"_id":"698ab9841b2dc6b37d61b098","name":"Ziyue Wang","hidden":false},{"_id":"698ab9841b2dc6b37d61b099","name":"Kun Ouyang","hidden":false},{"_id":"698ab9841b2dc6b37d61b09a","name":"Yuanxin Liu","hidden":false},{"_id":"698ab9841b2dc6b37d61b09b","name":"Lingpeng Kong","hidden":false},{"_id":"698ab9841b2dc6b37d61b09c","name":"Qi Liu","hidden":false},{"_id":"698ab9841b2dc6b37d61b09d","name":"Pengfei Wan","hidden":false},{"_id":"698ab9841b2dc6b37d61b09e","name":"Kun Gai","hidden":false},{"_id":"698ab9841b2dc6b37d61b09f","name":"Yuanxing Zhang","hidden":false},{"_id":"698ab9841b2dc6b37d61b0a0","name":"Xu Sun","hidden":false}],"publishedAt":"2026-02-09T14:21:58.000Z","submittedOnDailyAt":"2026-02-12T01:40:42.627Z","title":"TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions","submittedOnDailyBy":{"_id":"655ca347f426a304c6b393a1","avatarUrl":"/avatars/67f0310d59c5912d38c2ad8e6448614d.svg","isPro":false,"fullname":"Linli Yao","user":"yaolily","type":"user"},"summary":"This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.","upvotes":27,"discussionId":"698ab9851b2dc6b37d61b0a1","githubRepo":"https://github.com/yaolinli/TimeChat-Captioner","githubRepoAddedBy":"user","ai_summary":"Omni Dense Captioning introduces a six-dimensional structural schema for generating time-aware audio-visual narratives with explicit timestamps, along with a unified evaluation metric and strong baseline model.","ai_keywords":["Omni Dense Captioning","structured schema","script-like captions","TimeAware","SFT","GRPO","task-specific rewards","dense descriptions","audio-visual reasoning","temporal grounding"],"githubStars":18,"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}},"publishedAt":"2026-02-09T09:21:58.000Z","title":"TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions","summary":"This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08711.png","numComments":2,"submittedBy":{"_id":"655ca347f426a304c6b393a1","avatarUrl":"/avatars/67f0310d59c5912d38c2ad8e6448614d.svg","fullname":"Linli Yao","name":"yaolily","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08253","authors":[{"_id":"698bf9256052d3bed9630ad3","user":{"_id":"698ae59d70be6790cd1e6a53","avatarUrl":"/avatars/66125d952e7880baa4fe1635aafbc160.svg","isPro":false,"fullname":"Baoyun Zhao","user":"ZBoyn","type":"user"},"name":"Baoyun Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:17.040Z","hidden":false},{"_id":"698bf9256052d3bed9630ad4","user":{"_id":"617a39153490eedabaa5391f","avatarUrl":"/avatars/29ce0101b99bc96f81032ac6228ce878.svg","isPro":false,"fullname":"He Wang","user":"iphysresearch","type":"user"},"name":"He Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:46.758Z","hidden":false},{"_id":"698bf9256052d3bed9630ad5","name":"Liang Zeng","hidden":false}],"publishedAt":"2026-02-09T04:13:35.000Z","submittedOnDailyAt":"2026-02-12T00:10:48.214Z","title":"G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design","submittedOnDailyBy":{"_id":"698ae59d70be6790cd1e6a53","avatarUrl":"/avatars/66125d952e7880baa4fe1635aafbc160.svg","isPro":false,"fullname":"Baoyun Zhao","user":"ZBoyn","type":"user"},"summary":"While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.","upvotes":26,"discussionId":"698bf9256052d3bed9630ad6","projectPage":"https://zboyn.github.io/G-LNS/","githubRepo":"https://github.com/ZBoyn/G-LNS","githubRepoAddedBy":"user","ai_summary":"A generative evolutionary framework extends large language models for automated design of large neighborhood search operators in combinatorial optimization problems.","ai_keywords":["Large Language Models","Automated Heuristic Design","Combinatorial Optimization Problems","Large Neighborhood Search","generative evolutionary framework","constructive priority rules","parameterized local search","deep local optima","Traveling Salesman Problems","Capacitated Vehicle Routing Problems"],"githubStars":14},"publishedAt":"2026-02-08T23:13:35.000Z","title":"G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design","summary":"While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08253.png","numComments":3,"submittedBy":{"_id":"698ae59d70be6790cd1e6a53","avatarUrl":"/avatars/66125d952e7880baa4fe1635aafbc160.svg","fullname":"Baoyun Zhao","name":"ZBoyn","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.10224","authors":[{"_id":"698d401f65c0d15a6d162015","user":{"_id":"66ae3fbf491b555fef3bac0c","avatarUrl":"/avatars/47353470d46097ce108d32792dbbf2a2.svg","isPro":false,"fullname":"Shiting Huang","user":"chocckaka","type":"user"},"name":"Shiting Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:00.511Z","hidden":false},{"_id":"698d401f65c0d15a6d162016","name":"Zecheng Li","hidden":false},{"_id":"698d401f65c0d15a6d162017","user":{"_id":"665d652e0f35c005de892108","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg","isPro":false,"fullname":"Yu Zeng","user":"YuZeng260","type":"user"},"name":"Yu Zeng","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:02.566Z","hidden":false},{"_id":"698d401f65c0d15a6d162018","name":"Qingnan Ren","hidden":false},{"_id":"698d401f65c0d15a6d162019","name":"Zhen Fang","hidden":false},{"_id":"698d401f65c0d15a6d16201a","name":"Qisheng Su","hidden":false},{"_id":"698d401f65c0d15a6d16201b","name":"Kou Shi","hidden":false},{"_id":"698d401f65c0d15a6d16201c","name":"Lin Chen","hidden":false},{"_id":"698d401f65c0d15a6d16201d","name":"Zehui Chen","hidden":false},{"_id":"698d401f65c0d15a6d16201e","name":"Feng Zhao","hidden":false}],"publishedAt":"2026-02-10T19:16:09.000Z","submittedOnDailyAt":"2026-02-12T00:27:31.569Z","title":"Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models","submittedOnDailyBy":{"_id":"665d652e0f35c005de892108","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg","isPro":false,"fullname":"Yu Zeng","user":"YuZeng260","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.","upvotes":19,"discussionId":"698d401f65c0d15a6d16201f","ai_summary":"Meta-Experience Learning enhances LLM reasoning by incorporating self-distilled error representations into parametric memory through contrastive trajectory analysis and language-modeled reward signals.","ai_keywords":["reinforcement learning","large language models","meta-experience","self-verification","contrastive analysis","parametric memory","negative log-likelihood","language-modeled reward","Pass@1"]},"publishedAt":"2026-02-10T14:16:09.000Z","title":"Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10224.png","numComments":2,"submittedBy":{"_id":"665d652e0f35c005de892108","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg","fullname":"Yu Zeng","name":"YuZeng260","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.11089","authors":[{"_id":"698d4b1d65c0d15a6d1621a0","name":"Yicheng Chen","hidden":false},{"_id":"698d4b1d65c0d15a6d1621a1","name":"Zerun Ma","hidden":false},{"_id":"698d4b1d65c0d15a6d1621a2","name":"Xinchen Xie","hidden":false},{"_id":"698d4b1d65c0d15a6d1621a3","name":"Yining Li","hidden":false},{"_id":"698d4b1d65c0d15a6d1621a4","name":"Kai Chen","hidden":false}],"publishedAt":"2026-02-11T17:56:15.000Z","submittedOnDailyAt":"2026-02-12T01:09:34.064Z","title":"DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning","submittedOnDailyBy":{"_id":"6752dcb6ec430bad7a26c83e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6752dcb6ec430bad7a26c83e/F685Ij72H1gIve4s9pcup.jpeg","isPro":true,"fullname":"Chen Yicheng","user":"yichengchen24","type":"user"},"summary":"In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the data recipe, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.","upvotes":18,"discussionId":"698d4b1e65c0d15a6d1621a5","githubRepo":"https://github.com/yichengchen24/DataChef","githubRepoAddedBy":"user","ai_summary":"DataChef-32B automates data recipe generation for LLM adaptation through reinforcement learning with proxy rewards, achieving performance comparable to human-crafted recipes.","ai_keywords":["Large Language Models","data recipe","reinforcement learning","proxy reward","downstream performance","data synthesis","data filtering","automated data processing"],"githubStars":8},"publishedAt":"2026-02-11T12:56:15.000Z","title":"DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning","summary":"In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the data recipe, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11089.png","numComments":2,"submittedBy":{"_id":"6752dcb6ec430bad7a26c83e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6752dcb6ec430bad7a26c83e/F685Ij72H1gIve4s9pcup.jpeg","fullname":"Chen Yicheng","name":"yichengchen24","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.10975","authors":[{"_id":"698d4c7965c0d15a6d1621a7","name":"Qixing Zhou","hidden":false},{"_id":"698d4c7965c0d15a6d1621a8","user":{"_id":"6868f58a4757672a6da7c417","avatarUrl":"/avatars/73154b7e0f1af68054b97f10a6c2e670.svg","isPro":false,"fullname":"JiaCheng Zhang","user":"jiachengzhg","type":"user"},"name":"Jiacheng Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:04.846Z","hidden":false},{"_id":"698d4c7965c0d15a6d1621a9","user":{"_id":"65f43c3cc9940817caaf4434","avatarUrl":"/avatars/ecec2856ba7a7d3421a2071a0a88800b.svg","isPro":false,"fullname":"Haiyang Wang","user":"Haiyang-W","type":"user"},"name":"Haiyang Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:02.525Z","hidden":false},{"_id":"698d4c7965c0d15a6d1621aa","name":"Rui Hao","hidden":false},{"_id":"698d4c7965c0d15a6d1621ab","name":"Jiahe Wang","hidden":false},{"_id":"698d4c7965c0d15a6d1621ac","name":"Minghao Han","hidden":false},{"_id":"698d4c7965c0d15a6d1621ad","name":"Yuxue Yang","hidden":false},{"_id":"698d4c7965c0d15a6d1621ae","name":"Shuzhe Wu","hidden":false},{"_id":"698d4c7965c0d15a6d1621af","name":"Feiyang Pan","hidden":false},{"_id":"698d4c7965c0d15a6d1621b0","name":"Lue Fan","hidden":false},{"_id":"698d4c7965c0d15a6d1621b1","name":"Dandan Tu","hidden":false},{"_id":"698d4c7965c0d15a6d1621b2","name":"Zhaoxiang Zhang","hidden":false}],"publishedAt":"2026-02-11T16:06:32.000Z","submittedOnDailyAt":"2026-02-12T01:53:40.541Z","title":"FeatureBench: Benchmarking Agentic Coding for Complex Feature Development","submittedOnDailyBy":{"_id":"649ecf9827145c4463240177","avatarUrl":"/avatars/27696cf31790a3d58d8be2e0c983800e.svg","isPro":false,"fullname":"Lue Fan","user":"Abyssaledge","type":"user"},"summary":"Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.","upvotes":18,"discussionId":"698d4c7a65c0d15a6d1621b3","projectPage":"https://libercoders.github.io/FeatureBench/","githubRepo":"https://github.com/LiberCoders/FeatureBench","githubRepoAddedBy":"user","ai_summary":"FeatureBench evaluates agentic coding performance in comprehensive feature-oriented development through execution-based assessments and automated task derivation from code repositories.","ai_keywords":["large language models","agentic coding","software development","feature-oriented development","execution-based evaluation","test-driven method","dependency graph","unit tests","automated task collection","data leakage","agent training"],"githubStars":20},"publishedAt":"2026-02-11T11:06:32.000Z","title":"FeatureBench: Benchmarking Agentic Coding for Complex Feature Development","summary":"Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10975.png","numComments":2,"submittedBy":{"_id":"649ecf9827145c4463240177","avatarUrl":"/avatars/27696cf31790a3d58d8be2e0c983800e.svg","fullname":"Lue Fan","name":"Abyssaledge","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.11008","authors":[{"_id":"698d707d65c0d15a6d16224f","user":{"_id":"6166db59f78a267701a78c2a","avatarUrl":"/avatars/8784efc36f67719e9455b1f081340ed9.svg","isPro":false,"fullname":"Ammar Ali","user":"ammarali32","type":"user"},"name":"Ammar Ali","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:26:05.919Z","hidden":false},{"_id":"698d707d65c0d15a6d162250","name":"Baher Mohammad","hidden":false},{"_id":"698d707d65c0d15a6d162251","user":{"_id":"67bc9a9ed9a970de04255711","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_WzRhjLJnkf_ALkP4BuM_.png","isPro":false,"fullname":"Denis Makhov","user":"dennismak1994","type":"user"},"name":"Denis Makhov","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:06:47.223Z","hidden":false},{"_id":"698d707d65c0d15a6d162252","user":{"_id":"66465dfa508db0bde50d95f2","avatarUrl":"/avatars/8b4a583dc0f3cab0f1cd9a1be3daa01b.svg","isPro":false,"fullname":"Dmitry Shopkhoev","user":"dimitriish","type":"user"},"name":"Dmitriy Shopkhoev","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:06:49.035Z","hidden":false},{"_id":"698d707d65c0d15a6d162253","name":"Magauiya Zhussip","hidden":false},{"_id":"698d707d65c0d15a6d162254","name":"Stamatios Lefkimmiatis","hidden":false}],"publishedAt":"2026-02-11T16:34:52.000Z","submittedOnDailyAt":"2026-02-12T03:52:44.256Z","title":"ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression","submittedOnDailyBy":{"_id":"6166db59f78a267701a78c2a","avatarUrl":"/avatars/8784efc36f67719e9455b1f081340ed9.svg","isPro":false,"fullname":"Ammar Ali","user":"ammarali32","type":"user"},"summary":"We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\\% compression rates. Notably, it retains over 90\\% of the original model's performance at 30\\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.","upvotes":17,"discussionId":"698d707e65c0d15a6d162255","githubRepo":"https://github.com/mts-ai/ROCKET","githubRepoAddedBy":"user","ai_summary":"ROCKET is a training-free model compression method that formulates layer-wise compression as a multi-choice knapsack problem and uses sparse matrix factorization for efficient weight sparsification without iterative optimization.","ai_keywords":["model compression","multi-choice knapsack problem","sparse matrix factorization","dictionary learning","weight sparsification","activation-weights sensitivity","least squares","closed form update","fine-tuning","parameter-efficient fine-tuning"],"githubStars":20,"organization":{"_id":"65f1bb3789aedc3dbe201d53","name":"MTSAIR","fullname":"MTSAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6166db59f78a267701a78c2a/kTtlRzMs3RxfOT11vhhG4.jpeg"}},"publishedAt":"2026-02-11T11:34:52.000Z","title":"ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression","summary":"We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\\% compression rates. Notably, it retains over 90\\% of the original model's performance at 30\\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11008.png","numComments":3,"submittedBy":{"_id":"6166db59f78a267701a78c2a","avatarUrl":"/avatars/8784efc36f67719e9455b1f081340ed9.svg","fullname":"Ammar Ali","name":"ammarali32","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":15,"isUserFollowing":false},"organization":{"_id":"65f1bb3789aedc3dbe201d53","name":"MTSAIR","fullname":"MTSAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6166db59f78a267701a78c2a/kTtlRzMs3RxfOT11vhhG4.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.10609","authors":[{"_id":"698d37a66c5152984e4f3e7b","name":"Shuo He","hidden":false},{"_id":"698d37a66c5152984e4f3e7c","user":{"_id":"66ba29dd59e8e7a957154c5f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png","isPro":false,"fullname":"Lang Feng","user":"langfeng01","type":"user"},"name":"Lang Feng","status":"claimed_verified","statusLastChangedAt":"2026-02-12T20:23:25.095Z","hidden":false},{"_id":"698d37a66c5152984e4f3e7d","name":"Xin Cheng","hidden":false},{"_id":"698d37a66c5152984e4f3e7e","user":{"_id":"698dc46341d49f1062061c64","avatarUrl":"/avatars/34d7f69e61aeb37e226da8488f8b8c98.svg","isPro":false,"fullname":"Lei Feng","user":"CharlesFeng1995","type":"user"},"name":"Lei Feng","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:10.067Z","hidden":false},{"_id":"698d37a66c5152984e4f3e7f","name":"Bo An","hidden":false}],"publishedAt":"2026-02-11T07:57:43.000Z","submittedOnDailyAt":"2026-02-12T01:08:29.897Z","title":"Online Causal Kalman Filtering for Stable and Effective Policy Optimization","submittedOnDailyBy":{"_id":"66ba29dd59e8e7a957154c5f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png","isPro":false,"fullname":"Lang Feng","user":"langfeng01","type":"user"},"summary":"Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.","upvotes":17,"discussionId":"698d37a66c5152984e4f3e80","ai_summary":"Online Causal Kalman Filtering addresses high-variance token-level importance sampling in reinforcement learning for large language models by modeling IS ratios as evolving latent states and using Kalman filtering for stable policy optimization.","ai_keywords":["importance sampling","policy optimization","reinforcement learning","large language models","Kalman filter","token-level","sequence-level","off-policy derivation","policy gradient","training collapse"],"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"}},"publishedAt":"2026-02-11T02:57:43.000Z","title":"Online Causal Kalman Filtering for Stable and Effective Policy Optimization","summary":"Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10609.png","numComments":3,"submittedBy":{"_id":"66ba29dd59e8e7a957154c5f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png","fullname":"Lang Feng","name":"langfeng01","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"6508b28cf36bb51c50faad98","name":"NanyangTechnologicalUniversity","fullname":"Nanyang Technological University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.11451","authors":[{"_id":"698e892bcace060ff123abf5","user":{"_id":"65e8df170cda621164769f6f","avatarUrl":"/avatars/b5d7c7c49d1a777a1bd41d06d299868a.svg","isPro":false,"fullname":"Armen Jeddi","user":"armenjeddi","type":"user"},"name":"Ahmadreza Jeddi","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:06.865Z","hidden":false},{"_id":"698e892bcace060ff123abf6","name":"Marco Ciccone","hidden":false},{"_id":"698e892bcace060ff123abf7","name":"Babak Taati","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/YQLGDorWrKIWbzm4Y0FHP.png","https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/z8FmoOsy9WJIuMxje0rQg.jpeg","https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/TINAx5DDl1gwzxCQO0gKO.jpeg","https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/qvB_nILJZbxv0SloME_zT.jpeg"],"publishedAt":"2026-02-11T23:58:28.000Z","submittedOnDailyAt":"2026-02-12T23:49:05.014Z","title":"LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation","submittedOnDailyBy":{"_id":"65e8df170cda621164769f6f","avatarUrl":"/avatars/b5d7c7c49d1a777a1bd41d06d299868a.svg","isPro":false,"fullname":"Armen Jeddi","user":"armenjeddi","type":"user"},"summary":"Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling, opening a path toward controllable and budget-aware large language models.","upvotes":15,"discussionId":"698e892ccace060ff123abf8","projectPage":"https://loopformer.github.io/","githubRepo":"https://github.com/armenjeddi/loopformer","githubRepoAddedBy":"user","ai_summary":"LoopFormer is a looped Transformer architecture that enables adaptive computational depth through variable-length trajectory training and shortcut-consistency regularization, allowing flexible reasoning under different compute constraints.","ai_keywords":["looped Transformers","reasoning","inductive bias","loop iterations","variable compute budgets","LoopFormer","shortcut-consistency training","trajectory alignment","computational depth","adaptive language modeling"],"githubStars":4,"organization":{"_id":"65b2b60eade89bc3fac3108a","name":"vector-institute","fullname":"Vector Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"}},"publishedAt":"2026-02-11T18:58:28.000Z","title":"LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation","summary":"Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling, opening a path toward controllable and budget-aware large language models.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/YQLGDorWrKIWbzm4Y0FHP.png","https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/z8FmoOsy9WJIuMxje0rQg.jpeg","https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/TINAx5DDl1gwzxCQO0gKO.jpeg","https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/qvB_nILJZbxv0SloME_zT.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11451.png","numComments":2,"submittedBy":{"_id":"65e8df170cda621164769f6f","avatarUrl":"/avatars/b5d7c7c49d1a777a1bd41d06d299868a.svg","fullname":"Armen Jeddi","name":"armenjeddi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"65b2b60eade89bc3fac3108a","name":"vector-institute","fullname":"Vector Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.11103","authors":[{"_id":"698d4fd565c0d15a6d1621cb","name":"Wayne Chi","hidden":false},{"_id":"698d4fd565c0d15a6d1621cc","name":"Yixiong Fang","hidden":false},{"_id":"698d4fd565c0d15a6d1621cd","name":"Arnav Yayavaram","hidden":false},{"_id":"698d4fd565c0d15a6d1621ce","name":"Siddharth Yayavaram","hidden":false},{"_id":"698d4fd565c0d15a6d1621cf","user":{"_id":"6658e1c8ce1b2838885b2d7f","avatarUrl":"/avatars/8623555f14b62f40fd372da20cb59ccc.svg","isPro":false,"fullname":"Seth Karten","user":"milkkarten","type":"user"},"name":"Seth Karten","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:50:49.927Z","hidden":false},{"_id":"698d4fd565c0d15a6d1621d0","name":"Qiuhong Anna Wei","hidden":false},{"_id":"698d4fd565c0d15a6d1621d1","name":"Runkun Chen","hidden":false},{"_id":"698d4fd565c0d15a6d1621d2","name":"Alexander Wang","hidden":false},{"_id":"698d4fd565c0d15a6d1621d3","name":"Valerie Chen","hidden":false},{"_id":"698d4fd565c0d15a6d1621d4","name":"Ameet Talwalkar","hidden":false},{"_id":"698d4fd565c0d15a6d1621d5","name":"Chris Donahue","hidden":false}],"publishedAt":"2026-02-11T18:15:11.000Z","submittedOnDailyAt":"2026-02-12T18:15:39.552Z","title":"GameDevBench: Evaluating Agentic Capabilities Through Game Development","submittedOnDailyBy":{"_id":"6563bc8d6ef2a1d0f988445c","avatarUrl":"/avatars/19ed8a313c4fcb3ea617dd53772deeda.svg","isPro":false,"fullname":"Wayne Chi","user":"waynechi","type":"user"},"summary":"Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.","upvotes":15,"discussionId":"698d4fd565c0d15a6d1621d6","ai_summary":"GameDevBench is introduced as the first benchmark for evaluating agents on game development tasks that combine software development complexity with deep multimodal understanding requirements.","ai_keywords":["coding agents","multimodal counterparts","evaluation testbed","software development","multimodal understanding","game development","GameDevBench","web tutorials","video tutorials","multimodal complexity","agentic game development"],"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"}},"publishedAt":"2026-02-11T13:15:11.000Z","title":"GameDevBench: Evaluating Agentic Capabilities Through Game Development","summary":"Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene. We present GameDevBench, the first benchmark for evaluating agents on game development tasks. GameDevBench consists of 132 tasks derived from web and video tutorials. Tasks require significant multimodal understanding and are complex -- the average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5% of tasks. We find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9% on gameplay-oriented tasks to 31.6% on 2D graphics tasks. To improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents. Despite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5's performance from 33.3% to 47.7%. We release GameDevBench publicly to support further research into agentic game development.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11103.png","numComments":2,"submittedBy":{"_id":"6563bc8d6ef2a1d0f988445c","avatarUrl":"/avatars/19ed8a313c4fcb3ea617dd53772deeda.svg","fullname":"Wayne Chi","name":"waynechi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"691d9a1012cc4d473e1c862f","name":"CarnegieMellonU","fullname":"Carnegie Mellon University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/6I146aJvxxlRCEbYFFAeQ.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.12108","authors":[{"_id":"698e8affcace060ff123abfa","name":"Xiaoyuan Liu","hidden":false},{"_id":"698e8affcace060ff123abfb","name":"Tian Liang","hidden":false},{"_id":"698e8affcace060ff123abfc","name":"Dongyang Ma","hidden":false},{"_id":"698e8affcace060ff123abfd","name":"Deyu Zhou","hidden":false},{"_id":"698e8affcace060ff123abfe","name":"Haitao Mi","hidden":false},{"_id":"698e8affcace060ff123abff","name":"Pinjia He","hidden":false},{"_id":"698e8affcace060ff123ac00","user":{"_id":"64674375946476c5d215382d","avatarUrl":"/avatars/0f0b98080c064a17d003da8a22a047cc.svg","isPro":false,"fullname":"Yan Wang","user":"libertywing","type":"user"},"name":"Yan Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:03.071Z","hidden":false}],"publishedAt":"2026-02-12T16:00:01.000Z","submittedOnDailyAt":"2026-02-12T23:59:11.586Z","title":"The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context","submittedOnDailyBy":{"_id":"64674375946476c5d215382d","avatarUrl":"/avatars/0f0b98080c064a17d003da8a22a047cc.svg","isPro":false,"fullname":"Yan Wang","user":"libertywing","type":"user"},"summary":"In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.","upvotes":13,"discussionId":"698e8affcace060ff123ac01","githubRepo":"https://github.com/xyliu-cs/StateLM","githubRepoAddedBy":"user","ai_summary":"StateLM enables language models to actively manage their own memory and context through internal reasoning loops and memory tools, significantly improving performance on long-document tasks and chat memory challenges.","ai_keywords":["foundation models","internal reasoning loop","memory tools","context pruning","document indexing","note-taking","dynamic context engineering","long-document QA","chat memory task","deep research task","BrowseComp-Plus"],"githubStars":10,"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-12T11:00:01.000Z","title":"The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context","summary":"In the world of Harry Potter, when Dumbledore's mind is overburdened, he extracts memories into a Pensieve to be revisited later. In the world of AI, while we possess the Pensieve-mature databases and retrieval systems, our models inexplicably lack the \"wand\" to operate it. They remain like a Dumbledore without agency, passively accepting a manually engineered context as their entire memory. This work finally places the wand in the model's hand. We introduce StateLM, a new class of foundation models endowed with an internal reasoning loop to manage their own state. We equip our model with a suite of memory tools, such as context pruning, document indexing, and note-taking, and train it to actively manage these tools. By learning to dynamically engineering its own context, our model breaks free from the architectural prison of a fixed window. Experiments across various model sizes demonstrate StateLM's effectiveness across diverse scenarios. On long-document QA tasks, StateLMs consistently outperform standard LLMs across all model scales; on the chat memory task, they achieve absolute accuracy improvements of 10% to 20% over standard LLMs. On the deep research task BrowseComp-Plus, the performance gap becomes even more pronounced: StateLM achieves up to 52% accuracy, whereas standard LLM counterparts struggle around 5%. Ultimately, our approach shifts LLMs from passive predictors to state-aware agents where reasoning becomes a stateful and manageable process.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12108.png","numComments":4,"submittedBy":{"_id":"64674375946476c5d215382d","avatarUrl":"/avatars/0f0b98080c064a17d003da8a22a047cc.svg","fullname":"Yan Wang","name":"libertywing","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.10367","authors":[{"_id":"698e1234cace060ff123ab63","user":{"_id":"65aae63fc3fa44c7109559bb","avatarUrl":"/avatars/b3f3e5d09b410f717c07b6aea997d595.svg","isPro":false,"fullname":"Zhiling Yan","user":"JuelieYann","type":"user"},"name":"Zhiling Yan","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:17.425Z","hidden":false},{"_id":"698e1234cace060ff123ab64","user":{"_id":"619f01b8cc04eadf54fa5d5d","avatarUrl":"/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg","isPro":false,"fullname":"Song Dingjie","user":"songdj","type":"user"},"name":"Dingjie Song","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:19.177Z","hidden":false},{"_id":"698e1234cace060ff123ab65","name":"Zhe Fang","hidden":false},{"_id":"698e1234cace060ff123ab66","name":"Yisheng Ji","hidden":false},{"_id":"698e1234cace060ff123ab67","name":"Xiang Li","hidden":false},{"_id":"698e1234cace060ff123ab68","name":"Quanzheng Li","hidden":false},{"_id":"698e1234cace060ff123ab69","name":"Lichao Sun","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65aae63fc3fa44c7109559bb/Np4_KL-euIgN8XivhLJdU.png","https://cdn-uploads.huggingface.co/production/uploads/65aae63fc3fa44c7109559bb/nodaBYULwdaPbpiuBJTDU.png"],"publishedAt":"2026-02-10T23:38:25.000Z","submittedOnDailyAt":"2026-02-12T19:46:29.491Z","title":"LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation","submittedOnDailyBy":{"_id":"65aae63fc3fa44c7109559bb","avatarUrl":"/avatars/b3f3e5d09b410f717c07b6aea997d595.svg","isPro":false,"fullname":"Zhiling Yan","user":"JuelieYann","type":"user"},"summary":"The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.","upvotes":13,"discussionId":"698e1234cace060ff123ab6a","projectPage":"https://zhilingyan.github.io/LiveMedBench/","githubRepo":"https://github.com/ZhilingYan/LiveMedBench","githubRepoAddedBy":"user","ai_summary":"LiveMedBench addresses limitations in medical LLM evaluation by providing a continuously updated, contamination-free benchmark with rubric-based evaluation that better aligns with expert clinical reasoning.","ai_keywords":["Large Language Models","medical benchmarks","data contamination","temporal misalignment","clinical reasoning","automated rubric-based evaluation","multi-agent clinical curation framework"],"githubStars":2},"publishedAt":"2026-02-10T18:38:25.000Z","title":"LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation","summary":"The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65aae63fc3fa44c7109559bb/Np4_KL-euIgN8XivhLJdU.png","https://cdn-uploads.huggingface.co/production/uploads/65aae63fc3fa44c7109559bb/nodaBYULwdaPbpiuBJTDU.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10367.png","numComments":2,"submittedBy":{"_id":"65aae63fc3fa44c7109559bb","avatarUrl":"/avatars/b3f3e5d09b410f717c07b6aea997d595.svg","fullname":"Zhiling Yan","name":"JuelieYann","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.11149","authors":[{"_id":"698d88a365c0d15a6d1622cb","user":{"_id":"637de0fa1342ba1762422495","avatarUrl":"/avatars/b1cce386a8b33007fc381fcbfc5cdc9a.svg","isPro":false,"fullname":"Dawid","user":"dakopi","type":"user"},"name":"Dawid J. Kopiczko","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:26:00.517Z","hidden":false},{"_id":"698d88a365c0d15a6d1622cc","name":"Sagar Vaze","hidden":false},{"_id":"698d88a365c0d15a6d1622cd","name":"Tijmen Blankevoort","hidden":false},{"_id":"698d88a365c0d15a6d1622ce","name":"Yuki M. Asano","hidden":false}],"publishedAt":"2026-02-11T18:58:54.000Z","submittedOnDailyAt":"2026-02-12T06:08:23.696Z","title":"Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning","submittedOnDailyBy":{"_id":"637de0fa1342ba1762422495","avatarUrl":"/avatars/b1cce386a8b33007fc381fcbfc5cdc9a.svg","isPro":false,"fullname":"Dawid","user":"dakopi","type":"user"},"summary":"Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.","upvotes":12,"discussionId":"698d88a365c0d15a6d1622cf","githubRepo":"https://github.com/dkopi/data-repetition","githubRepoAddedBy":"user","ai_summary":"Training reasoning language models with repeated examples on smaller datasets yields better performance than single-pass training on larger datasets, with token accuracy serving as a reliable indicator for optimal training duration.","ai_keywords":["supervised fine-tuning","chain-of-thought data","reasoning language models","training epochs","token accuracy","memorization","generalization","AIME","GPQA","catastrophic forgetting"],"githubStars":4},"publishedAt":"2026-02-11T13:58:54.000Z","title":"Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning","summary":"Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11149.png","numComments":4,"submittedBy":{"_id":"637de0fa1342ba1762422495","avatarUrl":"/avatars/b1cce386a8b33007fc381fcbfc5cdc9a.svg","fullname":"Dawid","name":"dakopi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.10231","authors":[{"_id":"698da10c65c0d15a6d162353","name":"Kirill Pavlenko","hidden":false},{"_id":"698da10c65c0d15a6d162354","user":{"_id":"644e9ffcd6001776ed77d874","avatarUrl":"/avatars/b93e02caf929679b7e9bc589eed0b689.svg","isPro":false,"fullname":"Alexander","user":"djalexj","type":"user"},"name":"Alexander Golubev","status":"claimed_verified","statusLastChangedAt":"2026-02-12T11:55:59.641Z","hidden":false},{"_id":"698da10c65c0d15a6d162355","name":"Simon Karasik","hidden":false},{"_id":"698da10c65c0d15a6d162356","name":"Boris Yangel","hidden":false}],"publishedAt":"2026-02-10T19:22:37.000Z","submittedOnDailyAt":"2026-02-12T07:16:32.096Z","title":"Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards","submittedOnDailyBy":{"_id":"644e9ffcd6001776ed77d874","avatarUrl":"/avatars/b93e02caf929679b7e9bc589eed0b689.svg","isPro":false,"fullname":"Alexander","user":"djalexj","type":"user"},"summary":"Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.","upvotes":12,"discussionId":"698da10c65c0d15a6d162357","ai_summary":"Blockwise Advantage Estimation addresses reward interference in structured generations by assigning separate advantages to different text blocks, using outcome-conditioned baselines to avoid expensive nested rollouts.","ai_keywords":["Group Relative Policy Optimization","advantage estimation","reward interference","structured generations","text blocks","outcome-conditioned baseline","nested rollouts","confidence-weighted ensembling"],"organization":{"_id":"65d488f33d3bac311c3a6daa","name":"nebius","fullname":"Nebius","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65d46641d37914c4c8cff0a8/Xbv8r4bfgAQcQ2K6Bdiym.jpeg"}},"publishedAt":"2026-02-10T14:22:37.000Z","title":"Blockwise Advantage Estimation for Multi-Objective RL with Verifiable Rewards","summary":"Group Relative Policy Optimization (GRPO) assigns a single scalar advantage to all tokens in a completion. For structured generations with explicit segments and objectives, this couples unrelated reward signals across segments, leading to objective interference and misattributed credit. We propose Blockwise Advantage Estimation, a family of GRPO-compatible methods that assigns each objective its own advantage and applies it only to the tokens in the corresponding text block, reducing reliance on hand-designed scalar rewards and scaling naturally to additional objectives. A key challenge is estimating advantages for later blocks whose rewards are conditioned on sampled prefixes; standard unbiased approaches require expensive nested rollouts from intermediate states. Concretely, we introduce an Outcome-Conditioned Baseline that approximates intermediate state values using only within-group statistics by stratifying samples according to a prefix-derived intermediate outcome. On math tasks with uncertainty estimation, our method mitigates reward interference, is competitive with a state-of-the-art reward-designed approach, and preserves test-time gains from confidence-weighted ensembling. More broadly, it provides a modular recipe for optimizing sequential objectives in structured generations without additional rollouts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10231.png","numComments":3,"submittedBy":{"_id":"644e9ffcd6001776ed77d874","avatarUrl":"/avatars/b93e02caf929679b7e9bc589eed0b689.svg","fullname":"Alexander","name":"djalexj","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"65d488f33d3bac311c3a6daa","name":"nebius","fullname":"Nebius","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65d46641d37914c4c8cff0a8/Xbv8r4bfgAQcQ2K6Bdiym.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.02192","authors":[{"_id":"698b4deb6052d3bed96307aa","name":"Jie Xiao","hidden":false},{"_id":"698b4deb6052d3bed96307ab","name":"Meng Chen","hidden":false},{"_id":"698b4deb6052d3bed96307ac","name":"Qingnan Ren","hidden":false},{"_id":"698c9e456c5152984e4f3c98","name":"Jingwei Song","hidden":false},{"_id":"698b4deb6052d3bed96307ae","name":"Jiaqi Huang","hidden":false},{"_id":"698b4deb6052d3bed96307af","name":"Yangshen Deng","hidden":false},{"_id":"698b4deb6052d3bed96307b0","name":"Chris Tong","hidden":false},{"_id":"698b4deb6052d3bed96307b1","name":"Wanyi Chen","hidden":false},{"_id":"698b4deb6052d3bed96307b2","name":"Suli Wang","hidden":false},{"_id":"698b4deb6052d3bed96307b3","name":"Ziqian Bi","hidden":false},{"_id":"698b4deb6052d3bed96307b4","name":"Shuo Lu","hidden":false},{"_id":"698b4deb6052d3bed96307b5","name":"Yiqun Duan","hidden":false},{"_id":"698b4deb6052d3bed96307b6","name":"Xu Wang","hidden":false},{"_id":"698b4deb6052d3bed96307b7","name":"Rymon Yu","hidden":false},{"_id":"698b4deb6052d3bed96307b8","name":"Ween Yang","hidden":false},{"_id":"698b4deb6052d3bed96307b9","name":"Lynn Ai","hidden":false},{"_id":"698b4deb6052d3bed96307ba","name":"Eric Yang","hidden":false},{"_id":"698b4deb6052d3bed96307bb","name":"Bill Shi","hidden":false},{"_id":"698b4deb6052d3bed96307ad","user":{"_id":"6703c272c265fe2565f90e5c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6703c272c265fe2565f90e5c/Fu3--OfA6tFYeASPrxc_c.png","isPro":false,"fullname":"Jingwei Song","user":"JingweiSong","type":"user"},"name":"Song Jingwei","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:16:34.503Z","hidden":false}],"publishedAt":"2026-02-02T14:57:53.000Z","submittedOnDailyAt":"2026-02-12T13:34:09.565Z","title":"ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning","submittedOnDailyBy":{"_id":"6703c272c265fe2565f90e5c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6703c272c265fe2565f90e5c/Fu3--OfA6tFYeASPrxc_c.png","isPro":false,"fullname":"Jingwei Song","user":"JingweiSong","type":"user"},"summary":"Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.","upvotes":12,"discussionId":"698b4deb6052d3bed96307bc","ai_summary":"ECHO-2 is a distributed reinforcement learning framework that enables efficient post-training of large language models by overlapping rollout generation, dissemination, and training while managing policy staleness and network latency.","ai_keywords":["reinforcement learning","post-training","large language models","distributed RL","rollout generation","reward evaluation","centralized learning","policy staleness","peer-assisted pipelined broadcast","cost-aware activation","GRPO"],"organization":{"_id":"68906a5d50f806919e80a6ef","name":"GradientResearch","fullname":"Gradient","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68906890eb018fabfe8487bc/ZPXtRlYSostqSRvrt4AWY.jpeg"}},"publishedAt":"2026-02-02T09:57:53.000Z","title":"ECHO-2: A Large-Scale Distributed Rollout Framework for Cost-Efficient Reinforcement Learning","summary":"Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02192.png","numComments":2,"submittedBy":{"_id":"6703c272c265fe2565f90e5c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6703c272c265fe2565f90e5c/Fu3--OfA6tFYeASPrxc_c.png","fullname":"Jingwei Song","name":"JingweiSong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"68906a5d50f806919e80a6ef","name":"GradientResearch","fullname":"Gradient","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68906890eb018fabfe8487bc/ZPXtRlYSostqSRvrt4AWY.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.07106","authors":[{"_id":"698d49ec65c0d15a6d162170","user":{"_id":"665d72007bef1cfc313a92dd","avatarUrl":"/avatars/6d56671153bbf1ffff072472678819da.svg","isPro":false,"fullname":"Haoyu Zhang","user":"lemonade666","type":"user"},"name":"Haoyu Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:21.307Z","hidden":false},{"_id":"698d49ec65c0d15a6d162171","name":"Zhipeng Li","hidden":false},{"_id":"698d49ec65c0d15a6d162172","name":"Yiwen Guo","hidden":false},{"_id":"698d49ec65c0d15a6d162173","name":"Tianshu Yu","hidden":false}],"publishedAt":"2026-02-06T18:03:30.000Z","submittedOnDailyAt":"2026-02-12T03:17:34.016Z","title":"Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models","submittedOnDailyBy":{"_id":"660383b2527470e0164533a9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/660383b2527470e0164533a9/CXIpr6_vtoxPFXW5EKh8n.jpeg","isPro":false,"fullname":"Chengqian Ma","user":"ChengqianMa","type":"user"},"summary":"Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.","upvotes":11,"discussionId":"698d49ed65c0d15a6d162174","projectPage":"https://haoyu-ha.github.io/Ex-Omni-Project-Page/","ai_summary":"Ex-Omni is an open-source framework that enhances omni-modal large language models with speech-accompanied 3D facial animation by decoupling semantic reasoning from temporal generation and using speech units as temporal scaffolding.","ai_keywords":["omni-modal large language models","3D facial animation","speech units","token-as-query gated fusion","temporal scaffolding","semantic reasoning","temporal generation"]},"publishedAt":"2026-02-06T13:03:30.000Z","title":"Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models","summary":"Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07106.png","numComments":3,"submittedBy":{"_id":"660383b2527470e0164533a9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/660383b2527470e0164533a9/CXIpr6_vtoxPFXW5EKh8n.jpeg","fullname":"Chengqian Ma","name":"ChengqianMa","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.10999","authors":[{"_id":"698d437e65c0d15a6d162132","name":"Yusong Lin","hidden":false},{"_id":"698d437e65c0d15a6d162133","user":{"_id":"65f43c3cc9940817caaf4434","avatarUrl":"/avatars/ecec2856ba7a7d3421a2071a0a88800b.svg","isPro":false,"fullname":"Haiyang Wang","user":"Haiyang-W","type":"user"},"name":"Haiyang Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:33.349Z","hidden":false},{"_id":"698d437e65c0d15a6d162134","name":"Shuzhe Wu","hidden":false},{"_id":"698d437e65c0d15a6d162135","name":"Lue Fan","hidden":false},{"_id":"698d437e65c0d15a6d162136","name":"Feiyang Pan","hidden":false},{"_id":"698d437e65c0d15a6d162137","name":"Sanyuan Zhao","hidden":false},{"_id":"698d437e65c0d15a6d162138","name":"Dandan Tu","hidden":false}],"publishedAt":"2026-02-11T16:22:18.000Z","submittedOnDailyAt":"2026-02-12T00:35:37.899Z","title":"CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.","upvotes":10,"discussionId":"698d437e65c0d15a6d162139","githubRepo":"https://github.com/LiberCoders/CLI-Gym","githubRepoAddedBy":"user","ai_summary":"CLI-Gym enables scalable derivation of environment-intensive tasks by simulating and exploring environment histories, while LiberCoder achieves significant performance improvements on Terminal-Bench through fine-tuning.","ai_keywords":["CLI-Gym","LiberCoder","environment-intensive tasks","Dockerfile","agentic task","execution feedback","Terminal-Bench"],"githubStars":13},"publishedAt":"2026-02-11T11:22:18.000Z","title":"CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion","summary":"Agentic coding requires agents to effectively interact with runtime environments, e.g., command line interfaces (CLI), so as to complete tasks like resolving dependency issues, fixing system problems, etc. But it remains underexplored how such environment-intensive tasks can be obtained at scale to enhance agents' capabilities. To address this, based on an analogy between the Dockerfile and the agentic task, we propose to employ agents to simulate and explore environment histories, guided by execution feedback. By tracing histories of a healthy environment, its state can be inverted to an earlier one with runtime failures, from which a task can be derived by packing the buggy state and the corresponding error messages. With our method, named CLI-Gym, a total of 1,655 environment-intensive tasks are derived, being the largest collection of its kind. Moreover, with curated successful trajectories, our fine-tuned model, named LiberCoder, achieves substantial absolute improvements of +21.1% (to 46.1%) on Terminal-Bench, outperforming various strong baselines. To our knowledge, this is the first public pipeline for scalable derivation of environment-intensive tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10999.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.09514","authors":[{"_id":"698d873b65c0d15a6d1622b9","name":"Xavier Hu","hidden":false},{"_id":"698d873b65c0d15a6d1622ba","name":"Jinxiang Xia","hidden":false},{"_id":"698d873b65c0d15a6d1622bb","name":"Shengze Xu","hidden":false},{"_id":"698d873b65c0d15a6d1622bc","name":"Kangqi Song","hidden":false},{"_id":"698d873b65c0d15a6d1622bd","name":"Yishuo Yuan","hidden":false},{"_id":"698d873b65c0d15a6d1622be","name":"Guibin Zhang","hidden":false},{"_id":"698d873b65c0d15a6d1622bf","name":"JinCheng Ren","hidden":false},{"_id":"698d873b65c0d15a6d1622c0","name":"Boyu Feng","hidden":false},{"_id":"698d873b65c0d15a6d1622c1","name":"Li Lu","hidden":false},{"_id":"698d873b65c0d15a6d1622c2","name":"Tieyong Zeng","hidden":false},{"_id":"698d873b65c0d15a6d1622c3","name":"Jiaheng Liu","hidden":false},{"_id":"698d873b65c0d15a6d1622c4","name":"Minghao Liu","hidden":false},{"_id":"698d873b65c0d15a6d1622c5","name":"He Zhu","hidden":false},{"_id":"698d873b65c0d15a6d1622c6","name":"Yuchen Eleanor Jiang","hidden":false},{"_id":"698d873b65c0d15a6d1622c7","name":"Wei Wang","hidden":false},{"_id":"698d873b65c0d15a6d1622c8","name":"Wangchunshu Zhou","hidden":false}],"publishedAt":"2026-02-10T08:12:23.000Z","submittedOnDailyAt":"2026-02-12T05:25:33.724Z","title":"EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies","submittedOnDailyBy":{"_id":"65897684f8b453e1f57cdb26","avatarUrl":"/avatars/80096d6c808805e1a84a68fb6194a7d4.svg","isPro":false,"fullname":"huxueyu","user":"huxueyu","type":"user"},"summary":"Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.","upvotes":9,"discussionId":"698d873b65c0d15a6d1622c9","githubRepo":"https://github.com/OPPO-PersonalAI/EcoGym","githubRepoAddedBy":"user","ai_summary":"EcoGym presents a generalizable benchmark for evaluating long-horizon planning capabilities of LLM-based agents in interactive economic environments with persistent dynamics and multi-scenario evaluation.","ai_keywords":["long-horizon planning","LLM-based agents","continuous plan-and-execute decision making","interactive economies","persistent economic dynamics","business-relevant outcomes","partial observability","stochasticity","controllability-utility trade-offs"],"githubStars":7},"publishedAt":"2026-02-10T03:12:23.000Z","title":"EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies","summary":"Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09514.png","numComments":2,"submittedBy":{"_id":"65897684f8b453e1f57cdb26","avatarUrl":"/avatars/80096d6c808805e1a84a68fb6194a7d4.svg","fullname":"huxueyu","name":"huxueyu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.03773","authors":[{"_id":"698df6d1cace060ff123ab1c","name":"Ian Wu","hidden":false},{"_id":"698df6d1cace060ff123ab1d","name":"Yuxiao Qu","hidden":false},{"_id":"698df6d1cace060ff123ab1e","name":"Amrith Setlur","hidden":false},{"_id":"698df6d1cace060ff123ab1f","name":"Aviral Kumar","hidden":false}],"publishedAt":"2026-02-03T17:34:04.000Z","submittedOnDailyAt":"2026-02-12T13:21:44.244Z","title":"Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL","submittedOnDailyBy":{"_id":"5f0c746619cb630495b814fd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1594651707950-noauth.jpeg","isPro":true,"fullname":"Lewis Tunstall","user":"lewtun","type":"user"},"summary":"Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.","upvotes":9,"discussionId":"698df6d1cace060ff123ab20","ai_summary":"RC, an iterative decoding algorithm, enables large language models to extrapolate and continuously improve beyond training budgets by constructing reasoning chains that enhance across iterations, achieving superior performance on long-horizon tasks.","ai_keywords":["reinforcement learning","autoregressive decoding","reasoning chains","test-time performance","scaffolding","continual improvement","extrapolation"]},"publishedAt":"2026-02-03T12:34:04.000Z","title":"Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL","summary":"Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.03773.png","numComments":3,"submittedBy":{"_id":"5f0c746619cb630495b814fd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1594651707950-noauth.jpeg","fullname":"Lewis Tunstall","name":"lewtun","type":"user","isPro":true,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":1321,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.09713","authors":[{"_id":"698beee66052d3bed9630a1c","user":{"_id":"641590e5486c7c9a5d13fe35","avatarUrl":"/avatars/2ef3432815e34a0eee45297fd99e5c40.svg","isPro":false,"fullname":"Ruisi Zhao","user":"zhaors00","type":"user"},"name":"Ruisi Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:17:15.023Z","hidden":false},{"_id":"698beee66052d3bed9630a1d","user":{"_id":"64173f238f689506e71091c4","avatarUrl":"/avatars/dbfe7939d3f2bb846df8447485295cdc.svg","isPro":false,"fullname":"Zheng Haoren","user":"Zhroyn","type":"user"},"name":"Haoren Zheng","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:14:57.543Z","hidden":false},{"_id":"698beee66052d3bed9630a1e","name":"Zongxin Yang","hidden":false},{"_id":"698beee66052d3bed9630a1f","name":"Hehe Fan","hidden":false},{"_id":"698beee66052d3bed9630a20","name":"Yi Yang","hidden":false}],"publishedAt":"2026-02-10T12:17:00.000Z","submittedOnDailyAt":"2026-02-12T00:00:01.426Z","title":"Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models","submittedOnDailyBy":{"_id":"641590e5486c7c9a5d13fe35","avatarUrl":"/avatars/2ef3432815e34a0eee45297fd99e5c40.svg","isPro":false,"fullname":"Ruisi Zhao","user":"zhaors00","type":"user"},"summary":"Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.","upvotes":8,"discussionId":"698beee66052d3bed9630a21","projectPage":"https://github.com/Whalesong-zrs/Stroke3D_project_page","githubRepo":"https://github.com/Whalesong-zrs/Stroke3D","githubRepoAddedBy":"user","ai_summary":"Stroke3D generates rigged 3D meshes from 2D strokes and text prompts through a two-stage pipeline combining controllable skeleton generation with enhanced mesh synthesis.","ai_keywords":["Skeletal Graph VAE","Sk-VAE","Skeletal Graph DiT","Sk-DiT","TextuRig","SKA-DPO","skeleton-mesh alignment score","Objaverse-XL","preference optimization","controllable skeleton generation","enhanced mesh synthesis"],"githubStars":22,"organization":{"_id":"6345aadf5efccdc07f1365a5","name":"ZhejiangUniversity","fullname":"Zhejiang University"}},"publishedAt":"2026-02-10T07:17:00.000Z","title":"Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models","summary":"Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09713.png","numComments":3,"submittedBy":{"_id":"641590e5486c7c9a5d13fe35","avatarUrl":"/avatars/2ef3432815e34a0eee45297fd99e5c40.svg","fullname":"Ruisi Zhao","name":"zhaors00","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6345aadf5efccdc07f1365a5","name":"ZhejiangUniversity","fullname":"Zhejiang University"},"isAuthorParticipating":true},{"paper":{"id":"2602.10748","authors":[{"_id":"698d8ba365c0d15a6d1622ea","user":{"_id":"643515914ff6a19a90ca5865","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643515914ff6a19a90ca5865/PUJUnMtjQSfyf58F5kE9m.jpeg","isPro":false,"fullname":"Farzad Shami","user":"0xFarzad","type":"user"},"name":"Farzad Shami","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:25:57.460Z","hidden":false},{"_id":"698d8ba365c0d15a6d1622eb","user":{"_id":"65d608d515f94930d75d2eb6","avatarUrl":"/avatars/2898ccec4a5d37dcb559059b9cd64c5e.svg","isPro":false,"fullname":"Stefano Marchesin","user":"Aleste","type":"user"},"name":"Stefano Marchesin","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:25:55.385Z","hidden":false},{"_id":"698d8ba365c0d15a6d1622ec","name":"Gianmaria Silvello","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/643515914ff6a19a90ca5865/Z89mkjK46nBVBSpDn3nw8.png"],"publishedAt":"2026-02-11T11:24:46.000Z","submittedOnDailyAt":"2026-02-12T05:58:38.938Z","title":"Benchmarking Large Language Models for Knowledge Graph Validation","submittedOnDailyBy":{"_id":"643515914ff6a19a90ca5865","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643515914ff6a19a90ca5865/PUJUnMtjQSfyf58F5kE9m.jpeg","isPro":false,"fullname":"Farzad Shami","user":"0xFarzad","type":"user"},"summary":"Knowledge Graphs (KGs) store structured factual knowledge by linking entities through relationships, crucial for many applications. These applications depend on the KG's factual accuracy, so verifying facts is essential, yet challenging. Expert manual verification is ideal but impractical on a large scale. Automated methods show promise but are not ready for real-world KGs. Large Language Models (LLMs) offer potential with their semantic understanding and knowledge access, yet their suitability and effectiveness for KG fact validation remain largely unexplored.\n  In this paper, we introduce FactCheck, a benchmark designed to evaluate LLMs for KG fact validation across three key dimensions: (1) LLMs internal knowledge; (2) external evidence via Retrieval-Augmented Generation (RAG); and (3) aggregated knowledge employing a multi-model consensus strategy. We evaluated open-source and commercial LLMs on three diverse real-world KGs. FactCheck also includes a RAG dataset with 2+ million documents tailored for KG fact validation. Additionally, we offer an interactive exploration platform for analyzing verification decisions.\n  The experimental analyses demonstrate that while LLMs yield promising results, they are still not sufficiently stable and reliable to be used in real-world KG validation scenarios. Integrating external evidence through RAG methods yields fluctuating performance, providing inconsistent improvements over more streamlined approaches -- at higher computational costs. Similarly, strategies based on multi-model consensus do not consistently outperform individual models, underscoring the lack of a one-fits-all solution. These findings further emphasize the need for a benchmark like FactCheck to systematically evaluate and drive progress on this difficult yet crucial task.","upvotes":6,"discussionId":"698d8ba365c0d15a6d1622ed","githubRepo":"https://github.com/FactCheck-AI/FactCheck","githubRepoAddedBy":"user","ai_summary":"Large language models show promise but lack stability and reliability for knowledge graph fact validation, with retrieval-augmented generation and multi-model consensus approaches yielding inconsistent improvements.","ai_keywords":["Knowledge Graphs","Large Language Models","fact validation","Retrieval-Augmented Generation","multi-model consensus","benchmark","external evidence"],"githubStars":1,"organization":{"_id":"67c6d5fb101a52633b95409f","name":"FactCheck-AI","fullname":"Fact Check","avatar":"https://cdn-uploads.huggingface.co/production/uploads/643515914ff6a19a90ca5865/OA61l6fEAKuQfaaLNwHT8.png"}},"publishedAt":"2026-02-11T06:24:46.000Z","title":"Benchmarking Large Language Models for Knowledge Graph Validation","summary":"Knowledge Graphs (KGs) store structured factual knowledge by linking entities through relationships, crucial for many applications. These applications depend on the KG's factual accuracy, so verifying facts is essential, yet challenging. Expert manual verification is ideal but impractical on a large scale. Automated methods show promise but are not ready for real-world KGs. Large Language Models (LLMs) offer potential with their semantic understanding and knowledge access, yet their suitability and effectiveness for KG fact validation remain largely unexplored.\n  In this paper, we introduce FactCheck, a benchmark designed to evaluate LLMs for KG fact validation across three key dimensions: (1) LLMs internal knowledge; (2) external evidence via Retrieval-Augmented Generation (RAG); and (3) aggregated knowledge employing a multi-model consensus strategy. We evaluated open-source and commercial LLMs on three diverse real-world KGs. FactCheck also includes a RAG dataset with 2+ million documents tailored for KG fact validation. Additionally, we offer an interactive exploration platform for analyzing verification decisions.\n  The experimental analyses demonstrate that while LLMs yield promising results, they are still not sufficiently stable and reliable to be used in real-world KG validation scenarios. Integrating external evidence through RAG methods yields fluctuating performance, providing inconsistent improvements over more streamlined approaches -- at higher computational costs. Similarly, strategies based on multi-model consensus do not consistently outperform individual models, underscoring the lack of a one-fits-all solution. These findings further emphasize the need for a benchmark like FactCheck to systematically evaluate and drive progress on this difficult yet crucial task.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/643515914ff6a19a90ca5865/Z89mkjK46nBVBSpDn3nw8.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10748.png","numComments":2,"submittedBy":{"_id":"643515914ff6a19a90ca5865","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643515914ff6a19a90ca5865/PUJUnMtjQSfyf58F5kE9m.jpeg","fullname":"Farzad Shami","name":"0xFarzad","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67c6d5fb101a52633b95409f","name":"FactCheck-AI","fullname":"Fact Check","avatar":"https://cdn-uploads.huggingface.co/production/uploads/643515914ff6a19a90ca5865/OA61l6fEAKuQfaaLNwHT8.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.10179","authors":[{"_id":"698d419065c0d15a6d1620ff","name":"Jiacheng Hou","hidden":false},{"_id":"698d419065c0d15a6d162100","name":"Yining Sun","hidden":false},{"_id":"698d419065c0d15a6d162101","name":"Ruochong Jin","hidden":false},{"_id":"698d419065c0d15a6d162102","name":"Haochen Han","hidden":false},{"_id":"698d419065c0d15a6d162103","name":"Fangming Liu","hidden":false},{"_id":"698d419065c0d15a6d162104","name":"Wai Kin Victor Chan","hidden":false},{"_id":"698d419065c0d15a6d162105","name":"Alex Jinpeng Wang","hidden":false}],"publishedAt":"2026-02-10T18:59:55.000Z","submittedOnDailyAt":"2026-02-12T00:27:39.577Z","title":"When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models","submittedOnDailyBy":{"_id":"62333a88fd7bb4a39b92d387","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png","isPro":false,"fullname":"Alex Jinpeng Wang","user":"Awiny","type":"user"},"summary":"Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.","upvotes":6,"discussionId":"698d419165c0d15a6d162106","ai_summary":"Visual-to-visual jailbreak attacks compromise image editing models through malicious visual inputs, necessitating new safety benchmarks and defense mechanisms.","ai_keywords":["Vision-Centric Jailbreak Attack","image editing models","visual-to-visual attack","IESBench","introspective multimodal reasoning","safety-oriented benchmark","attack success rate"],"organization":{"_id":"67ab7720792eebb05080c926","name":"CSU-JPG","fullname":"Jinpeng Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62333a88fd7bb4a39b92d387/MHfLrhVz0KqH6ydx1UrOc.jpeg"}},"publishedAt":"2026-02-10T13:59:55.000Z","title":"When the Prompt Becomes Visual: Vision-Centric Jailbreak Attacks for Large Image Editing Models","summary":"Recent advances in large image editing models have shifted the paradigm from text-driven instructions to vision-prompt editing, where user intent is inferred directly from visual inputs such as marks, arrows, and visual-text prompts. While this paradigm greatly expands usability, it also introduces a critical and underexplored safety risk: the attack surface itself becomes visual. In this work, we propose Vision-Centric Jailbreak Attack (VJA), the first visual-to-visual jailbreak attack that conveys malicious instructions purely through visual inputs. To systematically study this emerging threat, we introduce IESBench, a safety-oriented benchmark for image editing models. Extensive experiments on IESBench demonstrate that VJA effectively compromises state-of-the-art commercial models, achieving attack success rates of up to 80.9% on Nano Banana Pro and 70.1% on GPT-Image-1.5. To mitigate this vulnerability, we propose a training-free defense based on introspective multimodal reasoning, which substantially improves the safety of poorly aligned models to a level comparable with commercial systems, without auxiliary guard models and with negligible computational overhead. Our findings expose new vulnerabilities, provide both a benchmark and practical defense to advance safe and trustworthy modern image editing systems. Warning: This paper contains offensive images created by large image editing models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10179.png","numComments":2,"submittedBy":{"_id":"62333a88fd7bb4a39b92d387","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62333a88fd7bb4a39b92d387/e21AhpcXq37Ak_7rZ-Ca9.png","fullname":"Alex Jinpeng Wang","name":"Awiny","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"organization":{"_id":"67ab7720792eebb05080c926","name":"CSU-JPG","fullname":"Jinpeng Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62333a88fd7bb4a39b92d387/MHfLrhVz0KqH6ydx1UrOc.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.09901","authors":[{"_id":"698d472965c0d15a6d16214b","name":"Jianzhao Huang","hidden":false},{"_id":"698d472965c0d15a6d16214c","name":"Xiaorui Huang","hidden":false},{"_id":"698d472965c0d15a6d16214d","user":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","isPro":false,"fullname":"Fei Zhao","user":"Hiiamein","type":"user"},"name":"Fei Zhao","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:27.151Z","hidden":false},{"_id":"698d472965c0d15a6d16214e","name":"Yunpeng Liu","hidden":false},{"_id":"698d472965c0d15a6d16214f","name":"Hui Zhang","hidden":false},{"_id":"698d472965c0d15a6d162150","name":"Fangcheng Shi","hidden":false},{"_id":"698d472965c0d15a6d162151","name":"Congfeng Li","hidden":false},{"_id":"698d472965c0d15a6d162152","name":"Zechen Sun","hidden":false},{"_id":"698d472965c0d15a6d162153","name":"Yi Wu","hidden":false},{"_id":"698d472965c0d15a6d162154","name":"Yao Hu","hidden":false},{"_id":"698d472965c0d15a6d162155","name":"Yunhan Bai","hidden":false},{"_id":"698d472965c0d15a6d162156","name":"Shaosheng Cao","hidden":false}],"publishedAt":"2026-02-10T15:38:17.000Z","submittedOnDailyAt":"2026-02-12T00:52:56.265Z","title":"QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search","submittedOnDailyBy":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","isPro":false,"fullname":"Fei Zhao","user":"Hiiamein","type":"user"},"summary":"Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.","upvotes":6,"discussionId":"698d472965c0d15a6d162157","ai_summary":"A unified generative large language model approach for social network search query processing that improves semantic understanding through multi-task learning and reinforcement learning while enhancing downstream task performance.","ai_keywords":["Large Language Models","query processing","discriminative models","sequence generation","progressive three-stage alignment","multi-reward Reinforcement Learning","intent descriptions","query rewriting","ranking","offline evaluations","online A/B tests"]},"publishedAt":"2026-02-10T10:38:17.000Z","title":"QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search","summary":"Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09901.png","numComments":2,"submittedBy":{"_id":"65328aa39326d6da5ff19b52","avatarUrl":"/avatars/5c3de984cd6eba69616bb608796865c5.svg","fullname":"Fei Zhao","name":"Hiiamein","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.10229","authors":[{"_id":"698e4757cace060ff123abca","name":"Weihao Liu","hidden":false},{"_id":"698e4757cace060ff123abcb","name":"Dehai Min","hidden":false},{"_id":"698e4757cace060ff123abcc","name":"Lu Cheng","hidden":false}],"publishedAt":"2026-02-10T19:19:10.000Z","submittedOnDailyAt":"2026-02-12T19:05:43.400Z","title":"Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens","submittedOnDailyBy":{"_id":"629c6ee73a3221bb210afc2d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg","isPro":false,"fullname":"Dehai Min","user":"ZhishanQ","type":"user"},"summary":"While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.","upvotes":5,"discussionId":"698e4758cace060ff123abcd","githubRepo":"https://github.com/NeosKnight233/Latent-Thoughts-Tuning","githubRepoAddedBy":"user","ai_summary":"Latent Thoughts Tuning introduces a novel framework for reasoning in continuous latent space by combining contextual hidden states with predictive semantic guidance, enabling robust inference through a progressive curriculum learning approach.","ai_keywords":["Chain-of-Thought","Large Language Models","latent space","hidden states","vocabulary embedding space","Context-Prediction-Fusion","curriculum learning","feature collapse","reasoning accuracy"],"githubStars":4,"organization":{"_id":"696e94fe3c4c7e3aa42e8692","name":"UIC-R2-lab","fullname":"University of Illinois Chicago-R2-lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/629c6ee73a3221bb210afc2d/AeY9Wy5SHbD6BjlxJzF88.png"}},"publishedAt":"2026-02-10T14:19:10.000Z","title":"Latent Thoughts Tuning: Bridging Context and Reasoning with Fused Information in Latent Tokens","summary":"While explicit Chain-of-Thought (CoT) equips Large Language Models (LLMs) with strong reasoning capabilities, it requires models to verbalize every intermediate step in text tokens, constraining the model thoughts to the discrete vocabulary space. Recently, reasoning in continuous latent space has emerged as a promising alternative, enabling more robust inference and flexible computation beyond discrete token constraints. However, current latent paradigms often suffer from feature collapse and instability, stemming from distribution mismatches when recurrently using hidden states as the input embeddings, or alignment issues when relying on assistant models. To address this, we propose Latent Thoughts Tuning (LT-Tuning), a framework that redefines how latent thoughts are constructed and deployed. Instead of relying solely on raw hidden states, our method introduces a Context-Prediction-Fusion mechanism that jointly leveraging contextual hidden states and predictive semantic guidance from the vocabulary embedding space. Combined with a progressive three-stage curriculum learning pipeline, LT-Tuning also enables dynamically switching between latent and explicit thinking modes. Experiments demonstrate that our method outperforms existing latent reasoning baselines, effectively mitigating feature collapse and achieving robust reasoning accuracy.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10229.png","numComments":2,"submittedBy":{"_id":"629c6ee73a3221bb210afc2d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg","fullname":"Dehai Min","name":"ZhishanQ","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"696e94fe3c4c7e3aa42e8692","name":"UIC-R2-lab","fullname":"University of Illinois Chicago-R2-lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/629c6ee73a3221bb210afc2d/AeY9Wy5SHbD6BjlxJzF88.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.08489","authors":[{"_id":"698c8152cea6ff55f58f0837","user":{"_id":"64b776ce81fbedb3938e7c0f","avatarUrl":"/avatars/684ad0a998acd9266e988fffbf396a4c.svg","isPro":false,"fullname":"HyunseokLee","user":"hyunseoki","type":"user"},"name":"Hyunseok Lee","status":"claimed_verified","statusLastChangedAt":"2026-02-11T22:16:28.996Z","hidden":false},{"_id":"698c8152cea6ff55f58f0838","name":"Soheil Abbasloo","hidden":false},{"_id":"698c8152cea6ff55f58f0839","name":"Jihoon Tack","hidden":false},{"_id":"698c8152cea6ff55f58f083a","name":"Jinwoo Shin","hidden":false}],"publishedAt":"2026-02-09T10:41:44.000Z","submittedOnDailyAt":"2026-02-12T02:28:07.317Z","title":"Beyond Correctness: Learning Robust Reasoning via Transfer","submittedOnDailyBy":{"_id":"64b776ce81fbedb3938e7c0f","avatarUrl":"/avatars/684ad0a998acd9266e988fffbf396a4c.svg","isPro":false,"fullname":"HyunseokLee","user":"hyunseoki","type":"user"},"summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.","upvotes":5,"discussionId":"698c8152cea6ff55f58f083b","ai_summary":"Reinforcement Learning with Transferable Reward (RLTR) enhances LLM reasoning robustness by ensuring reasoning stability and generalizability through transfer rewards that test cross-model guidance capabilities.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","Reinforcement Learning with Transferable Reward","LLM reasoning","transfer reward","reasoning robustness","cross-model guidance","sampling consistency","final answer accuracy","MATH500","Maj@64"]},"publishedAt":"2026-02-09T05:41:44.000Z","title":"Beyond Correctness: Learning Robust Reasoning via Transfer","summary":"Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08489.png","numComments":2,"submittedBy":{"_id":"64b776ce81fbedb3938e7c0f","avatarUrl":"/avatars/684ad0a998acd9266e988fffbf396a4c.svg","fullname":"HyunseokLee","name":"hyunseoki","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.08030","authors":[{"_id":"698c7b76eb12ea74539168fc","name":"Yilun Zheng","hidden":false},{"_id":"698c7b76eb12ea74539168fd","name":"Dongyang Ma","hidden":false},{"_id":"698c7b76eb12ea74539168fe","name":"Tian Liang","hidden":false},{"_id":"698c7b76eb12ea74539168ff","name":"Jiahao Xu","hidden":false},{"_id":"698c7b76eb12ea7453916900","name":"Xinting Huang","hidden":false},{"_id":"698c7b76eb12ea7453916901","name":"Lihui Chen","hidden":false},{"_id":"698c7b76eb12ea7453916902","name":"Haitao Mi","hidden":false},{"_id":"698c7b76eb12ea7453916903","user":{"_id":"64674375946476c5d215382d","avatarUrl":"/avatars/0f0b98080c064a17d003da8a22a047cc.svg","isPro":false,"fullname":"Yan Wang","user":"libertywing","type":"user"},"name":"Yan Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:33.824Z","hidden":false}],"publishedAt":"2026-02-08T16:04:23.000Z","submittedOnDailyAt":"2026-02-12T02:21:55.413Z","title":"Free(): Learning to Forget in Malloc-Only Reasoning Models","submittedOnDailyBy":{"_id":"64674375946476c5d215382d","avatarUrl":"/avatars/0f0b98080c064a17d003da8a22a047cc.svg","isPro":false,"fullname":"Yan Wang","user":"libertywing","type":"user"},"summary":"Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.\n  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.","upvotes":5,"discussionId":"698c7b76eb12ea7453916904","githubRepo":"https://github.com/TemporaryLoRA/FreeLM","githubRepoAddedBy":"user","ai_summary":"Free()LM addresses reasoning model limitations by introducing a self-forgetting mechanism through a Free-Module plug-and-play LoRA adapter, improving performance across scales and long-horizon tasks.","ai_keywords":["LLMs","reasoning models","test-time compute","thinking tokens","malloc-only engines","Free-Module","LoRA adapter","reasoning mode","cleaning mode","context pruning","Qwen3-235B-A22B","DeepSeek V3.2-Speciale","IMOanswerBench"],"githubStars":10,"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}},"publishedAt":"2026-02-08T11:04:23.000Z","title":"Free(): Learning to Forget in Malloc-Only Reasoning Models","summary":"Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.\n  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08030.png","numComments":2,"submittedBy":{"_id":"64674375946476c5d215382d","avatarUrl":"/avatars/0f0b98080c064a17d003da8a22a047cc.svg","fullname":"Yan Wang","name":"libertywing","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"66543b6e420092799d2f625c","name":"tencent","fullname":"Tencent","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.07954","authors":[{"_id":"698d864f65c0d15a6d1622b2","user":{"_id":"5e47d3eb178ca95365287400","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png","isPro":false,"fullname":"Krzysztof Wrbel","user":"djstrong","type":"user"},"name":"Krzysztof Wrbel","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:26:03.384Z","hidden":false},{"_id":"698d864f65c0d15a6d1622b3","name":"Jan Maria Kowalski","hidden":false},{"_id":"698d864f65c0d15a6d1622b4","name":"Jerzy Surma","hidden":false},{"_id":"698d864f65c0d15a6d1622b5","name":"Igor Ciuciura","hidden":false},{"_id":"698d864f65c0d15a6d1622b6","name":"Maciej Szymaski","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/5e47d3eb178ca95365287400/Wxb4zW8c0qwL55uFugG-K.png"],"publishedAt":"2026-02-08T12:57:04.000Z","submittedOnDailyAt":"2026-02-12T05:26:03.855Z","title":"Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation","submittedOnDailyBy":{"_id":"5e47d3eb178ca95365287400","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png","isPro":false,"fullname":"Krzysztof Wrbel","user":"djstrong","type":"user"},"summary":"As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65%) and very low false positive rate (0.63%) on real user prompts, outperforming HerBERT-PL-Guard (31.55% precision, 4.70% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.","upvotes":4,"discussionId":"698d865065c0d15a6d1622b7","projectPage":"https://guard.bielik.ai/","ai_summary":"Bielik Guard is a compact Polish language safety classifier family with two variants that effectively categorize content across five safety domains while maintaining high efficiency and accuracy.","ai_keywords":["Large Language Models","content safety classifiers","MMLW-RoBERTa-base","PKOBP/polish-roberta-8k","fine-tuned models","F1 scores","precision","false positive rate"],"organization":{"_id":"64c20f6b5b329de1b1df6522","name":"speakleash","fullname":"SpeakLeash | Spichlerz","avatar":"https://cdn-uploads.huggingface.co/production/uploads/628e3a115ec2eb796f94c1ed/5AbxAg0YRjMpCmv8Q4ehE.jpeg"}},"publishedAt":"2026-02-08T07:57:04.000Z","title":"Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation","summary":"As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65%) and very low false positive rate (0.63%) on real user prompts, outperforming HerBERT-PL-Guard (31.55% precision, 4.70% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/5e47d3eb178ca95365287400/Wxb4zW8c0qwL55uFugG-K.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07954.png","numComments":2,"submittedBy":{"_id":"5e47d3eb178ca95365287400","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1633554153914-5e47d3eb178ca95365287400.png","fullname":"Krzysztof Wrbel","name":"djstrong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":17,"isUserFollowing":false},"organization":{"_id":"64c20f6b5b329de1b1df6522","name":"speakleash","fullname":"SpeakLeash | Spichlerz","avatar":"https://cdn-uploads.huggingface.co/production/uploads/628e3a115ec2eb796f94c1ed/5AbxAg0YRjMpCmv8Q4ehE.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.07900","authors":[{"_id":"698dd9a8a873bfa20696c757","name":"Zhi Chen","hidden":false},{"_id":"698dd9a8a873bfa20696c758","name":"Zhensu Sun","hidden":false},{"_id":"698dd9a8a873bfa20696c759","user":{"_id":"645b0c3ec35da9c7afd95421","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg","isPro":false,"fullname":"Yuling","user":"YerbaPage","type":"user"},"name":"Yuling Shi","status":"claimed_verified","statusLastChangedAt":"2026-02-12T20:23:23.260Z","hidden":false},{"_id":"698dd9a8a873bfa20696c75a","name":"Chao Peng","hidden":false},{"_id":"698dd9a8a873bfa20696c75b","name":"Xiaodong Gu","hidden":false},{"_id":"698dd9a8a873bfa20696c75c","name":"David Lo","hidden":false},{"_id":"698dd9a8a873bfa20696c75d","name":"Lingxiao Jiang","hidden":false}],"publishedAt":"2026-02-08T10:26:31.000Z","submittedOnDailyAt":"2026-02-12T11:17:18.664Z","title":"Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents","submittedOnDailyBy":{"_id":"65ce10de8b4adee87192431d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65ce10de8b4adee87192431d/IOg4NGa2_NUxIBvVn0_a-.jpeg","isPro":true,"fullname":"Chen Zhi","user":"hellochenzhi","type":"user"},"summary":"Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.\n  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.","upvotes":4,"discussionId":"698dd9a8a873bfa20696c75e","ai_summary":"Empirical analysis of LLM code agents reveals that test writing provides limited improvement in issue resolution and is often replaced by observation-based debugging methods.","ai_keywords":["large language model","code agents","SWE-bench","agent trajectories","test writing","formal assertion-based checks","value-revealing print statements"],"organization":{"_id":"6296f43820dc74838613d1ba","name":"SingaporeManagementUniversity","fullname":"Singapore Management University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1654060072621-6296f3e0b493a00f946220de.png"}},"publishedAt":"2026-02-08T05:26:31.000Z","title":"Rethinking the Value of Agent-Generated Tests for LLM-Based Software Engineering Agents","summary":"Large Language Model (LLM) code agents increasingly resolve repository-level issues by iteratively editing code, invoking tools, and validating candidate patches. In these workflows, agents often write tests on the fly, a paradigm adopted by many high-ranking agents on the SWE-bench leaderboard. However, we observe that GPT-5.2, which writes almost no new tests, can even achieve performance comparable to top-ranking agents. This raises the critical question: whether such tests meaningfully improve issue resolution or merely mimic human testing practices while consuming a substantial interaction budget.\n  To reveal the impact of agent-written tests, we present an empirical study that analyzes agent trajectories across six state-of-the-art LLMs on SWE-bench Verified. Our results show that while test writing is commonly adopted, but resolved and unresolved tasks within the same model exhibit similar test-writing frequencies Furthermore, these tests typically serve as observational feedback channels, where agents prefer value-revealing print statements significantly more than formal assertion-based checks. Based on these insights, we perform a controlled experiment by revising the prompts of four agents to either increase or reduce test writing. The results suggest that changes in the volume of agent-written tests do not significantly change final outcomes. Taken together, our study reveals that current test-writing practices may provide marginal utility in autonomous software engineering tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07900.png","numComments":2,"submittedBy":{"_id":"65ce10de8b4adee87192431d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65ce10de8b4adee87192431d/IOg4NGa2_NUxIBvVn0_a-.jpeg","fullname":"Chen Zhi","name":"hellochenzhi","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6296f43820dc74838613d1ba","name":"SingaporeManagementUniversity","fullname":"Singapore Management University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1654060072621-6296f3e0b493a00f946220de.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.06008","authors":[{"_id":"6989932ebeecc443208d2773","user":{"_id":"658d6e3729ef008a12cc9817","avatarUrl":"/avatars/f1062c71ad3d76e15875b7becd7a8a13.svg","isPro":false,"fullname":"Xianyang Liu","user":"XianyangLiu","type":"user"},"name":"Xianyang Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-09T08:27:44.932Z","hidden":false},{"_id":"6989932ebeecc443208d2774","name":"Shangding Gu","hidden":false},{"_id":"6989932ebeecc443208d2775","name":"Dawn Song","hidden":false}],"publishedAt":"2026-02-05T18:50:36.000Z","submittedOnDailyAt":"2026-02-12T00:19:40.602Z","title":"AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions","submittedOnDailyBy":{"_id":"658d6e3729ef008a12cc9817","avatarUrl":"/avatars/f1062c71ad3d76e15875b7becd7a8a13.svg","isPro":false,"fullname":"Xianyang Liu","user":"XianyangLiu","type":"user"},"summary":"Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.","upvotes":4,"discussionId":"6989932ebeecc443208d2776","projectPage":"https://agenticpay-tutorial.readthedocs.io/en/latest/","githubRepo":"https://github.com/SafeRL-Lab/AgenticPay","githubRepoAddedBy":"admin","ai_summary":"AgenticPay presents a benchmark and simulation framework for evaluating multi-agent language-mediated economic interactions, focusing on negotiation performance and strategic reasoning challenges in complex market scenarios.","ai_keywords":["multi-agent negotiation","language-mediated interaction","strategic reasoning","economic interaction","buyer-seller markets","natural language processing","large language models","agent-based simulation"],"githubStars":8,"organization":{"_id":"61f20a9ce108f2cba2dc0730","name":"Berkeley","fullname":"UC Berkeley","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}},"publishedAt":"2026-02-05T13:50:36.000Z","title":"AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions","summary":"Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06008.png","numComments":2,"submittedBy":{"_id":"658d6e3729ef008a12cc9817","avatarUrl":"/avatars/f1062c71ad3d76e15875b7becd7a8a13.svg","fullname":"Xianyang Liu","name":"XianyangLiu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"61f20a9ce108f2cba2dc0730","name":"Berkeley","fullname":"UC Berkeley","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.10778","authors":[{"_id":"698daf1a8796581b3e5a65bf","name":"Maximilian Thang","hidden":false},{"_id":"698daf1a8796581b3e5a65c0","user":{"_id":"6786535551cd69b65efd2bee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png","isPro":false,"fullname":"Lichao Wu","user":"woorkhaarder","type":"user"},"name":"Lichao Wu","status":"claimed_verified","statusLastChangedAt":"2026-02-12T11:55:47.598Z","hidden":false},{"_id":"698daf1a8796581b3e5a65c1","name":"Sasha Behrouzi","hidden":false},{"_id":"698daf1a8796581b3e5a65c2","name":"Mohamadreza Rostami","hidden":false},{"_id":"698daf1a8796581b3e5a65c3","name":"Jona te Lintelo","hidden":false},{"_id":"698daf1a8796581b3e5a65c4","name":"Stjepan Picek","hidden":false},{"_id":"698daf1a8796581b3e5a65c5","name":"Ahmad-Reza Sadeghi","hidden":false}],"publishedAt":"2026-02-11T12:10:14.000Z","submittedOnDailyAt":"2026-02-12T08:15:53.554Z","title":"GoodVibe: Security-by-Vibe for LLM-Based Code Generation","submittedOnDailyBy":{"_id":"6786535551cd69b65efd2bee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png","isPro":false,"fullname":"Lichao Wu","user":"woorkhaarder","type":"user"},"summary":"Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.\n  We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.","upvotes":3,"discussionId":"698daf1a8796581b3e5a65c6","ai_summary":"GoodVibe is a neuron-level framework that enhances code language model security through targeted fine-tuning of security-relevant neurons while maintaining model utility and significantly reducing training costs.","ai_keywords":["neuron-level framework","security-relevant reasoning","gradient-based attribution","neuron-selective fine-tuning","activation-driven neuron clustering","parameter-efficient adaptations","catastrophic forgetting","supervised security task","trainable parameters","training computation"],"organization":{"_id":"656dd6ea7c934a7b3c4c59c2","name":"is-tuda","fullname":"Technical University of Darmstadt - Information Systems","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6464e09d0e6c7618f611d9d6/WXo06EWxaZCtWdSUftCzi.png"}},"publishedAt":"2026-02-11T07:10:14.000Z","title":"GoodVibe: Security-by-Vibe for LLM-Based Code Generation","summary":"Large language models (LLMs) are increasingly used for code generation in fast, informal development workflows, often referred to as vibe coding, where speed and convenience are prioritized, and security requirements are rarely made explicit. In this setting, models frequently produce functionally correct but insecure code, creating a growing security risk. Existing approaches to improving code security rely on full-parameter fine-tuning or parameter-efficient adaptations, which are either costly and prone to catastrophic forgetting or operate at coarse granularity with limited interpretability and control.\n  We present GoodVibe, a neuron-level framework for improving the security of code language models by default. GoodVibe is based on the key insight that security-relevant reasoning is localized to a small subset of neurons. We identify these neurons using gradient-based attribution from a supervised security task and perform neuron-selective fine-tuning that updates only this security-critical subspace. To further reduce training cost, we introduce activation-driven neuron clustering, enabling structured updates with minimal overhead. We evaluate GoodVibe on six LLMs across security-critical programming languages, including C++, Java, Swift, and Go. GoodVibe substantially improves the security of generated code while preserving general model utility, achieving up to a 2.5x improvement over base models, matching or exceeding full fine-tuning with over 4,700x fewer trainable parameters, and reducing training computation by more than 3.6x compared to the parameter-efficient baseline (LoRA). Our results demonstrate that neuron-level optimization offers an effective and scalable approach to securing code generation without sacrificing efficiency or generality.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10778.png","numComments":3,"submittedBy":{"_id":"6786535551cd69b65efd2bee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png","fullname":"Lichao Wu","name":"woorkhaarder","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"656dd6ea7c934a7b3c4c59c2","name":"is-tuda","fullname":"Technical University of Darmstadt - Information Systems","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6464e09d0e6c7618f611d9d6/WXo06EWxaZCtWdSUftCzi.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.09014","authors":[{"_id":"698ab6301b2dc6b37d61b008","user":{"_id":"64a6a1ecb1f875f03fa05a8d","avatarUrl":"/avatars/4ca06e57d43c66c76b71070f02003d7b.svg","isPro":false,"fullname":"Zihan Yang","user":"ymyy307","type":"user"},"name":"Zihan Yang","status":"admin_assigned","statusLastChangedAt":"2026-02-11T16:06:35.630Z","hidden":false},{"_id":"698ab6301b2dc6b37d61b009","name":"Shuyuan Tu","hidden":false},{"_id":"698ab6301b2dc6b37d61b00a","name":"Licheng Zhang","hidden":false},{"_id":"698ab6301b2dc6b37d61b00b","name":"Qi Dai","hidden":false},{"_id":"698ab6301b2dc6b37d61b00c","name":"Yu-Gang Jiang","hidden":false},{"_id":"698ab6301b2dc6b37d61b00d","name":"Zuxuan Wu","hidden":false}],"publishedAt":"2026-02-09T18:56:14.000Z","submittedOnDailyAt":"2026-02-12T00:23:58.363Z","title":"ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation","submittedOnDailyBy":{"_id":"64a6a1ecb1f875f03fa05a8d","avatarUrl":"/avatars/4ca06e57d43c66c76b71070f02003d7b.svg","isPro":false,"fullname":"Zihan Yang","user":"ymyy307","type":"user"},"summary":"Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.","upvotes":3,"discussionId":"698ab6301b2dc6b37d61b00e","githubRepo":"https://github.com/pnotp/ArcFlow","githubRepoAddedBy":"user","ai_summary":"ArcFlow is a few-step distillation framework that uses non-linear flow trajectories to approximate teacher diffusion models, achieving fast inference with minimal quality loss through lightweight adapter training.","ai_keywords":["diffusion models","distillation","velocity field","momentum processes","trajectory distillation","denoising steps","NFEs","teacher trajectory","non-linear flow trajectories","adaptive distillation"],"githubStars":60,"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}},"publishedAt":"2026-02-09T13:56:14.000Z","title":"ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation","summary":"Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09014.png","numComments":2,"submittedBy":{"_id":"64a6a1ecb1f875f03fa05a8d","avatarUrl":"/avatars/4ca06e57d43c66c76b71070f02003d7b.svg","fullname":"Zihan Yang","name":"ymyy307","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"643cb0625fcffe09fb6ca688","name":"Fudan-University","fullname":"Fudan University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.02459","authors":[{"_id":"698d0ac16c5152984e4f3e2f","name":"Zhiyu Huang","hidden":false},{"_id":"698d0ac16c5152984e4f3e30","name":"Yun Zhang","hidden":false},{"_id":"698d0ac16c5152984e4f3e31","name":"Johnson Liu","hidden":false},{"_id":"698d0ac16c5152984e4f3e32","name":"Rui Song","hidden":false},{"_id":"698d0ac16c5152984e4f3e33","name":"Chen Tang","hidden":false},{"_id":"698d0ac16c5152984e4f3e34","name":"Jiaqi Ma","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6684a5eddc6aeba7612cbf29/KCWPMv_0QBY5U1AklVEQi.png","https://cdn-uploads.huggingface.co/production/uploads/6684a5eddc6aeba7612cbf29/DqTA7a49D_z1LZp3ph57l.png"],"publishedAt":"2026-02-02T18:47:49.000Z","submittedOnDailyAt":"2026-02-12T21:01:57.295Z","title":"TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments","submittedOnDailyBy":{"_id":"6684a5eddc6aeba7612cbf29","avatarUrl":"/avatars/d90485fc02384f594c0e65ced118697c.svg","isPro":false,"fullname":"Yun Zhang","user":"handsomeYun","type":"user"},"summary":"Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/","upvotes":3,"discussionId":"698d0ac16c5152984e4f3e35","projectPage":"https://ucla-mobility.github.io/TIC-VLA/","githubRepo":"https://github.com/ucla-mobility/TIC-VLA","githubRepoAddedBy":"user","ai_summary":"Vision-language-action models for robotics are enhanced with a latency-aware framework that compensates for delayed semantic reasoning during real-time action generation through delayed semantic-control interfaces and latency-consistent training.","ai_keywords":["vla models","vision-language-action","delayed semantic reasoning","real-time reactive control","latency-aware framework","delayed semantic-control interface","imitation learning","online reinforcement learning","physics-accurate simulation","photo-realistic simulation"],"githubStars":18,"organization":{"_id":"67784c39dac147922d8d09f0","name":"UCLA","fullname":"University of California, Los Angeles","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"}},"publishedAt":"2026-02-02T13:47:49.000Z","title":"TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments","summary":"Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6684a5eddc6aeba7612cbf29/KCWPMv_0QBY5U1AklVEQi.png","https://cdn-uploads.huggingface.co/production/uploads/6684a5eddc6aeba7612cbf29/DqTA7a49D_z1LZp3ph57l.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02459.png","numComments":2,"submittedBy":{"_id":"6684a5eddc6aeba7612cbf29","avatarUrl":"/avatars/d90485fc02384f594c0e65ced118697c.svg","fullname":"Yun Zhang","name":"handsomeYun","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"67784c39dac147922d8d09f0","name":"UCLA","fullname":"University of California, Los Angeles","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.11137","authors":[{"_id":"698d61c965c0d15a6d16222b","name":"Tessa Han","hidden":false},{"_id":"698d61c965c0d15a6d16222c","name":"Sebastian Bordt","hidden":false},{"_id":"698d61c965c0d15a6d16222d","name":"Hanlin Zhang","hidden":false},{"_id":"698d61c965c0d15a6d16222e","name":"Sham Kakade","hidden":false}],"publishedAt":"2026-02-11T18:49:26.000Z","submittedOnDailyAt":"2026-02-12T02:46:24.426Z","title":"Weight Decay Improves Language Model Plasticity","submittedOnDailyBy":{"_id":"624054bcc2c17da6a63eb539","avatarUrl":"/avatars/bf52dc0683b4100733f8696a97696d0e.svg","isPro":true,"fullname":"hlzhang109","user":"hlzhang109","type":"user"},"summary":"The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.","upvotes":2,"discussionId":"698d61c965c0d15a6d16222f","ai_summary":"Pretraining with larger weight decay values improves model plasticity and downstream fine-tuning performance by encouraging linearly separable representations and reducing overfitting.","ai_keywords":["large language models","pretraining","fine-tuning","weight decay","model plasticity","cross-entropy loss","attention matrices","overfitting"]},"publishedAt":"2026-02-11T13:49:26.000Z","title":"Weight Decay Improves Language Model Plasticity","summary":"The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay's mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11137.png","numComments":2,"submittedBy":{"_id":"624054bcc2c17da6a63eb539","avatarUrl":"/avatars/bf52dc0683b4100733f8696a97696d0e.svg","fullname":"hlzhang109","name":"hlzhang109","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.10652","authors":[{"_id":"698d4a6165c0d15a6d16217c","name":"Yongshi Ye","hidden":false},{"_id":"698d4a6165c0d15a6d16217d","name":"Hui Jiang","hidden":false},{"_id":"698d4a6165c0d15a6d16217e","name":"Feihu Jiang","hidden":false},{"_id":"698d4a6165c0d15a6d16217f","user":{"_id":"627ca439a09edfe62239c671","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg","isPro":false,"fullname":"Tian Lan","user":"GMFTBY","type":"user"},"name":"Tian Lan","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:28:18.467Z","hidden":false},{"_id":"698d4a6165c0d15a6d162180","name":"Yichao Du","hidden":false},{"_id":"698d4a6165c0d15a6d162181","name":"Biao Fu","hidden":false},{"_id":"698d4a6165c0d15a6d162182","name":"Xiaodong Shi","hidden":false},{"_id":"698d4a6165c0d15a6d162183","name":"Qianghuai Jia","hidden":false},{"_id":"698d4a6165c0d15a6d162184","name":"Longyue Wang","hidden":false},{"_id":"698d4a6165c0d15a6d162185","name":"Weihua Luo","hidden":false}],"publishedAt":"2026-02-11T08:58:41.000Z","submittedOnDailyAt":"2026-02-12T01:16:05.124Z","title":"UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory","submittedOnDailyBy":{"_id":"627ca439a09edfe62239c671","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg","isPro":false,"fullname":"Tian Lan","user":"GMFTBY","type":"user"},"summary":"Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.","upvotes":2,"discussionId":"698d4a6165c0d15a6d162186","ai_summary":"A unified framework for memory extraction and management in LLM-based agents that improves generalization through semantic neighborhood modeling and marginal utility rewards.","ai_keywords":["self-evolving memory","Large Language Models","memory extraction","memory management","Semantic Neighborhood Modeling","GRPO","marginal utility reward","multi-turn interactive tasks"],"organization":{"_id":"6662a91edd706a226d18cc5a","name":"AIDC-AI","fullname":"AIDC-AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/666a9d46a638e57bb7907929/CRc-9MCuH2q9hjTScyTPE.png"}},"publishedAt":"2026-02-11T03:58:41.000Z","title":"UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory","summary":"Self-evolving memory serves as the trainable parameters for Large Language Models (LLMs)-based agents, where extraction (distilling insights from experience) and management (updating the memory bank) must be tightly coordinated. Existing methods predominately optimize memory management while treating memory extraction as a static process, resulting in poor generalization, where agents accumulate instance-specific noise rather than robust memories. To address this, we propose Unified Memory Extraction and Management (UMEM), a self-evolving agent framework that jointly optimizes a Large Language Model to simultaneous extract and manage memories. To mitigate overfitting to specific instances, we introduce Semantic Neighborhood Modeling and optimize the model with a neighborhood-level marginal utility reward via GRPO. This approach ensures memory generalizability by evaluating memory utility across clusters of semantically related queries. Extensive experiments across five benchmarks demonstrate that UMEM significantly outperforms highly competitive baselines, achieving up to a 10.67% improvement in multi-turn interactive tasks. Futhermore, UMEM maintains a monotonic growth curve during continuous evolution. Codes and models will be publicly released.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10652.png","numComments":2,"submittedBy":{"_id":"627ca439a09edfe62239c671","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1652335660508-noauth.jpeg","fullname":"Tian Lan","name":"GMFTBY","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":18,"isUserFollowing":false},"organization":{"_id":"6662a91edd706a226d18cc5a","name":"AIDC-AI","fullname":"AIDC-AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/666a9d46a638e57bb7907929/CRc-9MCuH2q9hjTScyTPE.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08995","authors":[{"_id":"698ab3181b2dc6b37d61af84","user":{"_id":"65ace92f64c9b93eca5c2bce","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65ace92f64c9b93eca5c2bce/pG0JRXH-8zEy0IoaEnMNw.jpeg","isPro":false,"fullname":"Yuting Ning","user":"nnnyt","type":"user"},"name":"Yuting Ning","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:56.664Z","hidden":false},{"_id":"698ab3181b2dc6b37d61af85","user":{"_id":"637029f831af06da86518bc4","avatarUrl":"/avatars/b569b77e7f261ef5dc0b072fed61a5ba.svg","isPro":false,"fullname":"Jaylen Jones ","user":"jjones62202","type":"user"},"name":"Jaylen Jones","status":"claimed_verified","statusLastChangedAt":"2026-02-10T09:06:25.356Z","hidden":false},{"_id":"698ab3181b2dc6b37d61af86","name":"Zhehao Zhang","hidden":false},{"_id":"698ab3181b2dc6b37d61af87","name":"Chentao Ye","hidden":false},{"_id":"698ab3181b2dc6b37d61af88","name":"Weitong Ruan","hidden":false},{"_id":"698ab3181b2dc6b37d61af89","name":"Junyi Li","hidden":false},{"_id":"698ab3181b2dc6b37d61af8a","name":"Rahul Gupta","hidden":false},{"_id":"698ab3181b2dc6b37d61af8b","name":"Huan Sun","hidden":false}],"publishedAt":"2026-02-09T18:41:15.000Z","submittedOnDailyAt":"2026-02-12T00:25:39.625Z","title":"When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents","submittedOnDailyBy":{"_id":"65ace92f64c9b93eca5c2bce","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65ace92f64c9b93eca5c2bce/pG0JRXH-8zEy0IoaEnMNw.jpeg","isPro":false,"fullname":"Yuting Ning","user":"nnnyt","type":"user"},"summary":"Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.","upvotes":2,"discussionId":"698ab3181b2dc6b37d61af8c","projectPage":"https://osu-nlp-group.github.io/Misaligned-Action-Detection/","githubRepo":"https://github.com/OSU-NLP-Group/Misaligned-Action-Detection","githubRepoAddedBy":"user","ai_summary":"Computer-use agents face safety risks from misaligned actions caused by external attacks or internal limitations, prompting the development of DeAction, a guardrail that detects and corrects such actions before execution.","ai_keywords":["computer-use agents","misaligned actions","prompt injection","erroneous reasoning","guardrail","structured feedback","attack success rate","task success rate"],"githubStars":3,"organization":{"_id":"6127b4827dcb442c226129da","name":"osunlp","fullname":"OSU NLP Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6477a323dbc2a416f8b852b3/oiPPBo_knuDrz0YN9slKj.png"}},"publishedAt":"2026-02-09T13:41:15.000Z","title":"When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents","summary":"Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08995.png","numComments":2,"submittedBy":{"_id":"65ace92f64c9b93eca5c2bce","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65ace92f64c9b93eca5c2bce/pG0JRXH-8zEy0IoaEnMNw.jpeg","fullname":"Yuting Ning","name":"nnnyt","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6127b4827dcb442c226129da","name":"osunlp","fullname":"OSU NLP Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6477a323dbc2a416f8b852b3/oiPPBo_knuDrz0YN9slKj.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.08741","authors":[{"_id":"698daf818796581b3e5a65c8","name":"Jona te Lintelo","hidden":false},{"_id":"698daf818796581b3e5a65c9","user":{"_id":"6786535551cd69b65efd2bee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png","isPro":false,"fullname":"Lichao Wu","user":"woorkhaarder","type":"user"},"name":"Lichao Wu","status":"claimed_verified","statusLastChangedAt":"2026-02-12T11:55:45.562Z","hidden":false},{"_id":"698daf818796581b3e5a65ca","name":"Stjepan Picek","hidden":false}],"publishedAt":"2026-02-09T14:42:11.000Z","submittedOnDailyAt":"2026-02-12T08:17:52.060Z","title":"Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing","submittedOnDailyBy":{"_id":"6786535551cd69b65efd2bee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png","isPro":false,"fullname":"Lichao Wu","user":"woorkhaarder","type":"user"},"summary":"The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L^3), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L^3 learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L^3 on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods.","upvotes":2,"discussionId":"698daf828796581b3e5a65cb","githubRepo":"https://github.com/jonatelintelo/LargeLanguageLobotomy","githubRepoAddedBy":"user","ai_summary":"Attack method exploits expert routing dynamics in Mixture-of-Experts language models to compromise safety alignment while maintaining language utility.","ai_keywords":["Mixture-of-Experts","Large Language Models","expert routing","safety alignment","adversarial attack","expert silencing","jailbreak methods"],"githubStars":1,"organization":{"_id":"633d6fcf748f94ec2daa31c3","name":"RadboudUniversity","fullname":"Radboud University Nijmegen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/A0fLxEuJaWuCA0fFfp9u6.png"}},"publishedAt":"2026-02-09T09:42:11.000Z","title":"Large Language Lobotomy: Jailbreaking Mixture-of-Experts via Expert Silencing","summary":"The rapid adoption of Mixture-of-Experts (MoE) architectures marks a major shift in the deployment of Large Language Models (LLMs). MoE LLMs improve scaling efficiency by activating only a small subset of parameters per token, but their routing structure introduces new safety attack surfaces. We find that safety-critical behaviors in MoE LLMs (e.g., refusal) are concentrated in a small set of experts rather than being uniformly distributed. Building on this, we propose Large Language Lobotomy (L^3), a training-free, architecture-agnostic attack that compromises safety alignment by exploiting expert routing dynamics. L^3 learns routing patterns that correlate with refusal, attributes safety behavior to specific experts, and adaptively silences the most safety-relevant experts until harmful outputs are produced. We evaluate L^3 on eight state-of-the-art open-source MoE LLMs and show that our adaptive expert silencing increases average attack success from 7.3% to 70.4%, reaching up to 86.3%, outperforming prior training-free MoE jailbreak methods. Moreover, bypassing guardrails typically requires silencing fewer than 20% of layer-wise experts while largely preserving general language utility. These results reveal a fundamental tension between efficiency-driven MoE design and robust safety alignment and motivate distributing safety mechanisms more robustly in future MoE LLMs with architecture- and routing-aware methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08741.png","numComments":3,"submittedBy":{"_id":"6786535551cd69b65efd2bee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png","fullname":"Lichao Wu","name":"woorkhaarder","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"633d6fcf748f94ec2daa31c3","name":"RadboudUniversity","fullname":"Radboud University Nijmegen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/A0fLxEuJaWuCA0fFfp9u6.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.10870","authors":[{"_id":"698d93af65c0d15a6d162312","user":{"_id":"65d836973381a5be278a475a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65d836973381a5be278a475a/dgOfY5qZbuUY6HK6s1WsK.jpeg","isPro":false,"fullname":"Xuefeng Xu","user":"xuefeng-xu","type":"user"},"name":"Xuefeng Xu","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:25:53.187Z","hidden":false},{"_id":"698d93af65c0d15a6d162313","name":"Graham Cormode","hidden":false}],"publishedAt":"2026-02-11T13:58:55.000Z","submittedOnDailyAt":"2026-02-12T06:19:42.122Z","title":"FedPS: Federated data Preprocessing via aggregated Statistics","submittedOnDailyBy":{"_id":"65d836973381a5be278a475a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65d836973381a5be278a475a/dgOfY5qZbuUY6HK6s1WsK.jpeg","isPro":false,"fullname":"Xuefeng Xu","user":"xuefeng-xu","type":"user"},"summary":"Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.","upvotes":1,"discussionId":"698d93af65c0d15a6d162314","projectPage":"https://xuefeng-xu.github.io/fedps.html","githubRepo":"https://github.com/xuefeng-xu/fedps","githubRepoAddedBy":"user","ai_summary":"FedPS is a federated data preprocessing framework that uses aggregated statistics and data-sketching techniques to enable efficient and privacy-preserving data preparation for collaborative machine learning across distributed datasets.","ai_keywords":["federated learning","data preprocessing","data-sketching","aggregated statistics","feature scaling","encoding","discretization","missing-value imputation","k-Means","k-Nearest Neighbors","Bayesian Linear Regression","horizontal FL","vertical FL"],"githubStars":5},"publishedAt":"2026-02-11T08:58:55.000Z","title":"FedPS: Federated data Preprocessing via aggregated Statistics","summary":"Federated Learning (FL) enables multiple parties to collaboratively train machine learning models without sharing raw data. However, before training, data must be preprocessed to address missing values, inconsistent formats, and heterogeneous feature scales. This preprocessing stage is critical for model performance but is largely overlooked in FL research. In practical FL systems, privacy constraints prohibit centralizing raw data, while communication efficiency introduces further challenges for distributed preprocessing. We introduce FedPS, a unified framework for federated data preprocessing based on aggregated statistics. FedPS leverages data-sketching techniques to efficiently summarize local datasets while preserving essential statistical information. Building on these summaries, we design federated algorithms for feature scaling, encoding, discretization, and missing-value imputation, and extend preprocessing-related models such as k-Means, k-Nearest Neighbors, and Bayesian Linear Regression to both horizontal and vertical FL settings. FedPS provides flexible, communication-efficient, and consistent preprocessing pipelines for practical FL deployments.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10870.png","numComments":3,"submittedBy":{"_id":"65d836973381a5be278a475a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65d836973381a5be278a475a/dgOfY5qZbuUY6HK6s1WsK.jpeg","fullname":"Xuefeng Xu","name":"xuefeng-xu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.10699","authors":[{"_id":"698d4e0565c0d15a6d1621b5","name":"Jie Jiang","hidden":false},{"_id":"698d4e0565c0d15a6d1621b6","name":"Yangru Huang","hidden":false},{"_id":"698d4e0565c0d15a6d1621b7","name":"Zeyu Wang","hidden":false},{"_id":"698d4e0565c0d15a6d1621b8","name":"Changping Wang","hidden":false},{"_id":"698d4e0565c0d15a6d1621b9","name":"Yuling Xiong","hidden":false},{"_id":"698d4e0565c0d15a6d1621ba","name":"Jun Zhang","hidden":false},{"_id":"698d4e0565c0d15a6d1621bb","name":"Huan Yu","hidden":false}],"publishedAt":"2026-02-11T09:57:36.000Z","submittedOnDailyAt":"2026-02-12T01:20:37.839Z","title":"Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.","upvotes":1,"discussionId":"698d4e0565c0d15a6d1621bc","ai_summary":"V-STAR addresses limitations in generative recommendation by combining value-guided decoding and tree-structured advantage reinforcement to improve exploration and reward signal quality.","ai_keywords":["autoregressive models","Reinforcement Learning","probability-reward mismatch","beam search","myopic bias","exploration","advantage compression","Value-guided Efficient Decoding","Sibling-GRPO","tree-structured advantage"]},"publishedAt":"2026-02-11T04:57:36.000Z","title":"Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation","summary":"Generative recommendation via autoregressive models has unified retrieval and ranking into a single conditional generation framework. However, fine-tuning these models with Reinforcement Learning (RL) often suffers from a fundamental probability-reward mismatch. Conventional likelihood-dominated decoding (e.g., beam search) exhibits a myopic bias toward locally probable prefixes, which causes two critical failures: (1) insufficient exploration, where high-reward items in low-probability branches are prematurely pruned and rarely sampled, and (2) advantage compression, where trajectories sharing high-probability prefixes receive highly correlated rewards with low within-group variance, yielding a weak comparative signal for RL. To address these challenges, we propose V-STAR, a Value-guided Sampling and Tree-structured Advantage Reinforcement framework. V-STAR forms a self-evolving loop via two synergistic components. First, a Value-Guided Efficient Decoding (VED) is developed to identify decisive nodes and selectively deepen high-potential prefixes. This improves exploration efficiency without exhaustive tree search. Second, we propose Sibling-GRPO, which exploits the induced tree topology to compute sibling-relative advantages and concentrates learning signals on decisive branching decisions. Extensive experiments on both offline and online datasets demonstrate that V-STAR outperforms state-of-the-art baselines, delivering superior accuracy and candidate-set diversity under strict latency constraints.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10699.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.08052","authors":[{"_id":"698dea07cace060ff123ab00","user":{"_id":"626273fbcbebf7e1ac2820ab","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/626273fbcbebf7e1ac2820ab/VPc5BVvdSel6ox7yzrcvb.jpeg","isPro":false,"fullname":"Bulent Soykan","user":"bulentsoykan","type":"user"},"name":"Bulent Soykan","status":"claimed_verified","statusLastChangedAt":"2026-02-13T09:37:21.043Z","hidden":false},{"_id":"698dea07cace060ff123ab01","name":"Sean Mondesire","hidden":false},{"_id":"698dea07cace060ff123ab02","name":"Ghaith Rabadi","hidden":false},{"_id":"698dea07cace060ff123ab03","name":"Grace Bochenek","hidden":false}],"publishedAt":"2026-02-08T16:54:47.000Z","submittedOnDailyAt":"2026-02-12T12:27:06.282Z","title":"Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling","submittedOnDailyBy":{"_id":"626273fbcbebf7e1ac2820ab","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/626273fbcbebf7e1ac2820ab/VPc5BVvdSel6ox7yzrcvb.jpeg","isPro":false,"fullname":"Bulent Soykan","user":"bulentsoykan","type":"user"},"summary":"The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.","upvotes":1,"discussionId":"698dea08cace060ff123ab04","projectPage":"https://bulentsoykan.github.io/GNN-DRL4UPMSP/","githubRepo":"https://github.com/bulentsoykan/GNN-DRL4UPMSP","githubRepoAddedBy":"user","ai_summary":"A Deep Reinforcement Learning framework combining Proximal Policy Optimization and Graph Neural Networks addresses multi-objective scheduling challenges by effectively balancing total weighted tardiness and total setup time.","ai_keywords":["Proximal Policy Optimization","Graph Neural Network","multi-objective optimization","scheduling problem","reinforcement learning"],"githubStars":0,"organization":{"_id":"64f8a9c00590f3db14a59b8f","name":"UCF-Ai-LLMs","fullname":"University of Central Florida"}},"publishedAt":"2026-02-08T11:54:47.000Z","title":"Graph-Enhanced Deep Reinforcement Learning for Multi-Objective Unrelated Parallel Machine Scheduling","summary":"The Unrelated Parallel Machine Scheduling Problem (UPMSP) with release dates, setups, and eligibility constraints presents a significant multi-objective challenge. Traditional methods struggle to balance minimizing Total Weighted Tardiness (TWT) and Total Setup Time (TST). This paper proposes a Deep Reinforcement Learning framework using Proximal Policy Optimization (PPO) and a Graph Neural Network (GNN). The GNN effectively represents the complex state of jobs, machines, and setups, allowing the PPO agent to learn a direct scheduling policy. Guided by a multi-objective reward function, the agent simultaneously minimizes TWT and TST. Experimental results on benchmark instances demonstrate that our PPO-GNN agent significantly outperforms a standard dispatching rule and a metaheuristic, achieving a superior trade-off between both objectives. This provides a robust and scalable solution for complex manufacturing scheduling.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08052.png","numComments":2,"submittedBy":{"_id":"626273fbcbebf7e1ac2820ab","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/626273fbcbebf7e1ac2820ab/VPc5BVvdSel6ox7yzrcvb.jpeg","fullname":"Bulent Soykan","name":"bulentsoykan","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"64f8a9c00590f3db14a59b8f","name":"UCF-Ai-LLMs","fullname":"University of Central Florida"},"isAuthorParticipating":true},{"paper":{"id":"2602.08934","authors":[{"_id":"698b9bff6052d3bed96308d8","user":{"_id":"68fa7c73f382c0374680ad98","avatarUrl":"/avatars/3753ca3353e296caf15dae203c69d876.svg","isPro":false,"fullname":"Suraj Ranganath","user":"suraj-ranganath","type":"user"},"name":"Suraj Ranganath","status":"claimed_verified","statusLastChangedAt":"2026-02-11T11:15:34.811Z","hidden":false},{"_id":"698b9bff6052d3bed96308d9","name":"Atharv Ramesh","hidden":false}],"publishedAt":"2026-02-09T17:33:46.000Z","submittedOnDailyAt":"2026-02-12T16:30:58.929Z","title":"StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors","submittedOnDailyBy":{"_id":"68fa7c73f382c0374680ad98","avatarUrl":"/avatars/3753ca3353e296caf15dae203c69d876.svg","isPro":false,"fullname":"Suraj Ranganath","user":"suraj-ranganath","type":"user"},"summary":"AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.","upvotes":0,"discussionId":"698b9bff6052d3bed96308da","githubRepo":"https://github.com/suraj-ranganath/StealthRL","githubRepoAddedBy":"user","ai_summary":"StealthRL uses reinforcement learning with LoRA adapters to create adversarial paraphrases that evade multiple AI text detectors while preserving meaning, demonstrating significant robustness gaps in current detection systems.","ai_keywords":["reinforcement learning","paraphrase policy","multi-detector ensemble","Group Relative Policy Optimization","LoRA adapters","Qwen3-4B","composite reward","detector evasion","semantic preservation","attack success rate","adversarial paraphrasing","false positive rate","AUROC","attack transfer","Likert scoring","score distributions","bootstrap confidence intervals"],"githubStars":4,"organization":{"_id":"697e87d12cc19315a8497001","name":"UCSanDiego","fullname":"University of California at San Diego","avatar":"https://cdn-uploads.huggingface.co/production/uploads/697e8687c00f332cf492d29e/KUQpvngxP4r9oBSDZwIwZ.png"}},"publishedAt":"2026-02-09T12:33:46.000Z","title":"StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors","summary":"AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08934.png","numComments":2,"submittedBy":{"_id":"68fa7c73f382c0374680ad98","avatarUrl":"/avatars/3753ca3353e296caf15dae203c69d876.svg","fullname":"Suraj Ranganath","name":"suraj-ranganath","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"697e87d12cc19315a8497001","name":"UCSanDiego","fullname":"University of California at San Diego","avatar":"https://cdn-uploads.huggingface.co/production/uploads/697e8687c00f332cf492d29e/KUQpvngxP4r9oBSDZwIwZ.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.06841","authors":[{"_id":"698d0b7a6c5152984e4f3e37","user":{"_id":"64c85111f3d2a59a43133747","avatarUrl":"/avatars/1fe304fc78553c4ac1a5704b7010aa0e.svg","isPro":false,"fullname":"Sindhuja Chaduvula","user":"Sindhujach217","type":"user"},"name":"Sindhuja Chaduvula","status":"claimed_verified","statusLastChangedAt":"2026-02-12T13:57:14.905Z","hidden":false},{"_id":"698d0b7a6c5152984e4f3e38","name":"Jessee Ho","hidden":false},{"_id":"698d0b7a6c5152984e4f3e39","name":"Kina Kim","hidden":false},{"_id":"698d0b7a6c5152984e4f3e3a","name":"Aravind Narayanan","hidden":false},{"_id":"698d0b7a6c5152984e4f3e3b","name":"Mahshid Alinoori","hidden":false},{"_id":"698d0b7a6c5152984e4f3e3c","name":"Muskan Garg","hidden":false},{"_id":"698d0b7a6c5152984e4f3e3d","name":"Dhanesh Ramachandram","hidden":false},{"_id":"698d0b7a6c5152984e4f3e3e","name":"Shaina Raza","hidden":false}],"publishedAt":"2026-02-06T16:34:29.000Z","submittedOnDailyAt":"2026-02-12T22:25:07.800Z","title":"From Features to Actions: Explainability in Traditional and Agentic AI Systems","submittedOnDailyBy":{"_id":"64c85111f3d2a59a43133747","avatarUrl":"/avatars/1fe304fc78553c4ac1a5704b7010aa0e.svg","isPro":false,"fullname":"Sindhuja Chaduvula","user":"Sindhujach217","type":"user"},"summary":"Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman = 0.86), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7times more prevalent in failed runs and reduces success probability by 49\\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.\n  Resources:\n  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework","upvotes":0,"discussionId":"698d0b7a6c5152984e4f3e3f","projectPage":"https://vectorinstitute.github.io/unified-xai-evaluation-framework/","githubRepo":"https://github.com/VectorInstitute/unified-xai-evaluation-framework","githubRepoAddedBy":"user","ai_summary":"Static and agentic explainability approaches differ in their ability to interpret model behavior, with attribution methods effective for individual predictions but inadequate for diagnosing failures in multi-step decision processes, where trace-based diagnostics prove more reliable.","ai_keywords":["explainable AI","large language models","agentic AI","post-hoc explanations","attribution-based explanations","trace-based diagnostics","static predictions","agentic trajectories","TAU-bench","AssistantBench","trajectory-level explainability"],"githubStars":1,"organization":{"_id":"65b2b60eade89bc3fac3108a","name":"vector-institute","fullname":"Vector Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"}},"publishedAt":"2026-02-06T11:34:29.000Z","title":"From Features to Actions: Explainability in Traditional and Agentic AI Systems","summary":"Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequences of decisions rather than a single output. While useful, it remains unclear how explanation approaches designed for static predictions translate to agentic settings where behaviour emerges over time. In this work, we bridge the gap between static and agentic explainability by comparing attribution-based explanations with trace-based diagnostics across both settings. To make this distinction explicit, we empirically compare attribution-based explanations used in static classification tasks with trace-based diagnostics used in agentic benchmarks (TAU-bench Airline and AssistantBench). Our results show that while attribution methods achieve stable feature rankings in static settings (Spearman = 0.86), they cannot be applied reliably to diagnose execution-level failures in agentic trajectories. In contrast, trace-grounded rubric evaluation for agentic settings consistently localizes behaviour breakdowns and reveals that state tracking inconsistency is 2.7times more prevalent in failed runs and reduces success probability by 49\\%. These findings motivate a shift towards trajectory-level explainability for agentic systems when evaluating and diagnosing autonomous AI behaviour.\n  Resources:\n  https://github.com/VectorInstitute/unified-xai-evaluation-framework https://vectorinstitute.github.io/unified-xai-evaluation-framework","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06841.png","numComments":2,"submittedBy":{"_id":"64c85111f3d2a59a43133747","avatarUrl":"/avatars/1fe304fc78553c4ac1a5704b7010aa0e.svg","fullname":"Sindhuja Chaduvula","name":"Sindhujach217","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"65b2b60eade89bc3fac3108a","name":"vector-institute","fullname":"Vector Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62f2d5d052ad88c930b7ba5e/_GqWuc2CjKyGsNtHDYKF7.png"},"isAuthorParticipating":true}]