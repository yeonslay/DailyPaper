[{"paper":{"id":"2602.13515","authors":[{"_id":"6997d8487a658569d5a101d2","user":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","isPro":false,"fullname":"Jintao Zhang","user":"jt-zhang","type":"user"},"name":"Jintao Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:32:47.646Z","hidden":false},{"_id":"6997d8487a658569d5a101d3","name":"Kai Jiang","hidden":false},{"_id":"6997d8487a658569d5a101d4","user":{"_id":"6329bdbbde087eac2921e6a9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1663679904323-noauth.jpeg","isPro":false,"fullname":"Xiangchendong","user":"Xiang-cd","type":"user"},"name":"Chendong Xiang","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:02:35.487Z","hidden":false},{"_id":"6997d8487a658569d5a101d5","user":{"_id":"6698a1efbf3edea3110a6e66","avatarUrl":"/avatars/8329c0f938af5765ae7c46aa8ef4df5b.svg","isPro":false,"fullname":"Weiqi Feng","user":"Fengwq22","type":"user"},"name":"Weiqi Feng","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:02:42.013Z","hidden":false},{"_id":"6997d8487a658569d5a101d6","user":{"_id":"67dc66fe55c24fc4f981a4ab","avatarUrl":"/avatars/7bd900ade802d99db7c562ad6c2f6661.svg","isPro":false,"fullname":"Yuezhou Hu","user":"yuezhouhu","type":"user"},"name":"Yuezhou Hu","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:02:47.984Z","hidden":false},{"_id":"6997d8487a658569d5a101d7","name":"Haocheng Xi","hidden":false},{"_id":"6997d8487a658569d5a101d8","name":"Jianfei Chen","hidden":false},{"_id":"6997d8487a658569d5a101d9","name":"Jun Zhu","hidden":false}],"publishedAt":"2026-02-13T23:01:42.000Z","submittedOnDailyAt":"2026-02-20T01:14:15.484Z","title":"SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning","submittedOnDailyBy":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","isPro":false,"fullname":"Jintao Zhang","user":"jt-zhang","type":"user"},"summary":"Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.","upvotes":36,"discussionId":"6997d8487a658569d5a101da","projectPage":"https://github.com/thu-ml/SpargeAttn","ai_summary":"A trainable sparse attention method called SpargeAttention2 is proposed that achieves high sparsity in diffusion models while maintaining generation quality through hybrid masking rules and distillation-inspired fine-tuning.","ai_keywords":["sparse attention","diffusion models","Top-k","Top-p","trainable sparse attention","hybrid masking rule","distillation-inspired fine-tuning","attention sparsity","attention speedup"],"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-02-13T18:01:42.000Z","title":"SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning","summary":"Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13515.png","numComments":5,"submittedBy":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","fullname":"Jintao Zhang","name":"jt-zhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":48,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":true},{"paper":{"id":"2602.16855","authors":[{"_id":"6997d6467a658569d5a101b7","name":"Haiyang Xu","hidden":false},{"_id":"6997d6467a658569d5a101b8","name":"Xi Zhang","hidden":false},{"_id":"6997d6467a658569d5a101b9","name":"Haowei Liu","hidden":false},{"_id":"6997d6467a658569d5a101ba","name":"Junyang Wang","hidden":false},{"_id":"6997d6467a658569d5a101bb","name":"Zhaozai Zhu","hidden":false},{"_id":"6997d6467a658569d5a101bc","user":{"_id":"6847d15573f604b8d2b9f738","avatarUrl":"/avatars/4e25ca44685afd597b7d4f5f7cb2aae4.svg","isPro":false,"fullname":"Shengjie Zhou","user":"ZSJ123","type":"user"},"name":"Shengjie Zhou","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:01:53.423Z","hidden":false},{"_id":"6997d6467a658569d5a101bd","user":{"_id":"6372813520a58a5e14c596a3","avatarUrl":"/avatars/9135151259db3e5b9c8969e1d00c949d.svg","isPro":false,"fullname":"XuHao Hu","user":"Foreshhh","type":"user"},"name":"Xuhao Hu","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:36:56.334Z","hidden":false},{"_id":"6997d6467a658569d5a101be","name":"Feiyu Gao","hidden":false},{"_id":"6997d6467a658569d5a101bf","user":{"_id":"659a39b95f7a6d40f75404ec","avatarUrl":"/avatars/791a6e8f31f0d0eead06374898a0ded7.svg","isPro":false,"fullname":"Junjie Cao","user":"flyingtom","type":"user"},"name":"Junjie Cao","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:01:46.813Z","hidden":false},{"_id":"6997d6467a658569d5a101c0","name":"Zihua Wang","hidden":false},{"_id":"6997d6467a658569d5a101c1","name":"Zhiyuan Chen","hidden":false},{"_id":"6997d6467a658569d5a101c2","name":"Jitong Liao","hidden":false},{"_id":"6997d6467a658569d5a101c3","name":"Qi Zheng","hidden":false},{"_id":"6997d6467a658569d5a101c4","name":"Jiahui Zeng","hidden":false},{"_id":"6997d6467a658569d5a101c5","name":"Ze Xu","hidden":false},{"_id":"6997d6467a658569d5a101c6","name":"Shuai Bai","hidden":false},{"_id":"6997d6467a658569d5a101c7","user":{"_id":"620760a26e3b7210c2ff1943","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg","isPro":false,"fullname":"Junyang Lin","user":"JustinLin610","type":"user"},"name":"Junyang Lin","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:01:28.321Z","hidden":false},{"_id":"6997d6467a658569d5a101c8","name":"Jingren Zhou","hidden":false},{"_id":"6997d6467a658569d5a101c9","name":"Ming Yan","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/EQN85w7A2VvZ34VP_BUF2.jpeg","https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/tuSteuSqp79bypJBdU-eR.jpeg"],"publishedAt":"2026-02-15T01:52:19.000Z","submittedOnDailyAt":"2026-02-20T01:10:42.877Z","title":"Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents","submittedOnDailyBy":{"_id":"645b10e80c73ea27d13f7aca","avatarUrl":"/avatars/95e565306472a15067440b5b43e07a6f.svg","isPro":false,"fullname":"xuhaiyang","user":"xhyandwyy","type":"user"},"summary":"The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.","upvotes":32,"discussionId":"6997d6467a658569d5a101ca","projectPage":"https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v3.5","ai_summary":"GUI-Owl-1.5 is a multi-platform GUI agent model with multiple sizes that achieves state-of-the-art performance on GUI automation, grounding, tool-calling, and memory tasks through innovations in hybrid data pipelines, unified reasoning enhancement, and multi-platform reinforcement learning.","ai_keywords":["GUI agent model","UI understanding","trajectory generation","simulated environments","cloud-based sandbox environments","thought-synthesis pipeline","tool-calling","multi-agent adaptation","environment RL algorithm","MRPO"],"organization":{"_id":"67d15cca6e2cf0e062dbfb54","name":"AlibabaTongyiLab","fullname":"TongyiLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}},"publishedAt":"2026-02-14T20:52:19.000Z","title":"Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents","summary":"The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/EQN85w7A2VvZ34VP_BUF2.jpeg","https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/tuSteuSqp79bypJBdU-eR.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16855.png","numComments":3,"submittedBy":{"_id":"645b10e80c73ea27d13f7aca","avatarUrl":"/avatars/95e565306472a15067440b5b43e07a6f.svg","fullname":"xuhaiyang","name":"xhyandwyy","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"organization":{"_id":"67d15cca6e2cf0e062dbfb54","name":"AlibabaTongyiLab","fullname":"TongyiLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.17270","authors":[{"_id":"6997ce527a658569d5a10165","name":"Jonathan Heek","hidden":false},{"_id":"6997ce527a658569d5a10166","user":{"_id":"671feaa3e269c38584cb065f","avatarUrl":"/avatars/d7bc35ab98b208ef1d009aa9b629682a.svg","isPro":false,"fullname":"Emiel Hoogeboom","user":"Emiel2","type":"user"},"name":"Emiel Hoogeboom","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:03:14.323Z","hidden":false},{"_id":"6997ce527a658569d5a10167","user":{"_id":"642ad5bc508b7246f9d27ab9","avatarUrl":"/avatars/6cdf6f855536896575733e273388d7fb.svg","isPro":false,"fullname":"Thomas Mensink","user":"tmensink","type":"user"},"name":"Thomas Mensink","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:03:21.125Z","hidden":false},{"_id":"6997ce527a658569d5a10168","name":"Tim Salimans","hidden":false}],"publishedAt":"2026-02-19T11:18:12.000Z","submittedOnDailyAt":"2026-02-20T00:30:40.692Z","title":"Unified Latents (UL): How to train your latents","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.","upvotes":27,"discussionId":"6997ce527a658569d5a10169","ai_summary":"Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.","ai_keywords":["diffusion prior","diffusion model","latent representations","training objective","latent bitrate","FID","PSNR","FLOPs","FVD"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-19T06:18:12.000Z","title":"Unified Latents (UL): How to train your latents","summary":"We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17270.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14457","authors":[{"_id":"699720497a658569d5a10060","user":{"_id":"6621f4eb64e84619e578aad6","avatarUrl":"/avatars/b1ad96ee354b999fcafb2998a636609c.svg","isPro":false,"fullname":"Dongrui Liu","user":"shenqiorient","type":"user"},"name":"Dongrui Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:37:12.402Z","hidden":false},{"_id":"699720497a658569d5a10061","name":"Yi Yu","hidden":false},{"_id":"699720497a658569d5a10062","name":"Jie Zhang","hidden":false},{"_id":"699720497a658569d5a10063","name":"Guanxu Chen","hidden":false},{"_id":"699720497a658569d5a10064","name":"Qihao Lin","hidden":false},{"_id":"699720497a658569d5a10065","name":"Hanxi Zhu","hidden":false},{"_id":"699720497a658569d5a10066","name":"Lige Huang","hidden":false},{"_id":"699720497a658569d5a10067","name":"Yijin Zhou","hidden":false},{"_id":"699720497a658569d5a10068","name":"Peng Wang","hidden":false},{"_id":"699720497a658569d5a10069","name":"Shuai Shao","hidden":false},{"_id":"699720497a658569d5a1006a","name":"Boxuan Zhang","hidden":false},{"_id":"699720497a658569d5a1006b","name":"Zicheng Liu","hidden":false},{"_id":"699720497a658569d5a1006c","name":"Jingwei Sun","hidden":false},{"_id":"699720497a658569d5a1006d","name":"Yu Li","hidden":false},{"_id":"699720497a658569d5a1006e","name":"Yuejin Xie","hidden":false},{"_id":"699720497a658569d5a1006f","name":"Jiaxuan Guo","hidden":false},{"_id":"699720497a658569d5a10070","name":"Jia Xu","hidden":false},{"_id":"699720497a658569d5a10071","user":{"_id":"6903043fdf78e6ca12c680f0","avatarUrl":"/avatars/4f79754d4eae0f5c3af9a1e10bbe1416.svg","isPro":false,"fullname":"Chaochao Lu","user":"chaochaolu","type":"user"},"name":"Chaochao Lu","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:03:56.687Z","hidden":false},{"_id":"699720497a658569d5a10072","user":{"_id":"669f614b59adf5b56e05bce3","avatarUrl":"/avatars/ffd4189efbceb0e63a03db273065a44b.svg","isPro":false,"fullname":"BowenZhou","user":"bowenZhou","type":"user"},"name":"Bowen Zhou","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:04:03.449Z","hidden":false},{"_id":"699720497a658569d5a10073","name":"Xia Hu","hidden":false},{"_id":"699720497a658569d5a10074","name":"Jing Shao","hidden":false}],"publishedAt":"2026-02-16T04:30:06.000Z","submittedOnDailyAt":"2026-02-20T01:03:37.208Z","title":"Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5","submittedOnDailyBy":{"_id":"66e2624a436a1798365e4581","avatarUrl":"/avatars/6c605807d34faa8fb505e135a4b47776.svg","isPro":false,"fullname":"Qihan Ren","user":"jasonrqh","type":"user"},"summary":"To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.","upvotes":25,"discussionId":"699720497a658569d5a10075","projectPage":"https://ai45lab.github.io/safeworkf1-page/","ai_summary":"Frontier AI risk analysis assesses critical dimensions including cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R&D, and self-replication, proposing mitigation strategies for secure deployment of advanced AI systems.","ai_keywords":["Large Language Models","agentic AI","cyber offense","persuasion and manipulation","strategic deception","uncontrolled AI R&D","self-replication","emergent misalignment","mis-evolution","OpenClaw","Moltbook"],"organization":{"_id":"68f716f832b31e42cbc2be7f","name":"AI45Research","fullname":"AI45Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}},"publishedAt":"2026-02-15T23:30:06.000Z","title":"Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report v1.5","summary":"To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, Frontier AI Risk Management Framework in Practice presents a comprehensive assessment of their frontier risks. As Large Language Models (LLMs) general capabilities rapidly evolve and the proliferation of agentic AI, this version of the risk analysis technical report presents an updated and granular assessment of five critical dimensions: cyber offense, persuasion and manipulation, strategic deception, uncontrolled AI R\\&D, and self-replication. Specifically, we introduce more complex scenarios for cyber offense. For persuasion and manipulation, we evaluate the risk of LLM-to-LLM persuasion on newly released LLMs. For strategic deception and scheming, we add the new experiment with respect to emergent misalignment. For uncontrolled AI R\\&D, we focus on the ``mis-evolution'' of agents as they autonomously expand their memory substrates and toolsets. Besides, we also monitor and evaluate the safety performance of OpenClaw during the interaction on the Moltbook. For self-replication, we introduce a new resource-constrained scenario. More importantly, we propose and validate a series of robust mitigation strategies to address these emerging threats, providing a preliminary technical and actionable pathway for the secure deployment of frontier AI. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14457.png","numComments":4,"submittedBy":{"_id":"66e2624a436a1798365e4581","avatarUrl":"/avatars/6c605807d34faa8fb505e135a4b47776.svg","fullname":"Qihan Ren","name":"jasonrqh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"68f716f832b31e42cbc2be7f","name":"AI45Research","fullname":"AI45Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.17004","authors":[{"_id":"6997cd777a658569d5a10149","name":"Varun Singh","hidden":false},{"_id":"6997cd777a658569d5a1014a","name":"Lucas Krauss","hidden":false},{"_id":"6997cd777a658569d5a1014b","name":"Sami Jaghouar","hidden":false},{"_id":"6997cd777a658569d5a1014c","user":{"_id":"658f19cfa02954c982f540eb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/658f19cfa02954c982f540eb/qN2eqJZnWQ2H1xlGYeAKj.jpeg","isPro":false,"fullname":"Matej Sirovatka","user":"siro1","type":"user"},"name":"Matej Sirovatka","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:07:12.184Z","hidden":false},{"_id":"6997cd777a658569d5a1014d","user":{"_id":"630495b0ce6b12280b193c25","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png","isPro":false,"fullname":"Charles Goddard","user":"chargoddard","type":"user"},"name":"Charles Goddard","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:07:18.720Z","hidden":false},{"_id":"6997cd777a658569d5a1014e","name":"Fares Obied","hidden":false},{"_id":"6997cd777a658569d5a1014f","user":{"_id":"634e2b60a00c472888747e4c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/634e2b60a00c472888747e4c/FGqQ1qcfN0pXZ9GgdRrBB.png","isPro":false,"fullname":"Jack Min Ong","user":"Jackmin108","type":"user"},"name":"Jack Min Ong","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:07:29.527Z","hidden":false},{"_id":"6997cd777a658569d5a10150","name":"Jannik Straube","hidden":false},{"_id":"6997cd777a658569d5a10151","name":"Fern","hidden":false},{"_id":"6997cd777a658569d5a10152","name":"Aria Harley","hidden":false},{"_id":"6997cd777a658569d5a10153","user":{"_id":"67f8381b383e0a7c03504e01","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/De8SEV0r_koGe4Ke7KBgD.png","isPro":false,"fullname":"Conner Stewart","user":"Chemicon0302","type":"user"},"name":"Conner Stewart","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:07:40.441Z","hidden":false},{"_id":"6997cd777a658569d5a10154","name":"Colin Kealty","hidden":false},{"_id":"6997cd777a658569d5a10155","user":{"_id":"5fd5e18a90b6dc4633f6d292","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png","isPro":true,"fullname":"Maziyar Panahi","user":"MaziyarPanahi","type":"user"},"name":"Maziyar Panahi","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:07:50.837Z","hidden":false},{"_id":"6997cd777a658569d5a10156","name":"Simon Kirsten","hidden":false},{"_id":"6997cd777a658569d5a10157","name":"Anushka Deshpande","hidden":false},{"_id":"6997cd777a658569d5a10158","user":{"_id":"65f1560b8a7ccf08ff84c044","avatarUrl":"/avatars/3563eeb42eea865ef20d8c50a6727970.svg","isPro":false,"fullname":"Anneketh Vij","user":"annekethvij","type":"user"},"name":"Anneketh Vij","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:08:17.060Z","hidden":true},{"_id":"6997cd777a658569d5a10159","user":{"_id":"663ce8bfc6827e9ba288eed4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/663ce8bfc6827e9ba288eed4/0xrGyvLOq8DtP4TQsesnS.jpeg","isPro":false,"fullname":"Arthur BRESNU","user":"arthurbresnu","type":"user"},"name":"Arthur Bresnu","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:08:23.592Z","hidden":false},{"_id":"6997cd777a658569d5a1015a","user":{"_id":"64b6c10161dc301f13235b47","avatarUrl":"/avatars/9f4f434e855077a33698560eaff0c58e.svg","isPro":true,"fullname":"Pranav Veldurthi","user":"pranavkr","type":"user"},"name":"Pranav Veldurthi","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:08:30.013Z","hidden":false},{"_id":"6997cd777a658569d5a1015b","user":{"_id":"6434d35aa5aed21dd11a294d","avatarUrl":"/avatars/4af84f8850c0386e706b5537b5c48b26.svg","isPro":false,"fullname":"Raghav Ravishankar","user":"Alyosha11","type":"user"},"name":"Raghav Ravishankar","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:08:36.828Z","hidden":false},{"_id":"6997cd777a658569d5a1015c","name":"Hardik Bishnoi","hidden":false},{"_id":"6997cd777a658569d5a1015d","name":"DatologyAI Team","hidden":false},{"_id":"6997cd777a658569d5a1015e","name":"Arcee AI Team","hidden":false},{"_id":"6997cd777a658569d5a1015f","name":"Prime Intellect Team","hidden":false},{"_id":"6997cd777a658569d5a10160","name":"Mark McQuade","hidden":false},{"_id":"6997cd777a658569d5a10161","name":"Johannes Hagemann","hidden":false},{"_id":"6997cd777a658569d5a10162","name":"Lucas Atkins","hidden":false}],"publishedAt":"2026-02-19T01:58:50.000Z","submittedOnDailyAt":"2026-02-20T00:26:56.558Z","title":"Arcee Trinity Large Technical Report","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.","upvotes":14,"discussionId":"6997cd777a658569d5a10163","ai_summary":"Arcee Trinity models are sparse Mixture-of-Experts architectures with varying parameter counts and activation patterns, utilizing advanced attention mechanisms and training optimizations.","ai_keywords":["Mixture-of-Experts","sparse Mixture-of-Experts","attention","gated attention","depth-scaled sandwich norm","sigmoid routing","Muon optimizer","MoE load balancing","Soft-clamped Momentum Expert Bias Updates"],"organization":{"_id":"650b09466554462d26214645","name":"arcee-ai","fullname":"Arcee AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/GZPnGkfMn8Ino6JbkL4fJ.png"}},"publishedAt":"2026-02-18T20:58:50.000Z","title":"Arcee Trinity Large Technical Report","summary":"We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17004.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"650b09466554462d26214645","name":"arcee-ai","fullname":"Arcee AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/GZPnGkfMn8Ino6JbkL4fJ.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.16699","authors":[{"_id":"69988ff0f723198bd53a5fe8","name":"Wenxuan Ding","hidden":false},{"_id":"69988ff0f723198bd53a5fe9","name":"Nicholas Tomlin","hidden":false},{"_id":"69988ff0f723198bd53a5fea","name":"Greg Durrett","hidden":false}],"publishedAt":"2026-02-18T18:46:14.000Z","submittedOnDailyAt":"2026-02-20T14:23:42.723Z","title":"Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents","submittedOnDailyBy":{"_id":"6502b65106c324fee1089ad5","avatarUrl":"/avatars/9e054900bc1dd9a178a427ca9cf939da.svg","isPro":false,"fullname":"Wenxuan D","user":"wenwenD","type":"user"},"summary":"LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.","upvotes":12,"discussionId":"69988ff0f723198bd53a5feb","githubRepo":"https://github.com/Wenwen-D/env-explorer","githubRepoAddedBy":"user","ai_summary":"Large language models can be improved for complex tasks by explicitly reasoning about cost-uncertainty tradeoffs through a Calibrate-Then-Act framework that enhances decision-making in sequential environments.","ai_keywords":["large language models","sequential decision-making","cost-uncertainty tradeoffs","environment exploration","Calibrate-Then-Act framework","information retrieval","coding tasks","latent environment state","prior knowledge","reinforcement learning"],"githubStars":5},"publishedAt":"2026-02-18T13:46:14.000Z","title":"Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents","summary":"LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16699.png","numComments":2,"submittedBy":{"_id":"6502b65106c324fee1089ad5","avatarUrl":"/avatars/9e054900bc1dd9a178a427ca9cf939da.svg","fullname":"Wenxuan D","name":"wenwenD","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.15569","authors":[{"_id":"6995681c8d17d1ee8c10ed0b","user":{"_id":"69838556dea8956a5cef3ebd","avatarUrl":"/avatars/1a82518bc4e1a14af9f7874e0d720924.svg","isPro":false,"fullname":"Johannes Kirmayr","user":"johanneskirmayr","type":"user"},"name":"Johannes Kirmayr","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:08:12.567Z","hidden":false},{"_id":"6995681c8d17d1ee8c10ed0c","user":{"_id":"639b47af621e87aa0d424bf9","avatarUrl":"/avatars/f843d68cc8e98d6173d5efd78f5213ee.svg","isPro":false,"fullname":"Raphael Wennmacher","user":"rpgraffi","type":"user"},"name":"Raphael Wennmacher","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:05:34.653Z","hidden":false},{"_id":"6995681c8d17d1ee8c10ed0d","name":"Khanh Huynh","hidden":false},{"_id":"6995681c8d17d1ee8c10ed0e","user":{"_id":"6985bb43ac2abc7a370153d8","avatarUrl":"/avatars/02da4efbe2706fc0a312172c9bfa0cfb.svg","isPro":false,"fullname":"Lukas Stappen","user":"lstappen","type":"user"},"name":"Lukas Stappen","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:08:13.970Z","hidden":false},{"_id":"6995681c8d17d1ee8c10ed0f","user":{"_id":"69860a2d8e94310526eb5d8b","avatarUrl":"/avatars/058f83d16d68e9bba21d1a0f6e54b19a.svg","isPro":false,"fullname":"Elisabeth André","user":"ElisabethAndre","type":"user"},"name":"Elisabeth André","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:05:46.083Z","hidden":false},{"_id":"6995681c8d17d1ee8c10ed10","user":{"_id":"6911ecd9f0c2157f7974fbf1","avatarUrl":"/avatars/a5a37c29fa79fc19bee7c110cb18844e.svg","isPro":false,"fullname":"Florian Alt","user":"floffi","type":"user"},"name":"Florian Alt","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:05:52.291Z","hidden":false}],"publishedAt":"2026-02-17T13:27:50.000Z","submittedOnDailyAt":"2026-02-20T05:38:37.801Z","title":"\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing","submittedOnDailyBy":{"_id":"69838556dea8956a5cef3ebd","avatarUrl":"/avatars/1a82518bc4e1a14af9f7874e0d720924.svg","isPro":false,"fullname":"Johannes Kirmayr","user":"johanneskirmayr","type":"user"},"summary":"Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience: how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant, we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts. Interviews further revealed user preferences for an adaptive approach: high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency.","upvotes":12,"discussionId":"6995681d8d17d1ee8c10ed11","githubRepo":"https://github.com/johanneskirmayr/agentic_llm_feedback","githubRepoAddedBy":"user","ai_summary":"Users prefer adaptive feedback mechanisms in in-car AI assistants, starting with high transparency to build trust and then reducing verbosity as reliability increases, particularly in attention-critical driving scenarios.","ai_keywords":["agentic AI assistants","multi-step tasks","user experience","feedback timing","verbosity","dual-task paradigm","in-car voice assistant","task complexity","interaction contexts","adaptive approach","transparency","efficiency"],"githubStars":2,"organization":{"_id":"6985af73f62635603d8c1610","name":"BMW-LLM-Research-Group","fullname":"BMW LLM Research Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/69838556dea8956a5cef3ebd/DvQdciVg8CQ68gZsa-VGI.png"}},"publishedAt":"2026-02-17T08:27:50.000Z","title":"\"What Are You Doing?\": Effects of Intermediate Feedback from Agentic LLM In-Car Assistants During Multi-Step Processing","summary":"Agentic AI assistants that autonomously perform multi-step tasks raise open questions for user experience: how should such systems communicate progress and reasoning during extended operations, especially in attention-critical contexts such as driving? We investigate feedback timing and verbosity from agentic LLM-based in-car assistants through a controlled, mixed-methods study (N=45) comparing planned steps and intermediate results feedback against silent operation with final-only response. Using a dual-task paradigm with an in-car voice assistant, we found that intermediate feedback significantly improved perceived speed, trust, and user experience while reducing task load - effects that held across varying task complexities and interaction contexts. Interviews further revealed user preferences for an adaptive approach: high initial transparency to establish trust, followed by progressively reducing verbosity as systems prove reliable, with adjustments based on task stakes and situational context. We translate our empirical findings into design implications for feedback timing and verbosity in agentic assistants, balancing transparency and efficiency.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15569.png","numComments":3,"submittedBy":{"_id":"69838556dea8956a5cef3ebd","avatarUrl":"/avatars/1a82518bc4e1a14af9f7874e0d720924.svg","fullname":"Johannes Kirmayr","name":"johanneskirmayr","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6985af73f62635603d8c1610","name":"BMW-LLM-Research-Group","fullname":"BMW LLM Research Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/69838556dea8956a5cef3ebd/DvQdciVg8CQ68gZsa-VGI.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.16968","authors":[{"_id":"6997ceac7a658569d5a1016b","user":{"_id":"64bcc06fb567ae97c3272d3d","avatarUrl":"/avatars/bcb61fe9e575154d84913a1501971f1a.svg","isPro":false,"fullname":"kim","user":"dahyekim","type":"user"},"name":"Dahye Kim","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:36:23.660Z","hidden":false},{"_id":"6997ceac7a658569d5a1016c","name":"Deepti Ghadiyaram","hidden":false},{"_id":"6997ceac7a658569d5a1016d","user":{"_id":"6621eba3d93296af86f59d43","avatarUrl":"/avatars/1ee9bbaab46ba6eb237fe797d03a1dbd.svg","isPro":true,"fullname":"Raghudeep Gadde","user":"raghudeepg","type":"user"},"name":"Raghudeep Gadde","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:04:41.675Z","hidden":false}],"publishedAt":"2026-02-19T00:15:20.000Z","submittedOnDailyAt":"2026-02-20T00:32:15.811Z","title":"DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.","upvotes":10,"discussionId":"6997ceac7a658569d5a1016e","projectPage":"https://ddit-fast.github.io/ddit/","ai_summary":"Dynamic tokenization improves diffusion transformer efficiency by adjusting patch sizes based on content complexity and denoising timestep, achieving significant speedup without quality loss.","ai_keywords":["diffusion transformers","tokenization","denoising phase","patch sizes","content complexity","denoising timestep","inference","computational efficiency","perceptual quality","speedup"],"organization":{"_id":"5ffdfbadbba2ae614d771970","name":"amazon","fullname":"Amazon","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"}},"publishedAt":"2026-02-18T19:15:20.000Z","title":"DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers","summary":"Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to 3.52times and 3.2times speedup on FLUX-1.Dev and Wan 2.1, respectively, without compromising the generation quality and prompt adherence.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16968.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"5ffdfbadbba2ae614d771970","name":"amazon","fullname":"Amazon","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/8y7msN6A6W82LdQhQd85a.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.13579","authors":[{"_id":"6996a91e1268a6b79e0d033f","user":{"_id":"626c8c89a317717549e4e5aa","avatarUrl":"/avatars/b8bf54005b7b2a8fa0d10ed4557b675d.svg","isPro":false,"fullname":"youngsun wi","user":"youngw","type":"user"},"name":"Youngsun Wi","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:51:39.647Z","hidden":false},{"_id":"6996a91e1268a6b79e0d0340","user":{"_id":"670205302fa99176353ffa86","avatarUrl":"/avatars/1a9f6d16c9b69f452574f203cfbb8413.svg","isPro":false,"fullname":"Jessica Yin","user":"Jessicayin","type":"user"},"name":"Jessica Yin","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:08:58.693Z","hidden":false},{"_id":"6996a91e1268a6b79e0d0341","user":{"_id":"68d8104df85bf039b0fb80aa","avatarUrl":"/avatars/41360c5db804c9940a5a2b30b49327ef.svg","isPro":false,"fullname":"Elvis Xiang","user":"elvisxiang","type":"user"},"name":"Elvis Xiang","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:09:04.921Z","hidden":false},{"_id":"6996a91e1268a6b79e0d0342","name":"Akash Sharma","hidden":false},{"_id":"6996a91e1268a6b79e0d0343","user":{"_id":"65369a95605a07338de78ab0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg","isPro":false,"fullname":"Jitendra Malik ","user":"jitendra1995","type":"user"},"name":"Jitendra Malik","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:09:17.040Z","hidden":false},{"_id":"6996a91e1268a6b79e0d0344","name":"Mustafa Mukadam","hidden":false},{"_id":"6996a91e1268a6b79e0d0345","user":{"_id":"66919fa1306a261f1850b27e","avatarUrl":"/avatars/34d7956986bef23d01c2bd4ebd4e9765.svg","isPro":false,"fullname":"Nima Fazeli","user":"nimafazeli","type":"user"},"name":"Nima Fazeli","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:09:26.108Z","hidden":false},{"_id":"6996a91e1268a6b79e0d0346","user":{"_id":"67a3e8063dd995045edcc833","avatarUrl":"/avatars/58d7fbade6a900a8748a245df5e5338e.svg","isPro":false,"fullname":"Tess","user":"tesshellebrekers","type":"user"},"name":"Tess Hellebrekers","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:09:32.523Z","hidden":false}],"publishedAt":"2026-02-14T03:31:32.000Z","submittedOnDailyAt":"2026-02-20T00:07:09.361Z","title":"TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment","submittedOnDailyBy":{"_id":"626c8c89a317717549e4e5aa","avatarUrl":"/avatars/b8bf54005b7b2a8fa0d10ed4557b675d.svg","isPro":false,"fullname":"youngsun wi","user":"youngw","type":"user"},"summary":"Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).","upvotes":10,"discussionId":"6996a91e1268a6b79e0d0347","projectPage":"https://yswi.github.io/tactalign/","ai_summary":"TactAlign enables transfer of human tactile demonstrations to robots with different embodiments through cross-embodiment tactile alignment without requiring paired data or manual labels.","ai_keywords":["human-to-robot transfer","tactile alignment","rectified flow","latent representation","pseudo-pairs","contact-rich tasks","zero-shot transfer"],"organization":{"_id":"63df4874e742e86dc925d67c","name":"umich","fullname":"University of Michigan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1675577443573-63df328115266dd945fc01f4.png"}},"publishedAt":"2026-02-13T22:31:32.000Z","title":"TactAlign: Human-to-Robot Policy Transfer via Tactile Alignment","summary":"Human demonstrations collected by wearable devices (e.g., tactile gloves) provide fast and dexterous supervision for policy learning, and are guided by rich, natural tactile feedback. However, a key challenge is how to transfer human-collected tactile signals to robots despite the differences in sensing modalities and embodiment. Existing human-to-robot (H2R) approaches that incorporate touch often assume identical tactile sensors, require paired data, and involve little to no embodiment gap between human demonstrator and the robots, limiting scalability and generality. We propose TactAlign, a cross-embodiment tactile alignment method that transfers human-collected tactile signals to a robot with different embodiment. TactAlign transforms human and robot tactile observations into a shared latent representation using a rectified flow, without paired datasets, manual labels, or privileged information. Our method enables low-cost latent transport guided by hand-object interaction-derived pseudo-pairs. We demonstrate that TactAlign improves H2R policy transfer across multiple contact-rich tasks (pivoting, insertion, lid closing), generalizes to unseen objects and tasks with human data (less than 5 minutes), and enables zero-shot H2R transfer on a highly dexterous tasks (light bulb screwing).","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13579.png","numComments":3,"submittedBy":{"_id":"626c8c89a317717549e4e5aa","avatarUrl":"/avatars/b8bf54005b7b2a8fa0d10ed4557b675d.svg","fullname":"youngsun wi","name":"youngw","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"63df4874e742e86dc925d67c","name":"umich","fullname":"University of Michigan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1675577443573-63df328115266dd945fc01f4.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.17365","authors":[{"_id":"6997d0587a658569d5a10173","name":"Yiming Guan","hidden":false},{"_id":"6997d0587a658569d5a10174","user":{"_id":"68e278859cfbd3ce4f4ad60b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FL65Bz18nprjeHchx52lV.png","isPro":false,"fullname":"Rui Yu","user":"ruiyu0","type":"user"},"name":"Rui Yu","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:11:06.649Z","hidden":false},{"_id":"6997d0587a658569d5a10175","name":"John Zhang","hidden":false},{"_id":"6997d0587a658569d5a10176","name":"Lu Wang","hidden":false},{"_id":"6997d0587a658569d5a10177","user":{"_id":"654dbac9938fbf1e696be8aa","avatarUrl":"/avatars/b3c4035c48169c1bfb04a439fce3499f.svg","isPro":true,"fullname":"Chaoyun Zhang","user":"vyokky","type":"user"},"name":"Chaoyun Zhang","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:11:37.500Z","hidden":false},{"_id":"6997d0587a658569d5a10178","name":"Liqun Li","hidden":false},{"_id":"6997d0587a658569d5a10179","name":"Bo Qiao","hidden":false},{"_id":"6997d0587a658569d5a1017a","name":"Si Qin","hidden":false},{"_id":"6997d0587a658569d5a1017b","name":"He Huang","hidden":false},{"_id":"6997d0587a658569d5a1017c","name":"Fangkai Yang","hidden":false},{"_id":"6997d0587a658569d5a1017d","name":"Pu Zhao","hidden":false},{"_id":"6997d0587a658569d5a1017e","user":{"_id":"6380a37a5c62156ce7dff8b9","avatarUrl":"/avatars/fbe5a20869cb55ec43759c1b5f9c4135.svg","isPro":false,"fullname":"Lukas Wutschitz","user":"wulu","type":"user"},"name":"Lukas Wutschitz","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:11:44.590Z","hidden":false},{"_id":"6997d0587a658569d5a1017f","name":"Samuel Kessler","hidden":false},{"_id":"6997d0587a658569d5a10180","name":"Huseyin A Inan","hidden":false},{"_id":"6997d0587a658569d5a10181","name":"Robert Sim","hidden":false},{"_id":"6997d0587a658569d5a10182","name":"Saravan Rajmohan","hidden":false},{"_id":"6997d0587a658569d5a10183","name":"Qingwei Lin","hidden":false},{"_id":"6997d0587a658569d5a10184","name":"Dongmei Zhang","hidden":false}],"publishedAt":"2026-02-19T13:48:29.000Z","submittedOnDailyAt":"2026-02-20T00:39:22.977Z","title":"Computer-Using World Model","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.","upvotes":9,"discussionId":"6997d0587a658569d5a10185","ai_summary":"A world model for desktop software that predicts UI state changes through textual description followed by visual synthesis, improving decision quality and execution robustness in computer-using tasks.","ai_keywords":["world model","user interface","UI state","action search","reinforcement learning","test-time scaling","computer-using environments","desktop software","textual description","visual synthesis"]},"publishedAt":"2026-02-19T08:48:29.000Z","title":"Computer-Using World Model","summary":"Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17365.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.16849","authors":[{"_id":"6997d62f7a658569d5a101b1","user":{"_id":"6852f69c7b834665dc38c837","avatarUrl":"/avatars/bf9c5fc72300756e88319ec45c75f337.svg","isPro":false,"fullname":"Jianliang He","user":"JLiangHe","type":"user"},"name":"Jianliang He","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:32:54.929Z","hidden":false},{"_id":"6997d62f7a658569d5a101b2","name":"Leda Wang","hidden":false},{"_id":"6997d62f7a658569d5a101b3","name":"Siyu Chen","hidden":false},{"_id":"6997d62f7a658569d5a101b4","name":"Zhuoran Yang","hidden":false}],"publishedAt":"2026-02-18T20:25:13.000Z","submittedOnDailyAt":"2026-02-20T11:45:48.545Z","title":"On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking","submittedOnDailyBy":{"_id":"6852f69c7b834665dc38c837","avatarUrl":"/avatars/bf9c5fc72300756e88319ec45c75f337.svg","isPro":false,"fullname":"Jianliang He","user":"JLiangHe","type":"user"},"summary":"We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the \"winner\" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay.","upvotes":6,"discussionId":"6997d62f7a658569d5a101b5","githubRepo":"https://github.com/Y-Agent/modular-addition-feature-learning","githubRepoAddedBy":"user","ai_summary":"Two-layer neural networks solve modular addition by learning Fourier features through phase symmetry and frequency diversification, enabling robust computation via majority voting despite individual neuron noise.","ai_keywords":["two-layer neural networks","modular addition","Fourier features","phase symmetry","frequency diversification","overparametrized","indicator function","majority-voting scheme","random initialization","lottery ticket mechanism","gradient flow analysis","phase coupling dynamics","ODE comparison lemma","grokking","memorization","generalization phases","loss minimization","weight decay"],"githubStars":4,"organization":{"_id":"67b18bdbd2ee8e627d9b0240","name":"y-agent","fullname":"Zhuoran Yang Research Group"}},"publishedAt":"2026-02-18T15:25:13.000Z","title":"On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking","summary":"We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the \"winner\" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16849.png","numComments":2,"submittedBy":{"_id":"6852f69c7b834665dc38c837","avatarUrl":"/avatars/bf9c5fc72300756e88319ec45c75f337.svg","fullname":"Jianliang He","name":"JLiangHe","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"67b18bdbd2ee8e627d9b0240","name":"y-agent","fullname":"Zhuoran Yang Research Group"},"isAuthorParticipating":true},{"paper":{"id":"2602.17363","authors":[{"_id":"6997d08a7a658569d5a10187","user":{"_id":"62a1280b88bfb47fc40fe75b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62a1280b88bfb47fc40fe75b/u6teJWcB6BWdD04G7g6uy.png","isPro":true,"fullname":"Gabriel Mongaras","user":"gmongaras","type":"user"},"name":"Gabriel Mongaras","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:10:43.401Z","hidden":false},{"_id":"6997d08a7a658569d5a10188","name":"Eric C. Larson","hidden":false}],"publishedAt":"2026-02-19T13:45:23.000Z","submittedOnDailyAt":"2026-02-20T00:41:59.598Z","title":"2Mamba2Furious: Linear in Complexity, Competitive in Accuracy","submittedOnDailyBy":{"_id":"62a1280b88bfb47fc40fe75b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62a1280b88bfb47fc40fe75b/u6teJWcB6BWdD04G7g6uy.png","isPro":true,"fullname":"Gabriel Mongaras","user":"gmongaras","type":"user"},"summary":"Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments","upvotes":5,"discussionId":"6997d08a7a658569d5a10189","githubRepo":"https://github.com/gmongaras/2Mamba2Furious","githubRepoAddedBy":"user","ai_summary":"Researchers enhance linear attention by simplifying Mamba-2 and improving its architectural components to achieve near-softmax accuracy while maintaining memory efficiency for long sequences.","ai_keywords":["linear attention","softmax attention","Mamba-2","hidden state","A-mask","2Mamba"],"githubStars":1,"organization":{"_id":"624c872c32593418bbadb691","name":"smu","fullname":"Southern Methodist University AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1649306109576-624c860229f11fb240177dbb.png"}},"publishedAt":"2026-02-19T08:45:23.000Z","title":"2Mamba2Furious: Linear in Complexity, Competitive in Accuracy","summary":"Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17363.png","numComments":4,"submittedBy":{"_id":"62a1280b88bfb47fc40fe75b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62a1280b88bfb47fc40fe75b/u6teJWcB6BWdD04G7g6uy.png","fullname":"Gabriel Mongaras","name":"gmongaras","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":10,"isUserFollowing":false},"organization":{"_id":"624c872c32593418bbadb691","name":"smu","fullname":"Southern Methodist University AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1649306109576-624c860229f11fb240177dbb.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.17288","authors":[{"_id":"6997cf657a658569d5a10170","user":{"_id":"659bb3d3539c808e846d71f4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/659bb3d3539c808e846d71f4/MAc7JwgNsa9HMVuuT8zb_.jpeg","isPro":false,"fullname":"Anuj Gupta","user":"anuj0456","type":"user"},"name":"Anuj Gupta","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:36:20.757Z","hidden":false}],"publishedAt":"2026-02-19T11:47:30.000Z","submittedOnDailyAt":"2026-02-20T06:38:35.874Z","title":"ArXiv-to-Model: A Practical Study of Scientific LM Training","submittedOnDailyBy":{"_id":"659bb3d3539c808e846d71f4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/659bb3d3539c808e846d71f4/MAc7JwgNsa9HMVuuT8zb_.jpeg","isPro":false,"fullname":"Anuj Gupta","user":"anuj0456","type":"user"},"summary":"While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.","upvotes":5,"discussionId":"6997cf667a658569d5a10171","projectPage":"https://kitefishai.com","githubRepo":"https://github.com/kitefishai/KiteFish-A1-1.5B-Math","githubRepoAddedBy":"user","ai_summary":"Training a 1.36B-parameter scientific language model from raw arXiv LaTeX sources demonstrates the impact of preprocessing decisions, tokenization, and infrastructure constraints on model development under limited computational resources.","ai_keywords":["large language models","scientific language models","arXiv LaTeX","transformers","tokenization","pretraining","training stability","scaling behavior","data yield losses","infrastructure bottlenecks","convergence dynamics"],"githubStars":2,"organization":{"_id":"696ca7f8ea65e4b952749c86","name":"KiteFishAI","fullname":"KiteFishAI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/659bb3d3539c808e846d71f4/uPi6a1htnTFrlnsaByIFe.png"}},"publishedAt":"2026-02-19T06:47:30.000Z","title":"ArXiv-to-Model: A Practical Study of Scientific LM Training","summary":"While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17288.png","numComments":2,"submittedBy":{"_id":"659bb3d3539c808e846d71f4","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/659bb3d3539c808e846d71f4/MAc7JwgNsa9HMVuuT8zb_.jpeg","fullname":"Anuj Gupta","name":"anuj0456","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"696ca7f8ea65e4b952749c86","name":"KiteFishAI","fullname":"KiteFishAI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/659bb3d3539c808e846d71f4/uPi6a1htnTFrlnsaByIFe.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.16928","authors":[{"_id":"6997d19a7a658569d5a1018b","name":"Zun Li","hidden":false},{"_id":"6997d19a7a658569d5a1018c","name":"John Schultz","hidden":false},{"_id":"6997d19a7a658569d5a1018d","name":"Daniel Hennes","hidden":false},{"_id":"6997d19a7a658569d5a1018e","user":{"_id":"6781563e5fe908323d5a4ae9","avatarUrl":"/avatars/8dc486f4310e772e7f462726d57a3508.svg","isPro":false,"fullname":"Marc Lanctot","user":"lanctot","type":"user"},"name":"Marc Lanctot","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:04:51.609Z","hidden":false}],"publishedAt":"2026-02-18T22:41:00.000Z","submittedOnDailyAt":"2026-02-20T00:44:41.311Z","title":"Discovering Multiagent Learning Algorithms with Large Language Models","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.","upvotes":5,"discussionId":"6997d19a7a658569d5a1018f","ai_summary":"AlphaEvolve, an evolutionary coding agent using large language models, automatically discovers new multiagent learning algorithms for imperfect-information games by evolving regret minimization and population-based training variants.","ai_keywords":["Multi-Agent Reinforcement Learning","imperfect-information games","Counterfactual Regret Minimization","Policy Space Response Oracles","evolutionary coding","large language models","regret accumulation","policy derivation","Volatility-Adaptive Discounted CFR","discounted predictive CFR","population based training","meta strategy solvers","Optimistic Regret Matching","smoothed distribution","equilibrium finding"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-18T17:41:00.000Z","title":"Discovering Multiagent Learning Algorithms with Large Language Models","summary":"Much of the advancement of Multi-Agent Reinforcement Learning (MARL) in imperfect-information games has historically depended on manual iterative refinement of baselines. While foundational families like Counterfactual Regret Minimization (CFR) and Policy Space Response Oracles (PSRO) rest on solid theoretical ground, the design of their most effective variants often relies on human intuition to navigate a vast algorithmic design space. In this work, we propose the use of AlphaEvolve, an evolutionary coding agent powered by large language models, to automatically discover new multiagent learning algorithms. We demonstrate the generality of this framework by evolving novel variants for two distinct paradigms of game-theoretic learning. First, in the domain of iterative regret minimization, we evolve the logic governing regret accumulation and policy derivation, discovering a new algorithm, Volatility-Adaptive Discounted (VAD-)CFR. VAD-CFR employs novel, non-intuitive mechanisms-including volatility-sensitive discounting, consistency-enforced optimism, and a hard warm-start policy accumulation schedule-to outperform state-of-the-art baselines like Discounted Predictive CFR+. Second, in the regime of population based training algorithms, we evolve training-time and evaluation-time meta strategy solvers for PSRO, discovering a new variant, Smoothed Hybrid Optimistic Regret (SHOR-)PSRO. SHOR-PSRO introduces a hybrid meta-solver that linearly blends Optimistic Regret Matching with a smoothed, temperature-controlled distribution over best pure strategies. By dynamically annealing this blending factor and diversity bonuses during training, the algorithm automates the transition from population diversity to rigorous equilibrium finding, yielding superior empirical convergence compared to standard static meta-solvers.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16928.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.17259","authors":[{"_id":"6997de047a658569d5a101dc","user":{"_id":"646dbbc8075bbcc48ddcecbf","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/646dbbc8075bbcc48ddcecbf/V52Em-78O5F3QxRbRwG5O.jpeg","isPro":false,"fullname":"Han Zhao","user":"han1997","type":"user"},"name":"Han Zhao","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:06:11.078Z","hidden":false},{"_id":"6997de047a658569d5a101dd","user":{"_id":"66a4ed2a9ba24c30408441b0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66a4ed2a9ba24c30408441b0/7mdKBlVyG1I_G5OTheOTS.jpeg","isPro":false,"fullname":"Jingbo Wang","user":"hhhJB","type":"user"},"name":"Jingbo Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:32:45.054Z","hidden":false},{"_id":"6997de047a658569d5a101de","name":"Wenxuan Song","hidden":false},{"_id":"6997de047a658569d5a101df","name":"Shuai Chen","hidden":false},{"_id":"6997de047a658569d5a101e0","name":"Yang Liu","hidden":false},{"_id":"6997de047a658569d5a101e1","name":"Yan Wang","hidden":false},{"_id":"6997de047a658569d5a101e2","name":"Haoang Li","hidden":false},{"_id":"6997de047a658569d5a101e3","user":{"_id":"67597be2f3cd6492d4162ef8","avatarUrl":"/avatars/ba580c04b7057927d4a22dcb44c52400.svg","isPro":false,"fullname":"DONGLIN","user":"wangdonglin130","type":"user"},"name":"Donglin Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:06:38.415Z","hidden":false}],"publishedAt":"2026-02-19T11:00:46.000Z","submittedOnDailyAt":"2026-02-20T03:29:46.311Z","title":"FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment","submittedOnDailyBy":{"_id":"646dbbc8075bbcc48ddcecbf","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/646dbbc8075bbcc48ddcecbf/V52Em-78O5F3QxRbRwG5O.jpeg","isPro":false,"fullname":"Han Zhao","user":"han1997","type":"user"},"summary":"Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.","upvotes":4,"discussionId":"6997de057a658569d5a101e4","projectPage":"https://h-zhao1997.github.io/frappe/","githubRepo":"https://github.com/OpenHelix-Team/frappe","githubRepoAddedBy":"user","ai_summary":"FRAPPE addresses limitations in world modeling for robotics by using parallel progressive expansion to improve representation alignment and reduce error accumulation in predictive models.","ai_keywords":["world modeling","robotic reasoning","generalization","pixel-level reconstruction","error accumulation","future representation alignment","parallel progressive expansion","fine-tuning strategy","latent representations","visual foundation models","world-awareness","generalist robotic policies","RoboTwin benchmark"],"githubStars":14},"publishedAt":"2026-02-19T06:00:46.000Z","title":"FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment","summary":"Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17259.png","numComments":2,"submittedBy":{"_id":"646dbbc8075bbcc48ddcecbf","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/646dbbc8075bbcc48ddcecbf/V52Em-78O5F3QxRbRwG5O.jpeg","fullname":"Han Zhao","name":"han1997","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":8,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.16756","authors":[{"_id":"69986136bf26544d68b40955","name":"Johannes Bertram","hidden":false},{"_id":"69986136bf26544d68b40956","name":"Jonas Geiping","hidden":false}],"publishedAt":"2026-02-18T09:41:51.000Z","submittedOnDailyAt":"2026-02-20T13:14:22.529Z","title":"NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist","submittedOnDailyBy":{"_id":"63d86dbf3130cadcaf8bdd11","avatarUrl":"/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg","isPro":false,"fullname":"Jonas Geiping","user":"JonasGeiping","type":"user"},"summary":"We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety, even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.","upvotes":3,"discussionId":"69986136bf26544d68b40957","projectPage":"https://huggingface.co/datasets/JByale/NESSiE","githubRepo":"https://github.com/JohannesBertram/NESSiE","githubRepoAddedBy":"user","ai_summary":"NESSiE benchmark reveals safety vulnerabilities in large language models through simple security tests, demonstrating that even state-of-the-art models fail basic safety requirements without adversarial attacks.","ai_keywords":["large language models","safety benchmark","information security","access security","sanity check","language model safety","Safe & Helpful metric","autonomous agents"],"githubStars":3},"publishedAt":"2026-02-18T04:41:51.000Z","title":"NESSiE: The Necessary Safety Benchmark -- Identifying Errors that should not Exist","summary":"We introduce NESSiE, the NEceSsary SafEty benchmark for large language models (LLMs). With minimal test cases of information and access security, NESSiE reveals safety-relevant failures that should not exist, given the low complexity of the tasks. NESSiE is intended as a lightweight, easy-to-use sanity check for language model safety and, as such, is not sufficient for guaranteeing safety in general -- but we argue that passing this test is necessary for any deployment. However, even state-of-the-art LLMs do not reach 100% on NESSiE and thus fail our necessary condition of language model safety, even in the absence of adversarial attacks. Our Safe & Helpful (SH) metric allows for direct comparison of the two requirements, showing models are biased toward being helpful rather than safe. We further find that disabled reasoning for some models, but especially a benign distraction context degrade model performance. Overall, our results underscore the critical risks of deploying such models as autonomous agents in the wild. We make the dataset, package and plotting code publicly available.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16756.png","numComments":2,"submittedBy":{"_id":"63d86dbf3130cadcaf8bdd11","avatarUrl":"/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg","fullname":"Jonas Geiping","name":"JonasGeiping","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":36,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.15823","authors":[{"_id":"699749637a658569d5a1009e","name":"Zarif Ikram","hidden":false},{"_id":"699749637a658569d5a1009f","name":"Arad Firouzkouhi","hidden":false},{"_id":"699749637a658569d5a100a0","name":"Stephen Tu","hidden":false},{"_id":"699749637a658569d5a100a1","name":"Mahdi Soltanolkotabi","hidden":false},{"_id":"699749637a658569d5a100a2","user":{"_id":"6488c5b833ff40e585bdcf64","avatarUrl":"/avatars/3bfbc2ecccd798a67afe1555ae7bb919.svg","isPro":false,"fullname":"Paria Rashidinejad","user":"pariard","type":"user"},"name":"Paria Rashidinejad","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:37:04.735Z","hidden":false}],"publishedAt":"2026-02-17T18:58:04.000Z","submittedOnDailyAt":"2026-02-20T14:38:23.620Z","title":"CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing","submittedOnDailyBy":{"_id":"6488c5b833ff40e585bdcf64","avatarUrl":"/avatars/3bfbc2ecccd798a67afe1555ae7bb919.svg","isPro":false,"fullname":"Paria Rashidinejad","user":"pariard","type":"user"},"summary":"A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.","upvotes":2,"discussionId":"699749637a658569d5a100a3","projectPage":"https://crispedit.github.io","githubRepo":"https://github.com/zarifikram/CrispEdit","githubRepoAddedBy":"user","ai_summary":"CrispEdit is a second-order editing algorithm for large language models that preserves capabilities by constraining updates to low-curvature subspaces of the capability-loss landscape using Bregman divergence and efficient Kronecker-factored approximations.","ai_keywords":["large language model","capability preservation","constrained optimization","Bregman divergence","Gauss-Newton Hessian","Kronecker-factored approximate curvature","matrix-free projector","low-curvature subspace","edit success","capability degradation"],"githubStars":2,"organization":{"_id":"66a403d0dcb5bbc6e98bb7d0","name":"UniversityofSouthernCalifornia","fullname":"University of Southern California","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"}},"publishedAt":"2026-02-17T13:58:04.000Z","title":"CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing","summary":"A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15823.png","numComments":2,"submittedBy":{"_id":"6488c5b833ff40e585bdcf64","avatarUrl":"/avatars/3bfbc2ecccd798a67afe1555ae7bb919.svg","fullname":"Paria Rashidinejad","name":"pariard","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"66a403d0dcb5bbc6e98bb7d0","name":"UniversityofSouthernCalifornia","fullname":"University of Southern California","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66a403728069e3c30e0d8524/tkYCfeIJfF1FxtYiRZ8bf.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.14857","authors":[{"_id":"6997f8677a658569d5a10242","user":{"_id":"6588fe10cef9b8827b1d0c82","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6588fe10cef9b8827b1d0c82/8vr9HnlvGa20p48k4oIVZ.jpeg","isPro":false,"fullname":"Yixin Zhang","user":"yxzhang2024","type":"user"},"name":"Yixin Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:32:38.896Z","hidden":false},{"_id":"6997f8677a658569d5a10243","name":"Ziyi Wang","hidden":false},{"_id":"6997f8677a658569d5a10244","name":"Yiming Rong","hidden":false},{"_id":"6997f8677a658569d5a10245","name":"Haoxi Wang","hidden":false},{"_id":"6997f8677a658569d5a10246","name":"Jinling Jiang","hidden":false},{"_id":"6997f8677a658569d5a10247","name":"Shuang Xu","hidden":false},{"_id":"6997f8677a658569d5a10248","name":"Haoran Wu","hidden":false},{"_id":"6997f8677a658569d5a10249","name":"Shiyu Zhou","hidden":false},{"_id":"6997f8677a658569d5a1024a","name":"Bo Xu","hidden":false}],"publishedAt":"2026-02-16T15:51:59.000Z","submittedOnDailyAt":"2026-02-20T06:21:30.016Z","title":"World Models for Policy Refinement in StarCraft II","submittedOnDailyBy":{"_id":"6588fe10cef9b8827b1d0c82","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6588fe10cef9b8827b1d0c82/8vr9HnlvGa20p48k4oIVZ.jpeg","isPro":false,"fullname":"Yixin Zhang","user":"yxzhang2024","type":"user"},"summary":"Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.","upvotes":2,"discussionId":"6997f8677a658569d5a1024b","githubRepo":"https://github.com/yxzzhang/StarWM","githubRepoAddedBy":"user","ai_summary":"StarWM, a world model for StarCraft II, predicts future observations under partial observability using a structured textual representation and achieves significant performance improvements in win rate and decision-making stability.","ai_keywords":["world model","StarCraft II","partial observability","action-conditioned transition model","structured textual representation","SC2-Dynamics-50k","offline evaluation","Generate-Simulate-Refine loop","foresight-driven policy refinement"],"githubStars":2,"organization":{"_id":"640a887796aae649741a586f","name":"CASIA","fullname":"Chinese Academic of Science Institute of Automation","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"}},"publishedAt":"2026-02-16T10:51:59.000Z","title":"World Models for Policy Refinement in StarCraft II","summary":"Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14857.png","numComments":3,"submittedBy":{"_id":"6588fe10cef9b8827b1d0c82","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6588fe10cef9b8827b1d0c82/8vr9HnlvGa20p48k4oIVZ.jpeg","fullname":"Yixin Zhang","name":"yxzhang2024","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"640a887796aae649741a586f","name":"CASIA","fullname":"Chinese Academic of Science Institute of Automation","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1678411888885-6388984e8a5dbe2f3dc5afee.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.17588","authors":[{"_id":"69988dbcf723198bd53a5fdd","name":"Faria Huq","hidden":false},{"_id":"69988dbcf723198bd53a5fde","name":"Zora Zhiruo Wang","hidden":false},{"_id":"69988dbcf723198bd53a5fdf","name":"Zhanqiu Guo","hidden":false},{"_id":"69988dbcf723198bd53a5fe0","name":"Venu Arvind Arangarajan","hidden":false},{"_id":"69988dbcf723198bd53a5fe1","name":"Tianyue Ou","hidden":false},{"_id":"69988dbcf723198bd53a5fe2","name":"Frank Xu","hidden":false},{"_id":"69988dbcf723198bd53a5fe3","name":"Shuyan Zhou","hidden":false},{"_id":"69988dbcf723198bd53a5fe4","name":"Graham Neubig","hidden":false},{"_id":"69988dbcf723198bd53a5fe5","name":"Jeffrey P. Bigham","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/638450f2834d3558a39939f4/K6q25ZcaC8rhmGSHVppse.png"],"publishedAt":"2026-02-19T18:11:28.000Z","submittedOnDailyAt":"2026-02-20T16:00:17.151Z","title":"Modeling Distinct Human Interaction in Web Agents","submittedOnDailyBy":{"_id":"638450f2834d3558a39939f4","avatarUrl":"/avatars/ab8efebd3aa50b31429046b60d8aa3c2.svg","isPro":false,"fullname":"Faria Huq","user":"oaishi","type":"user"},"summary":"Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.","upvotes":1,"discussionId":"69988dbdf723198bd53a5fe6","projectPage":"https://cowcorpus.github.io/","githubRepo":"https://github.com/oaishi/PlowPilot","githubRepoAddedBy":"user","ai_summary":"Human intervention patterns in web navigation are modeled to improve agent adaptability and collaboration, with language models achieving better intervention prediction and user satisfaction.","ai_keywords":["human intervention","web navigation trajectories","language models","intervention prediction","agent usefulness"],"githubStars":0,"organization":{"_id":"6362c6c180c1a705a6ed0d57","name":"CMU-SCS","fullname":"Carnegie Mellon University School of Computer Science"}},"publishedAt":"2026-02-19T13:11:28.000Z","title":"Modeling Distinct Human Interaction in Web Agents","summary":"Despite rapid progress in autonomous web agents, human involvement remains essential for shaping preferences and correcting agent behavior as tasks unfold. However, current agentic systems lack a principled understanding of when and why humans intervene, often proceeding autonomously past critical decision points or requesting unnecessary confirmation. In this work, we introduce the task of modeling human intervention to support collaborative web task execution. We collect CowCorpus, a dataset of 400 real-user web navigation trajectories containing over 4,200 interleaved human and agent actions. We identify four distinct patterns of user interaction with agents -- hands-off supervision, hands-on oversight, collaborative task-solving, and full user takeover. Leveraging these insights, we train language models (LMs) to anticipate when users are likely to intervene based on their interaction styles, yielding a 61.4-63.4% improvement in intervention prediction accuracy over base LMs. Finally, we deploy these intervention-aware models in live web navigation agents and evaluate them in a user study, finding a 26.5% increase in user-rated agent usefulness. Together, our results show structured modeling of human intervention leads to more adaptive, collaborative agents.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/638450f2834d3558a39939f4/K6q25ZcaC8rhmGSHVppse.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17588.png","numComments":2,"submittedBy":{"_id":"638450f2834d3558a39939f4","avatarUrl":"/avatars/ab8efebd3aa50b31429046b60d8aa3c2.svg","fullname":"Faria Huq","name":"oaishi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"6362c6c180c1a705a6ed0d57","name":"CMU-SCS","fullname":"Carnegie Mellon University School of Computer Science"},"isAuthorParticipating":false},{"paper":{"id":"2602.16802","authors":[{"_id":"6997e5f37a658569d5a10217","user":{"_id":"61993bc305b9430e5369db41","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61993bc305b9430e5369db41/YmLB-UlaeCHhp5QHxNga-.png","isPro":true,"fullname":"Kejian Shi","user":"kejian","type":"user"},"name":"Kejian Shi","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:09:51.993Z","hidden":false},{"_id":"6997e5f37a658569d5a10218","user":{"_id":"6244de1c1c560fb11edfca44","avatarUrl":"/avatars/36558928bd04be7f49837d4c603681d7.svg","isPro":true,"fullname":"Yixin Liu","user":"henryL7","type":"user"},"name":"Yixin Liu","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:10:11.122Z","hidden":false},{"_id":"6997e5f37a658569d5a10219","user":{"_id":"680fd00a1ae38d47c1670ef2","avatarUrl":"/avatars/31610878448dce5032ab9305f2b87643.svg","isPro":false,"fullname":"Peifeng Wang","user":"peifengw","type":"user"},"name":"Peifeng Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:10:17.382Z","hidden":false},{"_id":"6997e5f37a658569d5a1021a","name":"Alexander R. Fabbri","hidden":false},{"_id":"6997e5f37a658569d5a1021b","name":"Shafiq Joty","hidden":false},{"_id":"6997e5f37a658569d5a1021c","user":{"_id":"5f5ba21188f57f65f951f255","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1599840760465-noauth.png","isPro":false,"fullname":"Arman Cohan","user":"armanc","type":"user"},"name":"Arman Cohan","status":"admin_assigned","statusLastChangedAt":"2026-02-20T09:10:33.361Z","hidden":false}],"publishedAt":"2026-02-18T19:03:34.000Z","submittedOnDailyAt":"2026-02-20T02:12:33.322Z","title":"References Improve LLM Alignment in Non-Verifiable Domains","submittedOnDailyBy":{"_id":"6244de1c1c560fb11edfca44","avatarUrl":"/avatars/36558928bd04be7f49837d4c603681d7.svg","isPro":true,"fullname":"Yixin Liu","user":"henryL7","type":"user"},"summary":"While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.","upvotes":1,"discussionId":"6997e5f37a658569d5a1021d","githubRepo":"https://github.com/yale-nlp/RLRR","githubRepoAddedBy":"user","ai_summary":"Reference-guided LLM-evaluators enhance LLM alignment by serving as soft verifiers, improving judge accuracy and enabling effective post-training in non-verifiable domains through self-improvement techniques.","ai_keywords":["Reinforcement Learning with Verifiable Rewards","LLM alignment","reference-guided evaluators","LLM judges","self-improvement","SFT","ArmoRM","AlpacaEval","Arena-Hard"],"githubStars":1,"organization":{"_id":"6532df27d690f3012efde84c","name":"yale-nlp","fullname":"Yale NLP Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65204db5b0e0d57453cb1809/9OAeiZ-BrN2g1h1yd6-1W.png"}},"publishedAt":"2026-02-18T14:03:34.000Z","title":"References Improve LLM Alignment in Non-Verifiable Domains","summary":"While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16802.png","numComments":2,"submittedBy":{"_id":"6244de1c1c560fb11edfca44","avatarUrl":"/avatars/36558928bd04be7f49837d4c603681d7.svg","fullname":"Yixin Liu","name":"henryL7","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6532df27d690f3012efde84c","name":"yale-nlp","fullname":"Yale NLP Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65204db5b0e0d57453cb1809/9OAeiZ-BrN2g1h1yd6-1W.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.10377","authors":[{"_id":"6998cee6f723198bd53a601b","name":"Luoyang Sun","hidden":false},{"_id":"6998cee6f723198bd53a601c","name":"Jiwen Jiang","hidden":false},{"_id":"6998cee6f723198bd53a601d","name":"Yifeng Ding","hidden":false},{"_id":"6998cee6f723198bd53a601e","name":"Fengfa Li","hidden":false},{"_id":"6998cee6f723198bd53a601f","name":"Yan Song","hidden":false},{"_id":"6998cee6f723198bd53a6020","name":"Haifeng Zhang","hidden":false},{"_id":"6998cee6f723198bd53a6021","name":"Jian Ying","hidden":false},{"_id":"6998cee6f723198bd53a6022","name":"Lei Ren","hidden":false},{"_id":"6998cee6f723198bd53a6023","name":"Kun Zhan","hidden":false},{"_id":"6998cee6f723198bd53a6024","name":"Wei Chen","hidden":false},{"_id":"6998cee6f723198bd53a6025","name":"Yan Xie","hidden":false},{"_id":"6998cee6f723198bd53a6026","name":"Cheng Deng","hidden":false}],"publishedAt":"2026-02-10T23:51:00.000Z","submittedOnDailyAt":"2026-02-20T18:47:33.177Z","title":"Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs","submittedOnDailyBy":{"_id":"637b18137ce76c3b834e0938","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/637b18137ce76c3b834e0938/nOnd-QKIPml9MXHCJh4tb.jpeg","isPro":true,"fullname":"Daven","user":"daven3","type":"user"},"summary":"Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.","upvotes":1,"discussionId":"6998cee7f723198bd53a6027","ai_summary":"A hardware-software co-design framework for on-device large language model deployment that establishes accuracy-latency relationships through training loss modeling and roofline analysis, enabling rapid architecture selection and improved performance.","ai_keywords":["vision-language-action models","physical AI","large language models","hardware-software co-design","architectural hyperparameters","roofline modelling","scaling laws","pareto frontier","architecture search","precision","performance optimization"]},"publishedAt":"2026-02-10T18:51:00.000Z","title":"Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs","summary":"Vision-Language-Action Models (VLAs) have emerged as a key paradigm of Physical AI and are increasingly deployed in autonomous vehicles, robots, and smart spaces. In these resource-constrained on-device settings, selecting an appropriate large language model (LLM) backbone is a critical challenge: models must balance accuracy with strict inference latency and hardware efficiency constraints. This makes hardware-software co-design a game-changing requirement for on-device LLM deployment, where each hardware platform demands a tailored architectural solution. We propose a hardware co-design law that jointly captures model accuracy and inference performance. Specifically, we model training loss as an explicit function of architectural hyperparameters and characterise inference latency via roofline modelling. We empirically evaluate 1,942 candidate architectures on NVIDIA Jetson Orin, training 170 selected models for 10B tokens each to fit a scaling law relating architecture to training loss. By coupling this scaling law with latency modelling, we establish a direct accuracy-latency correspondence and identify the Pareto frontier for hardware co-designed LLMs. We further formulate architecture search as a joint optimisation over precision and performance, deriving feasible design regions under industrial hardware and application budgets. Our approach reduces architecture selection from months to days. At the same latency as Qwen2.5-0.5B on the target hardware, our co-designed architecture achieves 19.42% lower perplexity on WikiText-2. To our knowledge, this is the first principled and operational framework for hardware co-design scaling laws in on-device LLM deployment. We will make the code and related checkpoints publicly available.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10377.png","numComments":2,"submittedBy":{"_id":"637b18137ce76c3b834e0938","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/637b18137ce76c3b834e0938/nOnd-QKIPml9MXHCJh4tb.jpeg","fullname":"Daven","name":"daven3","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.16915","authors":[{"_id":"69982ef8053a02322a78ffa2","name":"Zeyu Ren","hidden":false},{"_id":"69982ef8053a02322a78ffa3","name":"Xiang Li","hidden":false},{"_id":"69982ef8053a02322a78ffa4","name":"Yiran Wang","hidden":false},{"_id":"69982ef8053a02322a78ffa5","name":"Zeyu Zhang","hidden":false},{"_id":"69982ef8053a02322a78ffa6","name":"Hao Tang","hidden":false}],"publishedAt":"2026-02-18T22:12:08.000Z","submittedOnDailyAt":"2026-02-20T07:24:17.182Z","title":"StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation","submittedOnDailyBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"summary":"Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.","upvotes":0,"discussionId":"69982ef9053a02322a78ffa7","projectPage":"https://aigeeksgroup.github.io/StereoAdapter-2","githubRepo":"https://github.com/AIGeeksGroup/StereoAdapter-2","githubRepoAddedBy":"user","ai_summary":"StereoAdapter-2 improves underwater stereo depth estimation by replacing ConvGRU with a selective state space ConvSS2D operator for efficient long-range propagation and introduces a large-scale synthetic underwater dataset.","ai_keywords":["stereo depth estimation","underwater robotic perception","domain shift","light attenuation","scattering","refraction","monocular foundation models","GRU-based iterative refinement","ConvGRU","selective state space models","ConvSS2D","epipolar geometry","long-range disparity propagation","LoRA adaptation","zero-shot performance","TartanAir-UW","SQUID","BlueROV2"],"githubStars":3,"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}},"publishedAt":"2026-02-18T17:12:08.000Z","title":"StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation","summary":"Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16915.png","numComments":2,"submittedBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","fullname":"Zeyu Zhang","name":"SteveZeyuZhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.16835","authors":[{"_id":"69985d11bf26544d68b40937","name":"Sasha Behrouzi","hidden":false},{"_id":"69985d11bf26544d68b40938","name":"Lichao Wu","hidden":false},{"_id":"69985d11bf26544d68b40939","name":"Mohamadreza Rostami","hidden":false},{"_id":"69985d11bf26544d68b4093a","name":"Ahmad-Reza Sadeghi","hidden":false}],"publishedAt":"2026-02-18T20:01:01.000Z","submittedOnDailyAt":"2026-02-20T10:40:07.249Z","title":"NeST: Neuron Selective Tuning for LLM Safety","submittedOnDailyBy":{"_id":"6786535551cd69b65efd2bee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png","isPro":false,"fullname":"Lichao Wu","user":"woorkhaarder","type":"user"},"summary":"Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.\n  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.","upvotes":0,"discussionId":"69985d12bf26544d68b4093b","ai_summary":"NeST is a lightweight safety alignment framework that selectively adapts safety-relevant neurons while keeping the rest of the model frozen, achieving significant reductions in unsafe generations with minimal trainable parameters.","ai_keywords":["safety alignment","large language models","parameter-efficient fine-tuning","circuit breakers","neuron clustering","targeted safety adaptation","refusal behavior","attack success rate","trainable parameters"],"organization":{"_id":"656dd6ea7c934a7b3c4c59c2","name":"is-tuda","fullname":"Technical University of Darmstadt - Information Systems","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6464e09d0e6c7618f611d9d6/WXo06EWxaZCtWdSUftCzi.png"}},"publishedAt":"2026-02-18T15:01:01.000Z","title":"NeST: Neuron Selective Tuning for LLM Safety","summary":"Safety alignment is essential for the responsible deployment of large language models (LLMs). Yet, existing approaches often rely on heavyweight fine-tuning that is costly to update, audit, and maintain across model families. Full fine-tuning incurs substantial computational and storage overhead, while parameter-efficient methods such as LoRA trade efficiency for inconsistent safety gains and sensitivity to design choices. Safety intervention mechanisms such as circuit breakers reduce unsafe outputs without modifying model weights, but do not directly shape or preserve the internal representations that govern safety behavior. These limitations hinder rapid and reliable safety updates, particularly in settings where models evolve frequently or must adapt to new policies and domains.\n  We present NeST, a lightweight, structure-aware safety alignment framework that strengthens refusal behavior by selectively adapting a small subset of safety-relevant neurons while freezing the remainder of the model. NeST aligns parameter updates with the internal organization of safety behavior by clustering functionally coherent safety neurons and enforcing shared updates within each cluster, enabling targeted and stable safety adaptation without broad model modification or inference-time overhead. We benchmark NeST against three dominant baselines: full fine-tuning, LoRA-based fine-tuning, and circuit breakers across 10 open-weight LLMs spanning multiple model families and sizes. Across all evaluated models, NeST reduces the attack success rate from an average of 44.5% to 4.36%, corresponding to a 90.2% reduction in unsafe generations, while requiring only 0.44 million trainable parameters on average. This amounts to a 17,310x decrease in updated parameters compared to full fine-tuning and a 9.25x reduction relative to LoRA, while consistently achieving stronger safety performance for alignment.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16835.png","numComments":2,"submittedBy":{"_id":"6786535551cd69b65efd2bee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/v7tb5ofEh7YDLeSwi1lVz.png","fullname":"Lichao Wu","name":"woorkhaarder","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"656dd6ea7c934a7b3c4c59c2","name":"is-tuda","fullname":"Technical University of Darmstadt - Information Systems","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6464e09d0e6c7618f611d9d6/WXo06EWxaZCtWdSUftCzi.png"},"isAuthorParticipating":false}]