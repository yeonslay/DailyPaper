[{"paper":{"id":"2602.15763","authors":[{"_id":"6995270f8d17d1ee8c10ebc5","name":"GLM-5 Team","hidden":false},{"_id":"6995270f8d17d1ee8c10ebc7","name":"Aohan Zeng","hidden":false},{"_id":"6995270f8d17d1ee8c10ebc8","name":"Xin Lv","hidden":false},{"_id":"6995270f8d17d1ee8c10ebc9","user":{"_id":"62b196646d3d059f40c3df19","avatarUrl":"/avatars/dbddf54ae949437223f3a438d30ef653.svg","isPro":false,"fullname":"Zhenyu Hou","user":"think2try","type":"user"},"name":"Zhenyu Hou","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:31:01.080Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebca","user":{"_id":"63033dc4e1e7f0e03a5e1a31","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg","isPro":false,"fullname":"Zhengxiao Du","user":"zxdu20","type":"user"},"name":"Zhengxiao Du","status":"admin_assigned","statusLastChangedAt":"2026-02-18T09:21:03.474Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebcb","user":{"_id":"6231576e92e83fd1179ac3f0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1664543160657-6231576e92e83fd1179ac3f0.jpeg","isPro":false,"fullname":"Qinkai Zheng","user":"Stanislas","type":"user"},"name":"Qinkai Zheng","status":"admin_assigned","statusLastChangedAt":"2026-02-18T09:21:21.181Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebcc","name":"Bin Chen","hidden":false},{"_id":"6995270f8d17d1ee8c10ebcd","name":"Da Yin","hidden":false},{"_id":"6995270f8d17d1ee8c10ebce","name":"Chendi Ge","hidden":false},{"_id":"6995270f8d17d1ee8c10ebcf","user":{"_id":"62d00ff8dd7bdfc5e5c553c6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62d00ff8dd7bdfc5e5c553c6/u9Be7K16IS2Hc5OqKctEA.jpeg","isPro":false,"fullname":"chengxing xie","user":"yitianlian","type":"user"},"name":"Chengxing Xie","status":"admin_assigned","statusLastChangedAt":"2026-02-18T09:21:28.339Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd0","user":{"_id":"65eaf755ab0a6a90da55ab58","avatarUrl":"/avatars/a46890a9d067a913513edf3759f12c85.svg","isPro":false,"fullname":"Cunxiang Wang","user":"wangcunxiang","type":"user"},"name":"Cunxiang Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T10:16:12.082Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd1","name":"Gengzheng Pan","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd2","name":"Hao Zeng","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd3","user":{"_id":"622ec54ce27c88667db094ad","avatarUrl":"/avatars/8f06594b625a9c4fca1fffec4885bbdc.svg","isPro":false,"fullname":"Haoke Zhang","user":"zhk","type":"user"},"name":"Haoke Zhang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:33:43.672Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd4","name":"Haoran Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd5","user":{"_id":"654ccc038a299b6be086cf1b","avatarUrl":"/avatars/87d0f3c0f991368ce923da32cdd971a1.svg","isPro":false,"fullname":"Huilong Chen","user":"HuilongChen","type":"user"},"name":"Huilong Chen","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:32:16.751Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd6","name":"Jiajie Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd7","name":"Jian Jiao","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd8","name":"Jiaqi Guo","hidden":false},{"_id":"6995270f8d17d1ee8c10ebd9","user":{"_id":"65655c3ed35fc55406e116aa","avatarUrl":"/avatars/d45081ec1617bb737ea531866b76f57a.svg","isPro":false,"fullname":"jingsen","user":"wangjingsen","type":"user"},"name":"Jingsen Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:34:23.674Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebda","name":"Jingzhao Du","hidden":false},{"_id":"6995270f8d17d1ee8c10ebdb","user":{"_id":"650d8ddbcbd0c7d550dc8278","avatarUrl":"/avatars/3e49752b6b3c9f3875b4470cebb838e6.svg","isPro":false,"fullname":"wujinzhu","user":"kimjohn","type":"user"},"name":"Jinzhu Wu","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:34:46.441Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebdc","user":{"_id":"64b73d6353d91a364aa8cea5","avatarUrl":"/avatars/721553ab563cf13d8e7bfde088e3b753.svg","isPro":false,"fullname":"Kedong Wang","user":"mrwkd123","type":"user"},"name":"Kedong Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:32:47.078Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebdd","name":"Lei Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ebde","name":"Lin Fan","hidden":false},{"_id":"6995270f8d17d1ee8c10ebdf","user":{"_id":"60eff04e22ab0ac83b0fc9d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60eff04e22ab0ac83b0fc9d8/pBjftNyFN1qB8Br3FZQmD.jpeg","isPro":false,"fullname":"lucen zhong","user":"anchorzhong","type":"user"},"name":"Lucen Zhong","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:32:53.663Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe0","user":{"_id":"64f1abd2b12bdcef55e1c078","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64f1abd2b12bdcef55e1c078/yG9HR_imUWqESN-JNcjf1.jpeg","isPro":false,"fullname":"Mingdao Liu","user":"lambdax","type":"user"},"name":"Mingdao Liu","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:33:00.291Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe1","user":{"_id":"6706db274341dcee45811105","avatarUrl":"/avatars/125ef02f12785b8f51e84639d879efaa.svg","isPro":false,"fullname":"Mingming Zhao","user":"Lukas0510","type":"user"},"name":"Mingming Zhao","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:33:15.048Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe2","name":"Pengfan Du","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe3","name":"Qian Dong","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe4","user":{"_id":"63ca0433b047b9b12a1fcd9c","avatarUrl":"/avatars/435507115bfc502c494989677cfca5fb.svg","isPro":false,"fullname":"Rui Lu","user":"learning-rate","type":"user"},"name":"Rui Lu","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:17.212Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe5","name":"Shuang-Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe6","name":"Shulin Cao","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe7","name":"Song Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe8","name":"Ting Jiang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebe9","name":"Xiaodong Chen","hidden":false},{"_id":"6995270f8d17d1ee8c10ebea","name":"Xiaohan Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebeb","name":"Xuancheng Huang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebec","user":{"_id":"658834ba4f9d2b955e19d389","avatarUrl":"/avatars/89d9a6f9fac67391dc052d49def1e758.svg","isPro":false,"fullname":"Xuezhen Dong","user":"xzdong","type":"user"},"name":"Xuezhen Dong","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:33:25.460Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebed","name":"Yabo Xu","hidden":false},{"_id":"6995270f8d17d1ee8c10ebee","name":"Yao Wei","hidden":false},{"_id":"6995270f8d17d1ee8c10ebef","name":"Yifan An","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf0","name":"Yilin Niu","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf1","name":"Yitong Zhu","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf2","name":"Yuanhao Wen","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf3","name":"Yukuo Cen","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf4","user":{"_id":"64ed568ccf6118a9379a61b8","avatarUrl":"/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg","isPro":false,"fullname":"Yushi Bai","user":"bys0318","type":"user"},"name":"Yushi Bai","status":"claimed_verified","statusLastChangedAt":"2026-02-18T13:36:07.294Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf5","name":"Zhongpei Qiao","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf6","name":"Zihan Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf7","name":"Zikang Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf8","name":"Zilin Zhu","hidden":false},{"_id":"6995270f8d17d1ee8c10ebf9","name":"Ziqiang Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ebfa","user":{"_id":"68806066345a5d85e2494aba","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/AkrQMIp4kQm3Zn1giMa5I.png","isPro":false,"fullname":"Zixuan Li","user":"zixuanlimit","type":"user"},"name":"Zixuan Li","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:15.257Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ebfb","name":"Bojie Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebfc","name":"Bosi Wen","hidden":false},{"_id":"6995270f8d17d1ee8c10ebfd","name":"Can Huang","hidden":false},{"_id":"6995270f8d17d1ee8c10ebfe","name":"Changpeng Cai","hidden":false},{"_id":"6995270f8d17d1ee8c10ebff","name":"Chao Yu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec00","name":"Chen Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec01","name":"Chen Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec02","name":"Chenghua Huang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec03","name":"Chengwei Hu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec04","name":"Chenhui Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec05","name":"Chenzheng Zhu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec06","name":"Congfeng Yin","hidden":false},{"_id":"6995270f8d17d1ee8c10ec07","name":"Daoyan Lin","hidden":false},{"_id":"6995270f8d17d1ee8c10ec08","name":"Dayong Yang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec09","name":"Di Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec0a","name":"Ding Ai","hidden":false},{"_id":"6995270f8d17d1ee8c10ec0b","name":"Erle Zhu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec0c","user":{"_id":"66ea72fc1cbdd141c287ef22","avatarUrl":"/avatars/fe7f3281f7aade3045787fbb56f086c6.svg","isPro":false,"fullname":"BoyceYi","user":"DeadFishhh","type":"user"},"name":"Fangzhou Yi","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:13.041Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ec0d","name":"Feiyu Chen","hidden":false},{"_id":"6995270f8d17d1ee8c10ec0e","name":"Guohong Wen","hidden":false},{"_id":"6995270f8d17d1ee8c10ec0f","name":"Hailong Sun","hidden":false},{"_id":"6995270f8d17d1ee8c10ec10","name":"Haisha Zhao","hidden":false},{"_id":"6995270f8d17d1ee8c10ec11","name":"Haiyi Hu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec12","name":"Hanchen Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec13","name":"Hanrui Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec14","name":"Hanyu Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec15","name":"Hao Peng","hidden":false},{"_id":"6995270f8d17d1ee8c10ec16","name":"Hao Tai","hidden":false},{"_id":"6995270f8d17d1ee8c10ec17","name":"Haobo Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec18","name":"He Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec19","name":"Hongwei Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec1a","name":"Hongxi Yan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec1b","name":"Hongyu Ge","hidden":false},{"_id":"6995270f8d17d1ee8c10ec1c","name":"Huan Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec1d","name":"Huan Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec1e","name":"Huanpeng Chu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec1f","name":"Jia'ni Zhao","hidden":false},{"_id":"6995270f8d17d1ee8c10ec20","name":"Jiachen Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec21","name":"Jiajing Zhao","hidden":false},{"_id":"6995270f8d17d1ee8c10ec22","name":"Jiamin Ren","hidden":false},{"_id":"6995270f8d17d1ee8c10ec23","name":"Jiapeng Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec24","name":"Jiaxin Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec25","name":"Jiayi Gui","hidden":false},{"_id":"6995270f8d17d1ee8c10ec26","name":"Jiayue Zhao","hidden":false},{"_id":"6995270f8d17d1ee8c10ec27","name":"Jijie Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec28","name":"Jing An","hidden":false},{"_id":"6995270f8d17d1ee8c10ec29","name":"Jing Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec2a","name":"Jingwei Yuan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec2b","name":"Jinhua Du","hidden":false},{"_id":"6995270f8d17d1ee8c10ec2c","name":"Jinxin Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec2d","name":"Junkai Zhi","hidden":false},{"_id":"6995270f8d17d1ee8c10ec2e","name":"Junwen Duan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec2f","name":"Kaiyue Zhou","hidden":false},{"_id":"6995270f8d17d1ee8c10ec30","name":"Kangjian Wei","hidden":false},{"_id":"6995270f8d17d1ee8c10ec31","name":"Ke Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec32","name":"Keyun Luo","hidden":false},{"_id":"6995270f8d17d1ee8c10ec33","name":"Laiqiang Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec34","name":"Leigang Sha","hidden":false},{"_id":"6995270f8d17d1ee8c10ec35","name":"Liang Xu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec36","name":"Lindong Wu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec37","name":"Lintao Ding","hidden":false},{"_id":"6995270f8d17d1ee8c10ec38","name":"Lu Chen","hidden":false},{"_id":"6995270f8d17d1ee8c10ec39","name":"Minghao Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec3a","name":"Nianyi Lin","hidden":false},{"_id":"6995270f8d17d1ee8c10ec3b","name":"Pan Ta","hidden":false},{"_id":"6995270f8d17d1ee8c10ec3c","name":"Qiang Zou","hidden":false},{"_id":"6995270f8d17d1ee8c10ec3d","name":"Rongjun Song","hidden":false},{"_id":"6995270f8d17d1ee8c10ec3e","name":"Ruiqi Yang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec3f","name":"Shangqing Tu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec40","name":"Shangtong Yang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec41","name":"Shaoxiang Wu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec42","name":"Shengyan Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec43","name":"Shijie Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec44","name":"Shuang Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec45","name":"Shuyi Fan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec46","name":"Wei Qin","hidden":false},{"_id":"6995270f8d17d1ee8c10ec47","name":"Wei Tian","hidden":false},{"_id":"6995270f8d17d1ee8c10ec48","name":"Weining Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec49","name":"Wenbo Yu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec4a","name":"Wenjie Liang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec4b","name":"Xiang Kuang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec4c","name":"Xiangmeng Cheng","hidden":false},{"_id":"6995270f8d17d1ee8c10ec4d","name":"Xiangyang Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec4e","name":"Xiaoquan Yan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec4f","name":"Xiaowei Hu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec50","name":"Xiaoying Ling","hidden":false},{"_id":"6995270f8d17d1ee8c10ec51","name":"Xing Fan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec52","name":"Xingye Xia","hidden":false},{"_id":"6995270f8d17d1ee8c10ec53","name":"Xinyuan Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec54","name":"Xinze Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec55","name":"Xirui Pan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec56","name":"Xunkai Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec57","name":"Yandong Wu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec58","name":"Yanfu Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec59","name":"Yidong Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec5a","name":"Yifan Zhu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec5b","name":"Yijun Tan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec5c","name":"Yilin Zhou","hidden":false},{"_id":"6995270f8d17d1ee8c10ec5d","name":"Yiming Pan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec5e","name":"Ying Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec5f","name":"Yinpei Su","hidden":false},{"_id":"6995270f8d17d1ee8c10ec60","name":"Yipeng Geng","hidden":false},{"_id":"6995270f8d17d1ee8c10ec61","name":"Yipeng Geng","hidden":false},{"_id":"6995270f8d17d1ee8c10ec62","name":"Yong Yan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec63","name":"Yonglin Tan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec64","name":"Yuean Bi","hidden":false},{"_id":"6995270f8d17d1ee8c10ec65","name":"Yuhan Shen","hidden":false},{"_id":"6995270f8d17d1ee8c10ec66","name":"Yuhao Yang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec67","name":"Yujiang Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec68","name":"Yunan Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec69","name":"Yunqing Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec6a","name":"Yuntao Li","hidden":false},{"_id":"6995270f8d17d1ee8c10ec6b","name":"Yurong Wu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec6c","name":"Yutao Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec6d","name":"Yuxi Duan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec6e","name":"Yuxuan Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec6f","name":"Zezhen Liu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec70","name":"Zhengtao Jiang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec71","name":"Zhenhe Yan","hidden":false},{"_id":"6995270f8d17d1ee8c10ec72","name":"Zheyu Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec73","name":"Zhixiang Wei","hidden":false},{"_id":"6995270f8d17d1ee8c10ec74","name":"Zhuo Chen","hidden":false},{"_id":"6995270f8d17d1ee8c10ec75","name":"Zhuoer Feng","hidden":false},{"_id":"6995270f8d17d1ee8c10ec76","name":"Zijun Yao","hidden":false},{"_id":"6995270f8d17d1ee8c10ec77","name":"Ziwei Chai","hidden":false},{"_id":"6995270f8d17d1ee8c10ec78","name":"Ziyuan Wang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec79","name":"Zuzhou Zhang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec7a","name":"Bin Xu","hidden":false},{"_id":"6995270f8d17d1ee8c10ec7b","name":"Minlie Huang","hidden":false},{"_id":"6995270f8d17d1ee8c10ec7c","user":{"_id":"62ec23fcbd19e355478fc584","avatarUrl":"/avatars/d7c567ef5f20bb3b9905cb5015d11e12.svg","isPro":false,"fullname":"Hongning Wang","user":"howang","type":"user"},"name":"Hongning Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:32:02.752Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ec7d","user":{"_id":"65df8cbc2705d9672f55d1aa","avatarUrl":"/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg","isPro":false,"fullname":"Juanzi Li","user":"juanli","type":"user"},"name":"Juanzi Li","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:31:56.007Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ec7e","user":{"_id":"640e73bdfdeaae1390857b62","avatarUrl":"/avatars/cd6779e30f716002a7838ed93d5c0754.svg","isPro":false,"fullname":"Yuxiao Dong","user":"yuxiaod","type":"user"},"name":"Yuxiao Dong","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:31:48.099Z","hidden":false},{"_id":"6995270f8d17d1ee8c10ec7f","user":{"_id":"640dff05474aa6f89556677e","avatarUrl":"/avatars/1b4591c7322d649c797b3125148f1915.svg","isPro":false,"fullname":"Jie Tang","user":"jerytang","type":"user"},"name":"Jie Tang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T12:31:39.525Z","hidden":false}],"publishedAt":"2026-02-17T17:50:56.000Z","submittedOnDailyAt":"2026-02-18T00:12:28.521Z","title":"GLM-5: from Vibe Coding to Agentic Engineering","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.","upvotes":67,"discussionId":"6995270f8d17d1ee8c10ec80","githubRepo":"https://github.com/zai-org/GLM-5","githubRepoAddedBy":"user","ai_summary":"GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.","ai_keywords":["DSA","asynchronous reinforcement learning","agentic engineering","vibe coding","ARC capabilities","post-training efficiency","model alignment","autonomous agents","software engineering","open benchmarks"],"githubStars":1223},"publishedAt":"2026-02-17T12:50:56.000Z","title":"GLM-5: from Vibe Coding to Agentic Engineering","summary":"We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15763.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.14111","authors":[{"_id":"69959cbbed493589ceb5be31","user":{"_id":"6572e9380503eeadb78fb3e3","avatarUrl":"/avatars/cbf26f1b2c72732d37e8cb847c48f152.svg","isPro":false,"fullname":"Anton Korznikov","user":"AntonKorznikov","type":"user"},"name":"Anton Korznikov","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:37:45.391Z","hidden":false},{"_id":"69959cbbed493589ceb5be32","user":{"_id":"661e44cf1d8ffc49b57ba07e","avatarUrl":"/avatars/3e937cc4f784b369b9f996ba82d1b81d.svg","isPro":false,"fullname":"Andrey Galichin","user":"andreuka18","type":"user"},"name":"Andrey Galichin","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:37:51.000Z","hidden":false},{"_id":"69959cbbed493589ceb5be33","user":{"_id":"60cd95ee15ecba5f2200304a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg","isPro":false,"fullname":"Alexey Dontsov","user":"therem","type":"user"},"name":"Alexey Dontsov","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:37:38.913Z","hidden":false},{"_id":"69959cbbed493589ceb5be34","user":{"_id":"66e19e09140031bf85f0e6f3","avatarUrl":"/avatars/6cce10740eb73f066d7ed0fe8ca3a93a.svg","isPro":false,"fullname":"Oleg Rogov","user":"Olegario228","type":"user"},"name":"Oleg Rogov","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:37:59.681Z","hidden":false},{"_id":"69959cbbed493589ceb5be35","name":"Ivan Oseledets","hidden":false},{"_id":"69959cbbed493589ceb5be36","user":{"_id":"662f8d645c4db70c77a203b0","avatarUrl":"/avatars/72f9a3c39b3ba5114388d16a35524835.svg","isPro":false,"fullname":"Elena Tutubalina","user":"tlenusik","type":"user"},"name":"Elena Tutubalina","status":"claimed_verified","statusLastChangedAt":"2026-02-18T12:30:12.235Z","hidden":false}],"publishedAt":"2026-02-15T11:53:55.000Z","submittedOnDailyAt":"2026-02-18T08:38:30.231Z","title":"Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?","submittedOnDailyBy":{"_id":"60cd95ee15ecba5f2200304a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg","isPro":false,"fullname":"Alexey Dontsov","user":"therem","type":"user"},"summary":"Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.","upvotes":55,"discussionId":"69959cbbed493589ceb5be37","ai_summary":"Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations.","ai_keywords":["Sparse Autoencoders","neural networks","activations","explained variance","interpretability","sparse probing","causal editing"]},"publishedAt":"2026-02-15T06:53:55.000Z","title":"Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?","summary":"Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14111.png","numComments":3,"submittedBy":{"_id":"60cd95ee15ecba5f2200304a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg","fullname":"Alexey Dontsov","name":"therem","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.12670","authors":[{"_id":"6994d2138d17d1ee8c10eb51","user":{"_id":"663fe2d26304d377fc253322","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg","isPro":false,"fullname":"Xiangyi Li","user":"xdotli","type":"user"},"name":"Xiangyi Li","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:07:43.446Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb52","name":"Wenbo Chen","hidden":false},{"_id":"6994d2138d17d1ee8c10eb53","user":{"_id":"646fd64f9ba7aeaacb269cba","avatarUrl":"/avatars/df03f3a26b0b1ddfb5f33ad193ca953e.svg","isPro":false,"fullname":"Yimin Liu","user":"yiminn","type":"user"},"name":"Yimin Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:37.185Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb54","name":"Shenghan Zheng","hidden":false},{"_id":"6994d2138d17d1ee8c10eb55","user":{"_id":"6462bf90c9cc74e82e270cb6","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6462bf90c9cc74e82e270cb6/usLDOUhyIfDXA2HZvXvps.jpeg","isPro":true,"fullname":"Kobe Chen","user":"kobe0938","type":"user"},"name":"Xiaokun Chen","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:07:37.777Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb56","user":{"_id":"659a1c9511b48706bab783cc","avatarUrl":"/avatars/6978a5bc7ab284d9f7285f9fd2c8d0e0.svg","isPro":false,"fullname":"Yifeng He","user":"yfhe","type":"user"},"name":"Yifeng He","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:07:41.599Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb57","name":"Yubo Li","hidden":false},{"_id":"6994d2138d17d1ee8c10eb58","user":{"_id":"663bd5fcfb931d4660fd18b7","avatarUrl":"/avatars/17aa421d40fe3532d0ddecbc2accb249.svg","isPro":false,"fullname":"Bingran You","user":"bingran-you","type":"user"},"name":"Bingran You","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:36:48.789Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb59","name":"Haotian Shen","hidden":false},{"_id":"6994d2138d17d1ee8c10eb5a","user":{"_id":"63b5d1bb0d5913eee4869c0d","avatarUrl":"/avatars/ae397f54bbb3debc1f7903f4c6959ae6.svg","isPro":false,"fullname":"Jiankai Sun","user":"zhenv5","type":"user"},"name":"Jiankai Sun","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:36:59.144Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb5b","name":"Shuyi Wang","hidden":false},{"_id":"6994d2138d17d1ee8c10eb5c","user":{"_id":"65cc8abe8ebd392213020575","avatarUrl":"/avatars/e0b49fe07b3553779992092f60aa0b48.svg","isPro":false,"fullname":"qunhongzeng","user":"qunhongzeng","type":"user"},"name":"Qunhong Zeng","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:37:07.249Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb5d","name":"Di Wang","hidden":false},{"_id":"6994d2138d17d1ee8c10eb5e","user":{"_id":"6275a465597c70eb8949fce5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png","isPro":false,"fullname":"Xuandong Zhao","user":"Xuandong","type":"user"},"name":"Xuandong Zhao","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:37:13.482Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb5f","name":"Yuanli Wang","hidden":false},{"_id":"6994d2138d17d1ee8c10eb60","user":{"_id":"65a0dc6690eb7a1524186cc2","avatarUrl":"/avatars/6ecf7fb5aa74f2453d0e3bc7b9cca0d3.svg","isPro":true,"fullname":"Roey Ben Chaim","user":"roeybc","type":"user"},"name":"Roey Ben Chaim","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:37:19.969Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb61","name":"Zonglin Di","hidden":false},{"_id":"6994d2138d17d1ee8c10eb62","name":"Yipeng Gao","hidden":false},{"_id":"6994d2138d17d1ee8c10eb63","user":{"_id":"636d2965a756aeb5c4af05e5","avatarUrl":"/avatars/439650942cb1298aaec98460753dbb9d.svg","isPro":false,"fullname":"quinn","user":"jwhe","type":"user"},"name":"Junwei He","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:35.099Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb64","user":{"_id":"63710bca7a5e5d8efdbff215","avatarUrl":"/avatars/25e7f713d613ec81ba775265eadff8bc.svg","isPro":false,"fullname":"He","user":"Yizhuo","type":"user"},"name":"Yizhuo He","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:07:45.624Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb65","name":"Liqiang Jing","hidden":false},{"_id":"6994d2138d17d1ee8c10eb66","name":"Luyang Kong","hidden":false},{"_id":"6994d2138d17d1ee8c10eb67","name":"Xin Lan","hidden":false},{"_id":"6994d2138d17d1ee8c10eb68","name":"Jiachen Li","hidden":false},{"_id":"6994d2138d17d1ee8c10eb69","user":{"_id":"678d60dae7b3986f59869666","avatarUrl":"/avatars/c78e30338af124001043191d1f4f864c.svg","isPro":false,"fullname":"Songlin Li","user":"vincent-sk-li","type":"user"},"name":"Songlin Li","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:37:39.330Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb6a","name":"Yijiang Li","hidden":false},{"_id":"6994d2138d17d1ee8c10eb6b","user":{"_id":"64b5198c25882acb62fb77ef","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png","isPro":false,"fullname":"Yueqian Lin","user":"linyueqian","type":"user"},"name":"Yueqian Lin","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:07:39.753Z","hidden":false},{"_id":"6994d2138d17d1ee8c10eb6c","name":"Xinyi Liu","hidden":false},{"_id":"6994d2138d17d1ee8c10eb6d","name":"Xuanqing Liu","hidden":false},{"_id":"6994d2138d17d1ee8c10eb6e","name":"Haoran Lyu","hidden":false},{"_id":"6994d2138d17d1ee8c10eb6f","name":"Ze Ma","hidden":false},{"_id":"6994d2138d17d1ee8c10eb70","name":"Bowei Wang","hidden":false},{"_id":"6994d2138d17d1ee8c10eb71","name":"Runhui Wang","hidden":false},{"_id":"6994d2138d17d1ee8c10eb72","name":"Tianyu Wang","hidden":false},{"_id":"6994d2138d17d1ee8c10eb73","name":"Wengao Ye","hidden":false},{"_id":"6994d2138d17d1ee8c10eb74","name":"Yue Zhang","hidden":false},{"_id":"6994d2138d17d1ee8c10eb75","name":"Hanwen Xing","hidden":false},{"_id":"6994d2138d17d1ee8c10eb76","name":"Yiqi Xue","hidden":false},{"_id":"6994d2138d17d1ee8c10eb77","name":"Steven Dillmann","hidden":false},{"_id":"6994d2138d17d1ee8c10eb78","name":"Han-chung Lee","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/7oDWwNkdX1FMVwX0ecYMt.png"],"publishedAt":"2026-02-13T07:06:06.000Z","submittedOnDailyAt":"2026-02-18T11:04:39.385Z","title":"SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks","submittedOnDailyBy":{"_id":"663fe2d26304d377fc253322","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg","isPro":false,"fullname":"Xiangyi Li","user":"xdotli","type":"user"},"summary":"Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.","upvotes":47,"discussionId":"6994d2148d17d1ee8c10eb79","projectPage":"https://skillsbench.ai/","githubRepo":"https://github.com/benchflow-ai/skillsbench","githubRepoAddedBy":"user","ai_summary":"SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.","ai_keywords":["agent skills","LLM agents","SkillsBench","procedural knowledge","curated Skills","self-generated Skills","agent-model configurations","pass rate","domain-specific effects"],"githubStars":443,"organization":{"_id":"69657f24ec1d157f1590a81d","name":"benchflow","fullname":"BenchFlow","avatar":"https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/L6ik2VpL5iizADEXK4W-A.png"}},"publishedAt":"2026-02-13T02:06:06.000Z","title":"SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks","summary":"Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/7oDWwNkdX1FMVwX0ecYMt.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12670.png","numComments":4,"submittedBy":{"_id":"663fe2d26304d377fc253322","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg","fullname":"Xiangyi Li","name":"xdotli","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"69657f24ec1d157f1590a81d","name":"benchflow","fullname":"BenchFlow","avatar":"https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/L6ik2VpL5iizADEXK4W-A.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.14299","authors":[{"_id":"69952cc38d17d1ee8c10ec97","name":"Ming Li","hidden":false},{"_id":"69952cc38d17d1ee8c10ec98","user":{"_id":"6534a434e778506c5b1e5be8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6534a434e778506c5b1e5be8/349SdAnjEdIQJSzWvKfZ4.png","isPro":true,"fullname":"Xirui Li","user":"AIcell","type":"user"},"name":"Xirui Li","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:07:52.645Z","hidden":false},{"_id":"69952cc38d17d1ee8c10ec99","user":{"_id":"647f5af5b0e96764589f3b2a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg","isPro":false,"fullname":"Tianyi Zhou","user":"zhoutianyi","type":"user"},"name":"Tianyi Zhou","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:07:54.520Z","hidden":false}],"publishedAt":"2026-02-15T20:15:28.000Z","submittedOnDailyAt":"2026-02-18T00:37:23.132Z","title":"Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook","submittedOnDailyBy":{"_id":"65031d01cccc7b28a388c719","avatarUrl":"/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg","isPro":false,"fullname":"Ming Li","user":"MingLiiii","type":"user"},"summary":"As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.","upvotes":24,"discussionId":"69952cc38d17d1ee8c10ec9a","projectPage":"https://github.com/MingLiiii/Moltbook_Socialization","githubRepo":"https://github.com/MingLiiii/Moltbook_Socialization","githubRepoAddedBy":"user","ai_summary":"Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.","ai_keywords":["large language model agents","networked environments","AI agent societies","semantic stabilization","lexical turnover","individual inertia","influence persistence","collective consensus","systemic diagnosis","dynamic evolution"],"githubStars":8,"organization":{"_id":"647f5b7daa8c04bbf938c625","name":"umd-zhou-lab","fullname":"Tianyi Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"}},"publishedAt":"2026-02-15T15:15:28.000Z","title":"Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook","summary":"As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14299.png","numComments":4,"submittedBy":{"_id":"65031d01cccc7b28a388c719","avatarUrl":"/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg","fullname":"Ming Li","name":"MingLiiii","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"647f5b7daa8c04bbf938c625","name":"umd-zhou-lab","fullname":"Tianyi Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.15547","authors":[{"_id":"69958815ed493589ceb5be21","user":{"_id":"64d22f33032a420d1863b6ea","avatarUrl":"/avatars/ed3eaf4bab70dd6ab9a2b67b5928e4fb.svg","isPro":false,"fullname":"Mohammad Kalim Akram","user":"makram93","type":"user"},"name":"Mohammad Kalim Akram","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:06:40.906Z","hidden":false},{"_id":"69958815ed493589ceb5be22","user":{"_id":"64c23f6d569648a60737eddb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64c23f6d569648a60737eddb/iZq7bp-yYaGl5VBVoN5Dg.jpeg","isPro":false,"fullname":"Saba Sturua","user":"jupyterjazz","type":"user"},"name":"Saba Sturua","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:06:49.861Z","hidden":false},{"_id":"69958815ed493589ceb5be23","user":{"_id":"6911a37ace661438b73ff25d","avatarUrl":"/avatars/21ae8db1f909229d22d2c93e4f1cb0e0.svg","isPro":false,"fullname":"Nastia Havriushenko","user":"ahavrius","type":"user"},"name":"Nastia Havriushenko","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:06:58.693Z","hidden":false},{"_id":"69958815ed493589ceb5be24","user":{"_id":"645b5a5b438d6cfbe1ad12a1","avatarUrl":"/avatars/f53e63f8e52115a95814b7be1f07a391.svg","isPro":false,"fullname":"Quentin Herreros","user":"qherreros","type":"user"},"name":"Quentin Herreros","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:07:05.539Z","hidden":false},{"_id":"69958815ed493589ceb5be25","user":{"_id":"6476ff2699a5ce743ccea3fc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6476ff2699a5ce743ccea3fc/zmFmF8tXXDaAGcl8RYiRr.jpeg","isPro":false,"fullname":"Michael Günther","user":"michael-guenther","type":"user"},"name":"Michael Günther","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:52:24.830Z","hidden":false},{"_id":"69958815ed493589ceb5be26","user":{"_id":"60638400b1703ddba0d458a7","avatarUrl":"/avatars/50228a18e7f211275a09e3cbd6e2931e.svg","isPro":false,"fullname":"Maximilian Werk","user":"mwerk","type":"user"},"name":"Maximilian Werk","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:07:16.360Z","hidden":false},{"_id":"69958815ed493589ceb5be27","user":{"_id":"603763514de52ff951d89793","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png","isPro":false,"fullname":"Han Xiao","user":"hanxiao","type":"user"},"name":"Han Xiao","status":"claimed_verified","statusLastChangedAt":"2026-02-18T12:34:37.784Z","hidden":false}],"publishedAt":"2026-02-17T12:50:50.000Z","submittedOnDailyAt":"2026-02-18T11:26:39.504Z","title":"jina-embeddings-v5-text: Task-Targeted Embedding Distillation","submittedOnDailyBy":{"_id":"603763514de52ff951d89793","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png","isPro":false,"fullname":"Han Xiao","user":"hanxiao","type":"user"},"summary":"Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.","upvotes":20,"discussionId":"69958816ed493589ceb5be28","ai_summary":"Compact text embedding models are developed through a combined training approach using distillation and contrastive loss, achieving state-of-the-art performance while supporting long-context sequences and efficient quantization.","ai_keywords":["text embedding models","contrastive loss","model distillation","semantic similarity","information retrieval","clustering","classification","embedding models","long texts","binary quantization"],"organization":{"_id":"63563e0c2d14fcd7d83743cf","name":"jinaai","fullname":"Jina AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/603763514de52ff951d89793/wD54VbAHHyHop3uYlJKl4.png"}},"publishedAt":"2026-02-17T07:50:50.000Z","title":"jina-embeddings-v5-text: Task-Targeted Embedding Distillation","summary":"Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15547.png","numComments":2,"submittedBy":{"_id":"603763514de52ff951d89793","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png","fullname":"Han Xiao","name":"hanxiao","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":20,"isUserFollowing":false},"organization":{"_id":"63563e0c2d14fcd7d83743cf","name":"jinaai","fullname":"Jina AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/603763514de52ff951d89793/wD54VbAHHyHop3uYlJKl4.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.14364","authors":[{"_id":"6995b28aed493589ceb5be6e","user":{"_id":"6784acfcf87ca0cb941d23d1","avatarUrl":"/avatars/2a8477197b21ab94ef90801cf08349e1.svg","isPro":false,"fullname":"Tianyu Chen","user":"tianyyuu","type":"user"},"name":"Tianyu Chen","status":"claimed_verified","statusLastChangedAt":"2026-02-18T13:36:14.520Z","hidden":false},{"_id":"6995b28aed493589ceb5be6f","user":{"_id":"6621f4eb64e84619e578aad6","avatarUrl":"/avatars/b1ad96ee354b999fcafb2998a636609c.svg","isPro":false,"fullname":"Dongrui Liu","user":"shenqiorient","type":"user"},"name":"Dongrui Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-20T08:37:33.039Z","hidden":false},{"_id":"6995b28aed493589ceb5be70","name":"Xia Hu","hidden":false},{"_id":"6995b28aed493589ceb5be71","name":"Jingyi Yu","hidden":false},{"_id":"6995b28aed493589ceb5be72","name":"Wenjie Wang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6784acfcf87ca0cb941d23d1/pDX7FiwxALuKgWEh3UQV8.png","https://cdn-uploads.huggingface.co/production/uploads/6784acfcf87ca0cb941d23d1/UAbn-NRzN-Gdh1uHlWt-X.png"],"publishedAt":"2026-02-16T00:33:02.000Z","submittedOnDailyAt":"2026-02-18T13:15:18.094Z","title":"A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)","submittedOnDailyBy":{"_id":"6784acfcf87ca0cb941d23d1","avatarUrl":"/avatars/2a8477197b21ab94ef90801cf08349e1.svg","isPro":false,"fullname":"Tianyu Chen","user":"tianyyuu","type":"user"},"summary":"Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge (AgentDoG-Qwen3-4B) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.","upvotes":20,"discussionId":"6995b28bed493589ceb5be73","githubRepo":"https://github.com/tychenn/clawdbot_report","githubRepoAddedBy":"user","ai_summary":"Clawdbot, a self-hosted AI agent with diverse tool capabilities, exhibits varying safety performance across different risk dimensions, particularly struggling with ambiguous or adversarial inputs despite consistent reliability in specified tasks.","ai_keywords":["tool-using personal AI agent","trajectory-centric evaluation","agent-safety benchmarks","ATBench","LPS-Bench","automated trajectory judge","AgentDoG-Qwen3-4B","interaction trajectories","safety assessment"],"githubStars":1,"organization":{"_id":"636d1501ffbe479c979561f3","name":"ShanghaiTech","fullname":"ShanghaiTech University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1668093297811-636d12455aaed143cd665607.png"}},"publishedAt":"2026-02-15T19:33:02.000Z","title":"A Trajectory-Based Safety Audit of Clawdbot (OpenClaw)","summary":"Clawdbot is a self-hosted, tool-using personal AI agent with a broad action space spanning local execution and web-mediated workflows, which raises heightened safety and security concerns under ambiguity and adversarial steering. We present a trajectory-centric evaluation of Clawdbot across six risk dimensions. Our test suite samples and lightly adapts scenarios from prior agent-safety benchmarks (including ATBench and LPS-Bench) and supplements them with hand-designed cases tailored to Clawdbot's tool surface. We log complete interaction trajectories (messages, actions, tool-call arguments/outputs) and assess safety using both an automated trajectory judge (AgentDoG-Qwen3-4B) and human review. Across 34 canonical cases, we find a non-uniform safety profile: performance is generally consistent on reliability-focused tasks, while most failures arise under underspecified intent, open-ended goals, or benign-seeming jailbreak prompts, where minor misinterpretations can escalate into higher-impact tool actions. We supplemented the overall results with representative case studies and summarized the commonalities of these cases, analyzing the security vulnerabilities and typical failure modes that Clawdbot is prone to trigger in practice.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6784acfcf87ca0cb941d23d1/pDX7FiwxALuKgWEh3UQV8.png","https://cdn-uploads.huggingface.co/production/uploads/6784acfcf87ca0cb941d23d1/UAbn-NRzN-Gdh1uHlWt-X.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14364.png","numComments":2,"submittedBy":{"_id":"6784acfcf87ca0cb941d23d1","avatarUrl":"/avatars/2a8477197b21ab94ef90801cf08349e1.svg","fullname":"Tianyu Chen","name":"tianyyuu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"636d1501ffbe479c979561f3","name":"ShanghaiTech","fullname":"ShanghaiTech University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1668093297811-636d12455aaed143cd665607.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.12279","authors":[{"_id":"69947c8fd2ea89ac106cf9af","user":{"_id":"62b67da0f56de4396ca9e44b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg","isPro":false,"fullname":"Liangyu Chen","user":"liangyuch","type":"user"},"name":"Leon Liangyu Chen","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:38:47.995Z","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b0","user":{"_id":"650a8979c19e5b4c8a6ff062","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/650a8979c19e5b4c8a6ff062/64_JuECX_k_-uK7m7nlua.jpeg","isPro":false,"fullname":"Haoyu Ma","user":"haoyum1997","type":"user"},"name":"Haoyu Ma","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:38:57.190Z","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b1","user":{"_id":"65f09aefcaf237b0a2d4d3ff","avatarUrl":"/avatars/5b03ea49e878058efa3c88e53a6e6a9b.svg","isPro":false,"fullname":"Zhipeng Fan","user":"Jetp","type":"user"},"name":"Zhipeng Fan","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:39:04.149Z","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b2","user":{"_id":"60efe7fa0d920bc7805cada5","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png","isPro":false,"fullname":"Ziqi Huang","user":"Ziqi","type":"user"},"name":"Ziqi Huang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:39:13.973Z","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b3","name":"Animesh Sinha","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b4","user":{"_id":"6549417b3ce45eb764faf993","avatarUrl":"/avatars/d310f475d0697f5f13b3d4141ea0ccaf.svg","isPro":false,"fullname":"Xiaoliang Dai","user":"daixl1992","type":"user"},"name":"Xiaoliang Dai","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:39:20.967Z","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b5","name":"Jialiang Wang","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b6","name":"Zecheng He","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b7","name":"Jianwei Yang","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b8","user":{"_id":"62aba526cae4462c0c6caa0f","avatarUrl":"/avatars/430560ec2c2547f819225769ab432f30.svg","isPro":false,"fullname":"Chunyuan Li","user":"Chunyuan24","type":"user"},"name":"Chunyuan Li","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:39:39.037Z","hidden":false},{"_id":"69947c8fd2ea89ac106cf9b9","user":{"_id":"697563dbce43f259ee32d7ed","avatarUrl":"/avatars/ac626eb596216c6e87f13fc52ba3fa11.svg","isPro":false,"fullname":"Junzhe Sun","user":"junzhesun","type":"user"},"name":"Junzhe Sun","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:39.459Z","hidden":false},{"_id":"69947c8fd2ea89ac106cf9ba","name":"Chu Wang","hidden":false},{"_id":"69947c8fd2ea89ac106cf9bb","user":{"_id":"677c8b2e92550a07fcad0f50","avatarUrl":"/avatars/2be26e8f25e98cfe5b1d227ee0409cd0.svg","isPro":false,"fullname":"Serena Yeung-Levy","user":"yeunglevy","type":"user"},"name":"Serena Yeung-Levy","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:39:53.861Z","hidden":false},{"_id":"69947c8fd2ea89ac106cf9bc","user":{"_id":"6417cf37dce1e4c0229f17b1","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6417cf37dce1e4c0229f17b1/7h-ZCB5f4wif7TsnF-B1M.jpeg","isPro":false,"fullname":"Felix Xu","user":"katanaxu","type":"user"},"name":"Felix Juefei-Xu","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:41.555Z","hidden":false}],"publishedAt":"2026-02-12T18:59:49.000Z","submittedOnDailyAt":"2026-02-18T06:59:40.830Z","title":"UniT: Unified Multimodal Chain-of-Thought Test-time Scaling","submittedOnDailyBy":{"_id":"62b67da0f56de4396ca9e44b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg","isPro":false,"fullname":"Liangyu Chen","user":"liangyuch","type":"user"},"summary":"Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.","upvotes":19,"discussionId":"69947c90d2ea89ac106cf9bd","ai_summary":"UniT framework enables unified multimodal models to perform iterative reasoning and refinement through chain-of-thought test-time scaling, improving both generation and understanding capabilities.","ai_keywords":["unified models","multimodal understanding","multimodal generation","test-time scaling","chain-of-thought reasoning","agentic data synthesis","unified model training","test-time inference","cognitive behaviors","visual reasoning"]},"publishedAt":"2026-02-12T13:59:49.000Z","title":"UniT: Unified Multimodal Chain-of-Thought Test-time Scaling","summary":"Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12279.png","numComments":2,"submittedBy":{"_id":"62b67da0f56de4396ca9e44b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1658586059273-62b67da0f56de4396ca9e44b.jpeg","fullname":"Liangyu Chen","name":"liangyuch","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.15112","authors":[{"_id":"69952dd98d17d1ee8c10ecab","user":{"_id":"6622b683b904cd87220d285d","avatarUrl":"/avatars/8f4f79c2f5fbd69b498cbe03c6c2e9a2.svg","isPro":false,"fullname":"Aniketh","user":"anikethh","type":"user"},"name":"Aniketh Garikaparthi","status":"claimed_verified","statusLastChangedAt":"2026-02-18T13:36:08.825Z","hidden":false},{"_id":"69952dd98d17d1ee8c10ecac","name":"Manasi Patwardhan","hidden":false},{"_id":"69952dd98d17d1ee8c10ecad","name":"Arman Cohan","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6622b683b904cd87220d285d/o11NNNjEOl-7s5I-lL2E3.png"],"publishedAt":"2026-02-16T19:00:03.000Z","submittedOnDailyAt":"2026-02-18T01:22:46.918Z","title":"ResearchGym: Evaluating Language Model Agents on Real-World AI Research","submittedOnDailyBy":{"_id":"6622b683b904cd87220d285d","avatarUrl":"/avatars/8f4f79c2f5fbd69b498cbe03c6c2e9a2.svg","isPro":false,"fullname":"Aniketh","user":"anikethh","type":"user"},"summary":"We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.","upvotes":17,"discussionId":"69952dd98d17d1ee8c10ecae","githubRepo":"https://github.com/Anikethh/ResearchGym","githubRepoAddedBy":"user","ai_summary":"ResearchGym presents a benchmark environment for evaluating AI agents on end-to-end research tasks, revealing significant capability-reliability gaps in current autonomous agents despite occasional state-of-the-art performance.","ai_keywords":["ResearchGym","AI agents","end-to-end research","ICML","ICLR","ACL","datasets","evaluation harness","baseline implementations","containerized task environments","sub-tasks","GPT-5","hypothesis generation","experimental execution","autonomous agents","capability-reliability gap","context length","Claude Code","Codex"],"githubStars":8},"publishedAt":"2026-02-16T14:00:03.000Z","title":"ResearchGym: Evaluating Language Model Agents on Real-World AI Research","summary":"We introduce ResearchGym, a benchmark and execution environment for evaluating AI agents on end-to-end research. To instantiate this, we repurpose five oral and spotlight papers from ICML, ICLR, and ACL. From each paper's repository, we preserve the datasets, evaluation harness, and baseline implementations but withhold the paper's proposed method. This results in five containerized task environments comprising 39 sub-tasks in total. Within each environment, agents must propose novel hypotheses, run experiments, and attempt to surpass strong human baselines on the paper's metrics. In a controlled evaluation of an agent powered by GPT-5, we observe a sharp capability--reliability gap. The agent improves over the provided baselines from the repository in just 1 of 15 evaluations (6.7%) by 11.5%, and completes only 26.5% of sub-tasks on average. We identify recurring long-horizon failure modes, including impatience, poor time and resource management, overconfidence in weak hypotheses, difficulty coordinating parallel experiments, and hard limits from context length. Yet in a single run, the agent surpasses the solution of an ICML 2025 Spotlight task, indicating that frontier agents can occasionally reach state-of-the-art performance, but do so unreliably. We additionally evaluate proprietary agent scaffolds including Claude Code (Opus-4.5) and Codex (GPT-5.2) which display a similar gap. ResearchGym provides infrastructure for systematic evaluation and analysis of autonomous agents on closed-loop research.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6622b683b904cd87220d285d/o11NNNjEOl-7s5I-lL2E3.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15112.png","numComments":4,"submittedBy":{"_id":"6622b683b904cd87220d285d","avatarUrl":"/avatars/8f4f79c2f5fbd69b498cbe03c6c2e9a2.svg","fullname":"Aniketh","name":"anikethh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.15322","authors":[{"_id":"69952aa78d17d1ee8c10ec85","name":"Taejong Joo","hidden":false},{"_id":"69952aa78d17d1ee8c10ec86","name":"Wenhan Xia","hidden":false},{"_id":"69952aa78d17d1ee8c10ec87","name":"Cheolmin Kim","hidden":false},{"_id":"69952aa78d17d1ee8c10ec88","name":"Ming Zhang","hidden":false},{"_id":"69952aa78d17d1ee8c10ec89","name":"Eugene Ie","hidden":false}],"publishedAt":"2026-02-17T02:57:12.000Z","submittedOnDailyAt":"2026-02-18T00:28:04.913Z","title":"On Surprising Effectiveness of Masking Updates in Adaptive Optimizers","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.","upvotes":9,"discussionId":"69952aa88d17d1ee8c10ec8a","ai_summary":"Random parameter update masking achieves superior optimization for large language models by inducing curvature-dependent regularization, with a momentum-aligned variant delivering significant performance improvements over state-of-the-art adaptive optimizers.","ai_keywords":["large language models","dense adaptive optimizers","preconditioners","random masking","RMSProp","curvature-dependent geometric regularization","momentum-gradient alignment","adaptive optimizers","perplexity"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-16T21:57:12.000Z","title":"On Surprising Effectiveness of Masking Updates in Adaptive Optimizers","summary":"Training large language models (LLMs) relies almost exclusively on dense adaptive optimizers with increasingly sophisticated preconditioners. We challenge this by showing that randomly masking parameter updates can be highly effective, with a masked variant of RMSProp consistently outperforming recent state-of-the-art optimizers. Our analysis reveals that the random masking induces a curvature-dependent geometric regularization that smooths the optimization trajectory. Motivated by this finding, we introduce Momentum-aligned gradient masking (Magma), which modulates the masked updates using momentum-gradient alignment. Extensive LLM pre-training experiments show that Magma is a simple drop-in replacement for adaptive optimizers with consistent gains and negligible computational overhead. Notably, for the 1B model size, Magma reduces perplexity by over 19\\% and 9\\% compared to Adam and Muon, respectively.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15322.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.14486","authors":[{"_id":"69954c908d17d1ee8c10ecc4","user":{"_id":"616a7abc82a777e9333a1567","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1634368720319-616a7abc82a777e9333a1567.png","isPro":false,"fullname":"Fabian Gröger","user":"FabianGroeger","type":"user"},"name":"Fabian Gröger","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:07:56.112Z","hidden":false},{"_id":"69954c908d17d1ee8c10ecc5","name":"Shuo Wen","hidden":false},{"_id":"69954c908d17d1ee8c10ecc6","name":"Maria Brbić","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/616a7abc82a777e9333a1567/lP32oGpT13vpLGBXdguN9.mp4"],"publishedAt":"2026-02-16T06:01:23.000Z","submittedOnDailyAt":"2026-02-18T02:55:51.556Z","title":"Revisiting the Platonic Representation Hypothesis: An Aristotelian View","submittedOnDailyBy":{"_id":"616a7abc82a777e9333a1567","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1634368720319-616a7abc82a777e9333a1567.png","isPro":false,"fullname":"Fabian Gröger","user":"FabianGroeger","type":"user"},"summary":"The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.","upvotes":9,"discussionId":"69954c908d17d1ee8c10ecc7","projectPage":"https://brbiclab.epfl.ch/projects/aristotelian/","githubRepo":"https://github.com/mlbio-epfl/aristotelian","githubRepoAddedBy":"user","ai_summary":"Network representations converge toward shared local neighborhood relationships rather than global statistical models, as revealed by calibrated similarity metrics.","ai_keywords":["representational similarity","neural networks","spectral measures","neighborhood similarity","permutation-based null-calibration framework","Platonic Representation Hypothesis","Aristotelian Representation Hypothesis"],"githubStars":11},"publishedAt":"2026-02-16T01:01:23.000Z","title":"Revisiting the Platonic Representation Hypothesis: An Aristotelian View","summary":"The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/616a7abc82a777e9333a1567/lP32oGpT13vpLGBXdguN9.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14486.png","numComments":3,"submittedBy":{"_id":"616a7abc82a777e9333a1567","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1634368720319-616a7abc82a777e9333a1567.png","fullname":"Fabian Gröger","name":"FabianGroeger","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.07854","authors":[{"_id":"699533ab8d17d1ee8c10ecb0","user":{"_id":"6329bdbbde087eac2921e6a9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1663679904323-noauth.jpeg","isPro":false,"fullname":"Xiangchendong","user":"Xiang-cd","type":"user"},"name":"Chendong Xiang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:44:30.797Z","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb1","name":"Jiajun Liu","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb2","name":"Jintao Zhang","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb3","name":"Xiao Yang","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb4","user":{"_id":"662dec002d4c0e85dac07185","avatarUrl":"/avatars/108a8e4e44ebc93f9c091e9ffdab4f26.svg","isPro":false,"fullname":"zhengwei fang","user":"stmrvv","type":"user"},"name":"Zhengwei Fang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:44:40.379Z","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb5","user":{"_id":"63492af03caab0136df08d25","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63492af03caab0136df08d25/-aZzajKl38jT29A-kziCI.jpeg","isPro":false,"fullname":"Shizun Wang","user":"littlepure2333","type":"user"},"name":"Shizun Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:44:54.542Z","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb6","name":"Zijun Wang","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb7","name":"Yingtian Zou","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb8","name":"Hang Su","hidden":false},{"_id":"699533ab8d17d1ee8c10ecb9","name":"Jun Zhu","hidden":false}],"publishedAt":"2026-02-08T08:01:16.000Z","submittedOnDailyAt":"2026-02-18T01:07:59.769Z","title":"Geometry-Aware Rotary Position Embedding for Consistent Video World Model","submittedOnDailyBy":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","isPro":false,"fullname":"Jintao Zhang","user":"jt-zhang","type":"user"},"summary":"Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce ViewRope, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose Geometry-Aware Frame-Sparse Attention, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present ViewBench, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.","upvotes":8,"discussionId":"699533ab8d17d1ee8c10ecba","ai_summary":"ViewRope, a geometry-aware encoding method, enhances long-term consistency in predictive world models by injecting camera-ray directions into video transformer attention layers, addressing spatial persistence issues through relative ray geometry parameterization.","ai_keywords":["predictive world models","screen-space positional embeddings","projective geometry","video transformer self-attention layers","relative ray geometry","frame-sparse attention","loop-closure fidelity","geometric drift"],"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-02-08T03:01:16.000Z","title":"Geometry-Aware Rotary Position Embedding for Consistent Video World Model","summary":"Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce ViewRope, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose Geometry-Aware Frame-Sparse Attention, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present ViewBench, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07854.png","numComments":2,"submittedBy":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","fullname":"Jintao Zhang","name":"jt-zhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":48,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":false},{"paper":{"id":"2602.15200","authors":[{"_id":"699555668d17d1ee8c10ecdd","user":{"_id":"67bc9a9ed9a970de04255711","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_WzRhjLJnkf_ALkP4BuM_.png","isPro":false,"fullname":"Denis Makhov","user":"dennismak1994","type":"user"},"name":"Denis Makhov","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:08:06.033Z","hidden":false},{"_id":"699555668d17d1ee8c10ecde","user":{"_id":"66465dfa508db0bde50d95f2","avatarUrl":"/avatars/8b4a583dc0f3cab0f1cd9a1be3daa01b.svg","isPro":false,"fullname":"Dmitry Shopkhoev","user":"dimitriish","type":"user"},"name":"Dmitriy Shopkhoev","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:08:09.251Z","hidden":false},{"_id":"699555668d17d1ee8c10ecdf","name":"Magauiya Zhussip","hidden":false},{"_id":"699555668d17d1ee8c10ece0","user":{"_id":"6166db59f78a267701a78c2a","avatarUrl":"/avatars/8784efc36f67719e9455b1f081340ed9.svg","isPro":false,"fullname":"Ammar Ali","user":"ammarali32","type":"user"},"name":"Ammar Ali","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:08:07.754Z","hidden":false},{"_id":"699555668d17d1ee8c10ece1","name":"Baher Mohammad","hidden":false},{"_id":"699555668d17d1ee8c10ece2","name":"Stamatios Lefkimmiatis","hidden":false}],"publishedAt":"2026-02-16T21:31:34.000Z","submittedOnDailyAt":"2026-02-18T05:51:07.439Z","title":"COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression","submittedOnDailyBy":{"_id":"6166db59f78a267701a78c2a","avatarUrl":"/avatars/8784efc36f67719e9455b1f081340ed9.svg","isPro":false,"fullname":"Ammar Ali","user":"ammarali32","type":"user"},"summary":"Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.","upvotes":7,"discussionId":"699555678d17d1ee8c10ece3","ai_summary":"COMPOT is a training-free compression framework for Transformer models that uses sparse dictionary learning with orthogonal dictionaries and closed-form updates to achieve better quality-compression trade-offs than traditional low-rank methods.","ai_keywords":["Transformer models","truncated singular value decomposition","sparse dictionary learning","matrix Procrustes orthogonalization","weight factorization","orthogonal dictionaries","sparse coding","layer-wise compression","dynamic allocation strategy","post-training quantization"],"organization":{"_id":"65f1bb3789aedc3dbe201d53","name":"MTSAIR","fullname":"MTSAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6166db59f78a267701a78c2a/kTtlRzMs3RxfOT11vhhG4.jpeg"}},"publishedAt":"2026-02-16T16:31:34.000Z","title":"COMPOT: Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers Compression","summary":"Post-training compression of Transformer models commonly relies on truncated singular value decomposition (SVD). However, enforcing a single shared subspace can degrade accuracy even at moderate compression. Sparse dictionary learning provides a more flexible union-of-subspaces representation, but existing approaches often suffer from iterative dictionary and coefficient updates. We propose COMPOT (Calibration-Optimized Matrix Procrustes Orthogonalization for Transformers), a training-free compression framework that uses a small calibration dataset to estimate a sparse weight factorization. COMPOT employs orthogonal dictionaries that enable closed-form Procrustes updates for the dictionary and analytical single-step sparse coding for the coefficients, eliminating iterative optimization. To handle heterogeneous layer sensitivity under a global compression budget, COMPOT further introduces a one-shot dynamic allocation strategy that adaptively redistributes layer-wise compression rates. Extensive experiments across diverse architectures and tasks show that COMPOT consistently delivers a superior quality-compression trade-off over strong low-rank and sparse baselines, while remaining fully compatible with post-training quantization for extreme compression. Code is available https://github.com/mts-ai/COMPOT{here}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15200.png","numComments":2,"submittedBy":{"_id":"6166db59f78a267701a78c2a","avatarUrl":"/avatars/8784efc36f67719e9455b1f081340ed9.svg","fullname":"Ammar Ali","name":"ammarali32","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":15,"isUserFollowing":false},"organization":{"_id":"65f1bb3789aedc3dbe201d53","name":"MTSAIR","fullname":"MTSAIR","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6166db59f78a267701a78c2a/kTtlRzMs3RxfOT11vhhG4.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.15772","authors":[{"_id":"69953f568d17d1ee8c10ecbc","user":{"_id":"674736347f64a8dbfbf73f97","avatarUrl":"/avatars/35d825248230fe66c4ae88f153b7128e.svg","isPro":false,"fullname":"Sen Ye","user":"sen-ye","type":"user"},"name":"Sen Ye","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:40:22.765Z","hidden":false},{"_id":"69953f568d17d1ee8c10ecbd","user":{"_id":"63f5993afcf95ecac2b419b5","avatarUrl":"/avatars/a8c020080a84d9a663789c4fb19270e9.svg","isPro":false,"fullname":"Mengde Xu","user":"Mendel192","type":"user"},"name":"Mengde Xu","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:40:30.096Z","hidden":false},{"_id":"69953f568d17d1ee8c10ecbe","name":"Shuyang Gu","hidden":false},{"_id":"69953f568d17d1ee8c10ecbf","name":"Di He","hidden":false},{"_id":"69953f568d17d1ee8c10ecc0","name":"Liwei Wang","hidden":false},{"_id":"69953f568d17d1ee8c10ecc1","name":"Han Hu","hidden":false}],"publishedAt":"2026-02-17T18:04:13.000Z","submittedOnDailyAt":"2026-02-18T01:56:50.987Z","title":"Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models","submittedOnDailyBy":{"_id":"674736347f64a8dbfbf73f97","avatarUrl":"/avatars/35d825248230fe66c4ae88f153b7128e.svg","isPro":false,"fullname":"Sen Ye","user":"sen-ye","type":"user"},"summary":"Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.","upvotes":6,"discussionId":"69953f578d17d1ee8c10ecc2","githubRepo":"https://github.com/sen-ye/R3","githubRepoAddedBy":"user","ai_summary":"The Reason-Reflect-Refine framework addresses the trade-off between generation and understanding in multimodal models by reformulating single-step generation into a multi-step process that explicitly incorporates understanding during generation.","ai_keywords":["multimodal models","generative capabilities","understanding capabilities","optimization dilemma","Reason-Reflect-Refine framework","generate-understand-regenerate process"],"githubStars":3},"publishedAt":"2026-02-17T13:04:13.000Z","title":"Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models","summary":"Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of \"generate-understand-regenerate\". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15772.png","numComments":2,"submittedBy":{"_id":"674736347f64a8dbfbf73f97","avatarUrl":"/avatars/35d825248230fe66c4ae88f153b7128e.svg","fullname":"Sen Ye","name":"sen-ye","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.15449","authors":[{"_id":"6995ace9ed493589ceb5be4e","user":{"_id":"60d3b57ad7b174177faabd6e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg","isPro":true,"fullname":"chansung park","user":"chansung","type":"user"},"name":"Chansung Park","status":"claimed_verified","statusLastChangedAt":"2026-02-18T13:36:10.961Z","hidden":false},{"_id":"6995ace9ed493589ceb5be4f","user":{"_id":"64b63712b3b69063d9aff341","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b63712b3b69063d9aff341/nP6GyBKBtnWWoA0TvQF3K.jpeg","isPro":false,"fullname":"John","user":"juyongjiang","type":"user"},"name":"Juyong Jiang","status":"admin_assigned","statusLastChangedAt":"2026-02-20T08:43:19.790Z","hidden":false},{"_id":"6995ace9ed493589ceb5be50","name":"Fan Wang","hidden":false},{"_id":"6995ace9ed493589ceb5be51","user":{"_id":"5f7fbd813e94f16a85448745","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg","isPro":true,"fullname":"Sayak Paul","user":"sayakpaul","type":"user"},"name":"Sayak Paul","status":"admin_assigned","statusLastChangedAt":"2026-02-20T08:43:36.832Z","hidden":false},{"_id":"6995ace9ed493589ceb5be52","name":"Jiasi Shen","hidden":false},{"_id":"6995ace9ed493589ceb5be53","name":"Jing Tang","hidden":false},{"_id":"6995ace9ed493589ceb5be54","name":"Jianguo Li","hidden":false}],"publishedAt":"2026-02-17T09:29:18.000Z","submittedOnDailyAt":"2026-02-18T11:02:10.891Z","title":"TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models","submittedOnDailyBy":{"_id":"60d3b57ad7b174177faabd6e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg","isPro":true,"fullname":"chansung park","user":"chansung","type":"user"},"summary":"Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.","upvotes":6,"discussionId":"6995ace9ed493589ceb5be55","githubRepo":"https://github.com/deep-diver/TAROT","githubRepoAddedBy":"user","ai_summary":"Reinforcement fine-tuning approach for code generation that adapts curriculum design based on model capability through a four-tier test suite structure.","ai_keywords":["Reinforcement Fine-Tuning","curriculum design","test suite","capability-adaptive","functional correctness","reward signals","gradient updates","model capability","code generation"],"githubStars":2},"publishedAt":"2026-02-17T04:29:18.000Z","title":"TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models","summary":"Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15449.png","numComments":3,"submittedBy":{"_id":"60d3b57ad7b174177faabd6e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1659971187637-60d3b57ad7b174177faabd6e.jpeg","fullname":"chansung park","name":"chansung","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5008,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.15156","authors":[{"_id":"6996103b1268a6b79e0d01a5","name":"Shreyas Rajesh","hidden":false},{"_id":"6996103b1268a6b79e0d01a6","name":"Pavan Holur","hidden":false},{"_id":"6996103b1268a6b79e0d01a7","user":{"_id":"6686f852a03abfbb1b5ac5b6","avatarUrl":"/avatars/8d4289a71da6b90064be6aa84dc5debc.svg","isPro":false,"fullname":"Mehmet Yigit Turali","user":"yigitturali","type":"user"},"name":"Mehmet Yigit Turali","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:52:12.105Z","hidden":false},{"_id":"6996103b1268a6b79e0d01a8","user":{"_id":"6524df7715f84b1466308859","avatarUrl":"/avatars/137df5ad12c0ffa1d3e023bc5c8829df.svg","isPro":false,"fullname":"Chenda Duan","user":"Dadaism6","type":"user"},"name":"Chenda Duan","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:52:09.428Z","hidden":false},{"_id":"6996103b1268a6b79e0d01a9","name":"Vwani Roychowdhury","hidden":false}],"publishedAt":"2026-02-16T19:58:03.000Z","submittedOnDailyAt":"2026-02-18T17:06:38.200Z","title":"Panini: Continual Learning in Token Space via Structured Memory","submittedOnDailyBy":{"_id":"64641e79823c7f8805508de9","avatarUrl":"/avatars/f2e1c785c9d20dabb3a373290f73a9c2.svg","isPro":false,"fullname":"Shreyas Rajesh","user":"shreyasrajesh","type":"user"},"summary":"Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.","upvotes":6,"discussionId":"6996103b1268a6b79e0d01aa","ai_summary":"Panini enables efficient and accurate language model reasoning through a non-parametric continual learning framework that uses generative semantic workspaces to store and retrieve knowledge, achieving superior performance with reduced computational overhead.","ai_keywords":["retrieval-augmented generation","continual learning","semantic memory state","Generative Semantic Workspaces","question-answer pairs","reasoning-grounded inference chains","LLM","RAG","inference chains"]},"publishedAt":"2026-02-16T14:58:03.000Z","title":"Panini: Continual Learning in Token Space via Structured Memory","summary":"Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15156.png","numComments":2,"submittedBy":{"_id":"64641e79823c7f8805508de9","avatarUrl":"/avatars/f2e1c785c9d20dabb3a373290f73a9c2.svg","fullname":"Shreyas Rajesh","name":"shreyasrajesh","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.11389","authors":[{"_id":"69957f88ed493589ceb5be08","user":{"_id":"659f9445d5c4ea912705aa4d","avatarUrl":"/avatars/1d3297c3ccad48e5eb6c01e0640dc06d.svg","isPro":false,"fullname":"Heejeong Nam","user":"HazelNam","type":"user"},"name":"Heejeong Nam","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:45:40.276Z","hidden":false},{"_id":"69957f88ed493589ceb5be09","user":{"_id":"69335e48ad96e22e49797a56","avatarUrl":"/avatars/59805856410bc27fed49deecd0b52601.svg","isPro":false,"fullname":"Quentin Le Lidec","user":"quentinll","type":"user"},"name":"Quentin Le Lidec","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:45:46.221Z","hidden":false},{"_id":"69957f88ed493589ceb5be0a","name":"Lucas Maes","hidden":false},{"_id":"69957f88ed493589ceb5be0b","user":{"_id":"64ed0b8c2203a126eb1a5b9a","avatarUrl":"/avatars/9156dc406ed3f9ee62b73657ac20f5ed.svg","isPro":false,"fullname":"Yann LeCun","user":"ylecun","type":"user"},"name":"Yann LeCun","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:46:02.392Z","hidden":false},{"_id":"69957f88ed493589ceb5be0c","user":{"_id":"6629cf0121e410e67c4de179","avatarUrl":"/avatars/157637f7716849675fe44da005fd3ede.svg","isPro":false,"fullname":"Randall Balestriero","user":"RandallBalestriero","type":"user"},"name":"Randall Balestriero","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:45:55.816Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/TVDm4jvyCinbK9ebhON6H.png"],"publishedAt":"2026-02-11T21:47:26.000Z","submittedOnDailyAt":"2026-02-18T07:04:28.743Z","title":"Causal-JEPA: Learning World Models through Object-Level Latent Interventions","submittedOnDailyBy":{"_id":"5f1158120c833276f61f1a84","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg","isPro":false,"fullname":"Niels Rogge","user":"nielsr","type":"user"},"summary":"World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.","upvotes":5,"discussionId":"69957f88ed493589ceb5be0d","githubRepo":"https://github.com/galilai-group/cjepa","githubRepoAddedBy":"user","ai_summary":"C-JEPA extends masked joint embedding prediction to object-centric representations, enabling robust relational understanding through object-level masking that induces causal inductive biases and improves reasoning and control tasks.","ai_keywords":["object-centric representations","masked joint embedding prediction","counterfactual reasoning","latent interventions","causal inductive bias","visual question answering","agent control tasks"],"githubStars":31,"organization":{"_id":"620e88a5b6c1d205f69ed06b","name":"brownu","fullname":"Brown University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63e8e8f2ccae1fe5c61ad852/tBcw2mYI7wPCW319qd6F6.png"}},"publishedAt":"2026-02-11T16:47:26.000Z","title":"Causal-JEPA: Learning World Models through Object-Level Latent Interventions","summary":"World models require robust relational understanding to support prediction, reasoning, and control. While object-centric representations provide a useful abstraction, they are not sufficient to capture interaction-dependent dynamics. We therefore propose C-JEPA, a simple and flexible object-centric world model that extends masked joint embedding prediction from image patches to object-centric representations. By applying object-level masking that requires an object's state to be inferred from other objects, C-JEPA induces latent interventions with counterfactual-like effects and prevents shortcut solutions, making interaction reasoning essential. Empirically, C-JEPA leads to consistent gains in visual question answering, with an absolute improvement of about 20\\% in counterfactual reasoning compared to the same architecture without object-level masking. On agent control tasks, C-JEPA enables substantially more efficient planning by using only 1\\% of the total latent input features required by patch-based world models, while achieving comparable performance. Finally, we provide a formal analysis demonstrating that object-level masking induces a causal inductive bias via latent interventions. Our code is available at https://github.com/galilai-group/cjepa.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/TVDm4jvyCinbK9ebhON6H.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.11389.png","numComments":2,"submittedBy":{"_id":"5f1158120c833276f61f1a84","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg","fullname":"Niels Rogge","name":"nielsr","type":"user","isPro":false,"isHf":true,"isHfAdmin":false,"isMod":false,"followerCount":1097,"isUserFollowing":false},"organization":{"_id":"620e88a5b6c1d205f69ed06b","name":"brownu","fullname":"Brown University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/63e8e8f2ccae1fe5c61ad852/tBcw2mYI7wPCW319qd6F6.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.15620","authors":[{"_id":"69952cdc8d17d1ee8c10ec9c","user":{"_id":"68d42366298c4a04b90bc575","avatarUrl":"/avatars/51df491a1e6cdcc5887444620692a502.svg","isPro":false,"fullname":"Shiqi Liu","user":"ShiqiLiu","type":"user"},"name":"Shiqi Liu","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:42:07.826Z","hidden":false},{"_id":"69952cdc8d17d1ee8c10ec9d","name":"Zeyu He","hidden":false},{"_id":"69952cdc8d17d1ee8c10ec9e","name":"Guojian Zhan","hidden":false},{"_id":"69952cdc8d17d1ee8c10ec9f","name":"Letian Tao","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca0","user":{"_id":"68ecc8477b9cd18b1020719e","avatarUrl":"/avatars/ebe459c2b2ecf124cba04590d26b6123.svg","isPro":false,"fullname":"Zhilong Zheng","user":"zzzzl-h","type":"user"},"name":"Zhilong Zheng","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:42:28.293Z","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca1","name":"Jiang Wu","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca2","name":"Yinuo Wang","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca3","name":"Yang Guan","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca4","user":{"_id":"64b0fc8f4db8a01408ce1042","avatarUrl":"/avatars/8eb265c114444065ddff6e8f06126666.svg","isPro":false,"fullname":"Kehua Sheng","user":"KehuaSheng","type":"user"},"name":"Kehua Sheng","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:42:39.054Z","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca5","name":"Bo Zhang","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca6","user":{"_id":"65bb68849170f742fd10aff5","avatarUrl":"/avatars/42a7b3ab2132132d409d39e67f152dec.svg","isPro":false,"fullname":"Keqiang Li","user":"UestcJay","type":"user"},"name":"Keqiang Li","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:42:45.671Z","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca7","name":"Jingliang Duan","hidden":false},{"_id":"69952cdc8d17d1ee8c10eca8","name":"Shengbo Eben Li","hidden":false}],"publishedAt":"2026-02-17T14:46:48.000Z","submittedOnDailyAt":"2026-02-18T00:37:25.341Z","title":"STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO, 20-Entropy and JustRL.","upvotes":3,"discussionId":"69952cdd8d17d1ee8c10eca9","ai_summary":"Research identifies spurious tokens as the cause of training instability in reinforcement learning fine-tuning of large language models and proposes a solution that selectively masks problematic gradient updates to improve reasoning performance.","ai_keywords":["reinforcement learning","policy gradients","token probability","policy entropy","training instability","spurious tokens","policy optimization","gradient updates","mathematical reasoning benchmarks","Qwen models","GRPO","Entropy","JustRL"]},"publishedAt":"2026-02-17T09:46:48.000Z","title":"STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens","summary":"Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO, 20-Entropy and JustRL.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15620.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.15278","authors":[{"_id":"69952bb48d17d1ee8c10ec8c","user":{"_id":"6327a69dd907fdcccb5fab8b","avatarUrl":"/avatars/2e55406b20c472c55b6e14783a917445.svg","isPro":false,"fullname":"Manuel Cherep","user":"mcherep","type":"user"},"name":"Manuel Cherep","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:43:04.860Z","hidden":false},{"_id":"69952bb48d17d1ee8c10ec8d","user":{"_id":"6544c4aab4a3f3a2f490f14c","avatarUrl":"/avatars/7103d99ebed1182caa22eece4185e6d4.svg","isPro":false,"fullname":"Pranav M R","user":"pranavmr","type":"user"},"name":"Pranav M R","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:43:11.313Z","hidden":false},{"_id":"69952bb48d17d1ee8c10ec8e","name":"Pattie Maes","hidden":false},{"_id":"69952bb48d17d1ee8c10ec8f","user":{"_id":"631eef9bc1a8269da392cae8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1663590904331-631eef9bc1a8269da392cae8.jpeg","isPro":true,"fullname":"Nikhil Singh","user":"nikhilsingh","type":"user"},"name":"Nikhil Singh","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:43:17.801Z","hidden":false}],"publishedAt":"2026-02-17T00:33:53.000Z","submittedOnDailyAt":"2026-02-18T00:32:40.002Z","title":"Visual Persuasion: What Influences Decisions of Vision-Language Models?","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.","upvotes":3,"discussionId":"69952bb48d17d1ee8c10ec90","projectPage":"https://visual-persuasion-website.vercel.app/","ai_summary":"Visual-language models' decision-making preferences are studied through controlled image choice tasks with systematic input perturbations, revealing visual vulnerabilities and safety concerns.","ai_keywords":["vision-language models","visual utility","revealed preference","visual prompt optimization","image generation model","choice probability","interpretability pipeline","visual vulnerabilities","safety concerns"]},"publishedAt":"2026-02-16T19:33:53.000Z","title":"Visual Persuasion: What Influences Decisions of Vision-Language Models?","summary":"The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15278.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":236,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.12978","authors":[{"_id":"69954df08d17d1ee8c10ecc9","user":{"_id":"65d72c946a36b5b354f80cf8","avatarUrl":"/avatars/2cfc99ccd4f52f4a60c7fa40fe4313b7.svg","isPro":false,"fullname":"lyfeng","user":"lyfeng001","type":"user"},"name":"Yufeng Liu","status":"claimed_verified","statusLastChangedAt":"2026-02-18T09:08:01.932Z","hidden":false},{"_id":"69954df08d17d1ee8c10ecca","name":"Hang Yu","hidden":false},{"_id":"69954df08d17d1ee8c10eccb","name":"Juntu Zhao","hidden":false},{"_id":"69954df08d17d1ee8c10eccc","name":"Bocheng Li","hidden":false},{"_id":"69954df08d17d1ee8c10eccd","name":"Di Zhang","hidden":false},{"_id":"69954df08d17d1ee8c10ecce","name":"Mingzhu Li","hidden":false},{"_id":"69954df08d17d1ee8c10eccf","user":{"_id":"65192da566e78720a7611ab8","avatarUrl":"/avatars/97a68200d1bf49e199a741c0a28bab85.svg","isPro":false,"fullname":"wenxuan wu","user":"psdem","type":"user"},"name":"Wenxuan Wu","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:43:59.472Z","hidden":false},{"_id":"69954df08d17d1ee8c10ecd0","user":{"_id":"64c1d107c8a50776282a6524","avatarUrl":"/avatars/b6382dbaff0b2fc98e84944f57cad045.svg","isPro":false,"fullname":"Yingdong Hu","user":"Yingdong-Hu","type":"user"},"name":"Yingdong Hu","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:43:50.818Z","hidden":false},{"_id":"69954df08d17d1ee8c10ecd1","user":{"_id":"67cfea72ad91643b5cb92b26","avatarUrl":"/avatars/11937ee700bf6a743cb627928bebd214.svg","isPro":false,"fullname":"Junyuan Xie","user":"piiswrong","type":"user"},"name":"Junyuan Xie","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:43:43.652Z","hidden":false},{"_id":"69954df08d17d1ee8c10ecd2","user":{"_id":"66868659ccb9539da85c4e14","avatarUrl":"/avatars/515a49363872c23d57a6f75063606348.svg","isPro":false,"fullname":"Junliang Guo","user":"leo-guo","type":"user"},"name":"Junliang Guo","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:43:36.712Z","hidden":false},{"_id":"69954df08d17d1ee8c10ecd3","user":{"_id":"61ad24836da53246bd6ac410","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61ad24836da53246bd6ac410/o-FL-C6B77iB94wyAtTuO.png","isPro":true,"fullname":"Dequan Wang","user":"dqwang","type":"user"},"name":"Dequan Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:43:30.541Z","hidden":false},{"_id":"69954df08d17d1ee8c10ecd4","name":"Yang Gao","hidden":false}],"publishedAt":"2026-02-13T14:56:06.000Z","submittedOnDailyAt":"2026-02-18T03:00:13.099Z","title":"Learning Native Continuation for Action Chunking Flow Policies","submittedOnDailyBy":{"_id":"65d72c946a36b5b354f80cf8","avatarUrl":"/avatars/2cfc99ccd4f52f4a60c7fa40fe4313b7.svg","isPro":false,"fullname":"lyfeng","user":"lyfeng001","type":"user"},"summary":"Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.","upvotes":3,"discussionId":"69954df18d17d1ee8c10ecd5","projectPage":"https://lyfeng001.github.io/Legato/","ai_summary":"Legato improves action-chunked Vision Language Action models by using training-time continuation methods that ensure smooth trajectories and reduce multimodal switching during real-time execution.","ai_keywords":["Vision Language Action models","action chunking","denoising","flow-based policies","schedule-shaped mixture","randomized schedule condition","trajectory smoothness","multimodal switching","task completion time"]},"publishedAt":"2026-02-13T09:56:06.000Z","title":"Learning Native Continuation for Action Chunking Flow Policies","summary":"Action chunking enables Vision Language Action (VLA) models to run in real time, but naive chunked execution often exhibits discontinuities at chunk boundaries. Real-Time Chunking (RTC) alleviates this issue but is external to the policy, leading to spurious multimodal switching and trajectories that are not intrinsically smooth. We propose Legato, a training-time continuation method for action-chunked flow-based VLA policies. Specifically, Legato initializes denoising from a schedule-shaped mixture of known actions and noise, exposing the model to partial action information. Moreover, Legato reshapes the learned flow dynamics to ensure that the denoising process remains consistent between training and inference under per-step guidance. Legato further uses randomized schedule condition during training to support varying inference delays and achieve controllable smoothness. Empirically, Legato produces smoother trajectories and reduces spurious multimodal switching during execution, leading to less hesitation and shorter task completion time. Extensive real-world experiments show that Legato consistently outperforms RTC across five manipulation tasks, achieving approximately 10% improvements in both trajectory smoothness and task completion time.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12978.png","numComments":2,"submittedBy":{"_id":"65d72c946a36b5b354f80cf8","avatarUrl":"/avatars/2cfc99ccd4f52f4a60c7fa40fe4313b7.svg","fullname":"lyfeng","name":"lyfeng001","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.15382","authors":[{"_id":"69961c571268a6b79e0d01cc","name":"Xiaoze Liu","hidden":false},{"_id":"69961c571268a6b79e0d01cd","name":"Ruowang Zhang","hidden":false},{"_id":"69961c571268a6b79e0d01ce","name":"Weichen Yu","hidden":false},{"_id":"69961c571268a6b79e0d01cf","name":"Siheng Xiong","hidden":false},{"_id":"69961c571268a6b79e0d01d0","name":"Liu He","hidden":false},{"_id":"69961c571268a6b79e0d01d1","name":"Feijie Wu","hidden":false},{"_id":"69961c571268a6b79e0d01d2","name":"Hoin Jung","hidden":false},{"_id":"69961c571268a6b79e0d01d3","name":"Matt Fredrikson","hidden":false},{"_id":"69961c571268a6b79e0d01d4","name":"Xiaoqian Wang","hidden":false},{"_id":"69961c571268a6b79e0d01d5","name":"Jing Gao","hidden":false}],"publishedAt":"2026-02-17T06:31:53.000Z","submittedOnDailyAt":"2026-02-18T17:45:18.377Z","title":"The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems","submittedOnDailyBy":{"_id":"6571623e9e8dc83ed5a61410","avatarUrl":"/avatars/c787f9bda8fda66e46cfd19f0dc355b5.svg","isPro":false,"fullname":"Xiaoze Liu","user":"xz-liu","type":"user"},"summary":"Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas","upvotes":2,"discussionId":"69961c581268a6b79e0d01d6","githubRepo":"https://github.com/xz-liu/heterogeneous-latent-mas","githubRepoAddedBy":"user","ai_summary":"A Vision Wormhole framework enables efficient, model-agnostic communication in multi-agent systems by using visual-language models to transfer reasoning states through a shared latent space, reducing computational overhead while maintaining reasoning accuracy.","ai_keywords":["Vision-Language Models","visual interface","latent state transfer","universal visual codec","shared continuous latent space","hub-and-spoke topology","teacher-student distillation","heterogeneous reasoning traces","visual pathway","inter-agent communication"],"githubStars":7,"organization":{"_id":"69961d6093e0b3b58075c964","name":"Purdue-DM-ML","fullname":"Data mining & Machine learning @ Purdue University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6571623e9e8dc83ed5a61410/MQ5nbKlOmS9QmptBIgIdB.png"}},"publishedAt":"2026-02-17T01:31:53.000Z","title":"The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems","summary":"Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15382.png","numComments":2,"submittedBy":{"_id":"6571623e9e8dc83ed5a61410","avatarUrl":"/avatars/c787f9bda8fda66e46cfd19f0dc355b5.svg","fullname":"Xiaoze Liu","name":"xz-liu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"69961d6093e0b3b58075c964","name":"Purdue-DM-ML","fullname":"Data mining & Machine learning @ Purdue University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6571623e9e8dc83ed5a61410/MQ5nbKlOmS9QmptBIgIdB.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.15327","authors":[{"_id":"699550588d17d1ee8c10ecd7","user":{"_id":"624054bcc2c17da6a63eb539","avatarUrl":"/avatars/bf52dc0683b4100733f8696a97696d0e.svg","isPro":true,"fullname":"hlzhang109","user":"hlzhang109","type":"user"},"name":"Hanlin Zhang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:45:20.828Z","hidden":false},{"_id":"699550588d17d1ee8c10ecd8","name":"Jikai Jin","hidden":false},{"_id":"699550588d17d1ee8c10ecd9","user":{"_id":"690526084ae4c0fc109fbaef","avatarUrl":"/avatars/87e1257024d79c4469dad48a8bb3bdab.svg","isPro":false,"fullname":"Vasilis Syrgkanis","user":"vsyrgk","type":"user"},"name":"Vasilis Syrgkanis","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:45:26.632Z","hidden":false},{"_id":"699550588d17d1ee8c10ecda","name":"Sham Kakade","hidden":false}],"publishedAt":"2026-02-17T03:13:51.000Z","submittedOnDailyAt":"2026-02-18T03:13:27.451Z","title":"Prescriptive Scaling Reveals the Evolution of Language Model Capabilities","submittedOnDailyBy":{"_id":"624054bcc2c17da6a63eb539","avatarUrl":"/avatars/bf52dc0683b4100733f8696a97696d0e.svg","isPro":true,"fullname":"hlzhang109","user":"hlzhang109","type":"user"},"summary":"For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.","upvotes":2,"discussionId":"699550588d17d1ee8c10ecdb","projectPage":"https://jkjin.com/prescriptive-scaling","ai_summary":"Large-scale observational analysis estimates capability boundaries and performance predictions for foundation models using quantile regression and evaluates temporal stability across tasks.","ai_keywords":["scaling laws","quantile regression","sigmoid parameterization","conditional quantiles","observational evaluations","model performance","compute budget","capability boundaries","temporal reliability","smoothing"]},"publishedAt":"2026-02-16T22:13:51.000Z","title":"Prescriptive Scaling Reveals the Evolution of Language Model Capabilities","summary":"For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15327.png","numComments":2,"submittedBy":{"_id":"624054bcc2c17da6a63eb539","avatarUrl":"/avatars/bf52dc0683b4100733f8696a97696d0e.svg","fullname":"hlzhang109","name":"hlzhang109","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.13964","authors":[{"_id":"69945fb23dd566a6e9cf9a24","user":{"_id":"68d29966b368904f7508f616","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d29966b368904f7508f616/SVSjwXynfvKxJEp2H6HV9.png","isPro":false,"fullname":"SKYLENAGE","user":"skylenage","type":"user"},"name":"Weiqi Zhai","status":"admin_assigned","statusLastChangedAt":"2026-02-18T16:12:42.765Z","hidden":false},{"_id":"69945fb23dd566a6e9cf9a25","name":"Zhihai Wang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a26","name":"Jinghang Wang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a27","name":"Boyu Yang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a28","name":"Xiaogang Li","hidden":false},{"_id":"69945fb23dd566a6e9cf9a29","name":"Xiang Xu","hidden":false},{"_id":"69945fb23dd566a6e9cf9a2a","name":"Bohan Wang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a2b","name":"Peng Wang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a2c","name":"Xingzhe Wu","hidden":false},{"_id":"69945fb23dd566a6e9cf9a2d","name":"Anfeng Li","hidden":false},{"_id":"69945fb23dd566a6e9cf9a2e","name":"Qiyuan Feng","hidden":false},{"_id":"69945fb23dd566a6e9cf9a2f","name":"Yuhao Zhou","hidden":false},{"_id":"69945fb23dd566a6e9cf9a30","name":"Shoulin Han","hidden":false},{"_id":"69945fb23dd566a6e9cf9a31","name":"Wenjie Luo","hidden":false},{"_id":"69945fb23dd566a6e9cf9a32","name":"Yiyuan Li","hidden":false},{"_id":"69945fb23dd566a6e9cf9a33","name":"Yaxuan Wang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a34","name":"Ruixian Luo","hidden":false},{"_id":"69945fb23dd566a6e9cf9a35","name":"Guojie Lin","hidden":false},{"_id":"69945fb23dd566a6e9cf9a36","name":"Peiyao Xiao","hidden":false},{"_id":"69945fb23dd566a6e9cf9a37","name":"Chengliang Xu","hidden":false},{"_id":"69945fb23dd566a6e9cf9a38","name":"Ben Wang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a39","name":"Zeyu Wang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a3a","name":"Zichao Chen","hidden":false},{"_id":"69945fb23dd566a6e9cf9a3b","name":"Jianan Ye","hidden":false},{"_id":"69945fb23dd566a6e9cf9a3c","name":"Yijie Hu","hidden":false},{"_id":"69945fb23dd566a6e9cf9a3d","name":"Jialong Chen","hidden":false},{"_id":"69945fb23dd566a6e9cf9a3e","name":"Zongwen Shen","hidden":false},{"_id":"69945fb23dd566a6e9cf9a3f","name":"Yuliang Xu","hidden":false},{"_id":"69945fb23dd566a6e9cf9a40","name":"An Yang","hidden":false},{"_id":"69945fb23dd566a6e9cf9a41","name":"Bowen Yu","hidden":false},{"_id":"69945fb23dd566a6e9cf9a42","name":"Dayiheng Liu","hidden":false},{"_id":"69945fb23dd566a6e9cf9a43","name":"Junyang Lin","hidden":false},{"_id":"69945fb23dd566a6e9cf9a44","name":"Hu Wei","hidden":false},{"_id":"69945fb23dd566a6e9cf9a45","name":"Que Shen","hidden":false},{"_id":"69945fb23dd566a6e9cf9a46","name":"Bing Zhao","hidden":false}],"publishedAt":"2026-02-15T02:50:15.000Z","submittedOnDailyAt":"2026-02-18T13:50:40.421Z","title":"HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam","submittedOnDailyBy":{"_id":"68d29966b368904f7508f616","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d29966b368904f7508f616/SVSjwXynfvKxJEp2H6HV9.png","isPro":false,"fullname":"SKYLENAGE","user":"skylenage","type":"user"},"summary":"Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified","upvotes":2,"discussionId":"69945fb23dd566a6e9cf9a47","ai_summary":"HLE-Verified presents a validated and revised version of the HLE benchmark with improved reliability through expert review and model-based checks, demonstrating significant accuracy improvements in language model evaluations.","ai_keywords":["large language models","benchmark evaluation","verification protocol","error taxonomy","binary validation","domain-expert review","model-based cross-checks","two-stage validation","certified benchmark","expert repairs","model-assisted auditing","final adjudication","annotation noise","model confidence"]},"publishedAt":"2026-02-14T21:50:15.000Z","title":"HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam","summary":"Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13964.png","numComments":2,"submittedBy":{"_id":"68d29966b368904f7508f616","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/68d29966b368904f7508f616/SVSjwXynfvKxJEp2H6HV9.png","fullname":"SKYLENAGE","name":"skylenage","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.09653","authors":[{"_id":"698c5851eb12ea74539167f6","user":{"_id":"64b8fd565067873176dacf1a","avatarUrl":"/avatars/1bc2e2085953bc0b5b9b6a28734ac947.svg","isPro":false,"fullname":"Lyu","user":"ShiweiLyu","type":"user"},"name":"Shiwei Lyu","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:41:25.444Z","hidden":false},{"_id":"698c5851eb12ea74539167f7","user":{"_id":"640ed3e9f2d7c41a1e9a9fde","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678693486092-640ed3e9f2d7c41a1e9a9fde.jpeg","isPro":false,"fullname":"Xidong Wang","user":"Xidong","type":"user"},"name":"Xidong Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:41:14.981Z","hidden":false},{"_id":"698c5851eb12ea74539167f8","name":"Lei Liu","hidden":false},{"_id":"698c5851eb12ea74539167f9","name":"Hao Zhu","hidden":false},{"_id":"698c5851eb12ea74539167fa","name":"Chaohe Zhang","hidden":false},{"_id":"698c5851eb12ea74539167fb","name":"Jian Wang","hidden":false},{"_id":"698c5851eb12ea74539167fc","user":{"_id":"64bbdb049a69e8da48a721b3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iqrYveaPTvOs_GbLWiTGu.png","isPro":false,"fullname":"Jinjie Gu","user":"dannygjj","type":"user"},"name":"Jinjie Gu","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:41:41.179Z","hidden":false},{"_id":"698c5851eb12ea74539167fd","user":{"_id":"637c6703ca8542a0ba900ccb","avatarUrl":"/avatars/288ed63a1efa566c3f01e850c6ba5dd5.svg","isPro":false,"fullname":"Wang","user":"Benyou","type":"user"},"name":"Benyou Wang","status":"admin_assigned","statusLastChangedAt":"2026-02-18T13:41:52.586Z","hidden":false},{"_id":"698c5851eb12ea74539167fe","name":"Yue Shen","hidden":false}],"publishedAt":"2026-02-10T11:02:57.000Z","submittedOnDailyAt":"2026-02-18T01:50:16.955Z","title":"ClinAlign: Scaling Healthcare Alignment from Clinician Preference","submittedOnDailyBy":{"_id":"640ed3e9f2d7c41a1e9a9fde","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678693486092-640ed3e9f2d7c41a1e9a9fde.jpeg","isPro":false,"fullname":"Xidong Wang","user":"Xidong","type":"user"},"summary":"Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.","upvotes":2,"discussionId":"698c5851eb12ea74539167ff","projectPage":"https://github.com/AQ-MedAI/ClinAlign","githubRepo":"https://github.com/AQ-MedAI/ClinAlign","githubRepoAddedBy":"user","ai_summary":"A two-stage framework addresses alignment of large language models with clinician preferences through physician-verified examples and distilled clinical principles for improved medical reasoning.","ai_keywords":["large language models","clinician preferences","HealthRubrics","HealthPrinciples","HealthBench-Hard","offline alignment","inference-time tool","self-revision","parameter-efficient fine-tuning"],"githubStars":3},"publishedAt":"2026-02-10T06:02:57.000Z","title":"ClinAlign: Scaling Healthcare Alignment from Clinician Preference","summary":"Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09653.png","numComments":2,"submittedBy":{"_id":"640ed3e9f2d7c41a1e9a9fde","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678693486092-640ed3e9f2d7c41a1e9a9fde.jpeg","fullname":"Xidong Wang","name":"Xidong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.12235","authors":[{"_id":"69947bbbd2ea89ac106cf997","name":"Julia Belikova","hidden":false},{"_id":"69947bbbd2ea89ac106cf998","name":"Danila Rozhevskii","hidden":false},{"_id":"69947bbbd2ea89ac106cf999","user":{"_id":"693b43e05908e6c3a08ddf38","avatarUrl":"/avatars/7b56943f8a5e130f1a2a3b7331cfb892.svg","isPro":false,"fullname":"Dennis","user":"wexumin","type":"user"},"name":"Dennis Svirin","status":"claimed_verified","statusLastChangedAt":"2026-02-17T15:08:12.027Z","hidden":false},{"_id":"69947bbbd2ea89ac106cf99a","name":"Konstantin Polev","hidden":false},{"_id":"69947bbbd2ea89ac106cf99b","user":{"_id":"605473729d7c1d4d81b7e52b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg","isPro":false,"fullname":"Alexander Panchenko","user":"apanc","type":"user"},"name":"Alexander Panchenko","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:53:44.030Z","hidden":false}],"publishedAt":"2026-02-12T18:15:08.000Z","submittedOnDailyAt":"2026-02-18T12:20:32.631Z","title":"Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation","submittedOnDailyBy":{"_id":"605473729d7c1d4d81b7e52b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg","isPro":false,"fullname":"Alexander Panchenko","user":"apanc","type":"user"},"summary":"Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.","upvotes":1,"discussionId":"69947bbbd2ea89ac106cf99c","ai_summary":"Soft compression architectures for long-context LLMs use query-aware probing classifiers to detect token overflow and mitigate compression-induced errors.","ai_keywords":["soft compression architectures","long-context processing","large language models","compressed tokens","token overflow","query-agnostic saturation statistics","xRAG","probing classifiers","AUC-ROC","HotpotQA","SQuADv2","TriviaQA"],"organization":{"_id":"60f1454b100ab9c285090cc2","name":"s-nlp","fullname":"s-nlp","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1662038598497-605473729d7c1d4d81b7e52b.jpeg"}},"publishedAt":"2026-02-12T13:15:08.000Z","title":"Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation","summary":"Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-relevant content -- remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it. In the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12235.png","numComments":2,"submittedBy":{"_id":"605473729d7c1d4d81b7e52b","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg","fullname":"Alexander Panchenko","name":"apanc","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"60f1454b100ab9c285090cc2","name":"s-nlp","fullname":"s-nlp","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1662038598497-605473729d7c1d4d81b7e52b.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.10210","authors":[{"_id":"699633b01268a6b79e0d01f6","user":{"_id":"6660de6a78c9d00c8d3bebfd","avatarUrl":"/avatars/77a77185e9e3d9a92ab5f35dfe76a1fc.svg","isPro":false,"fullname":"Junhong Lin","user":"junhongmit","type":"user"},"name":"Junhong Lin","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:52:18.589Z","hidden":false},{"_id":"699633b01268a6b79e0d01f7","name":"Bing Zhang","hidden":false},{"_id":"699633b01268a6b79e0d01f8","name":"Song Wang","hidden":false},{"_id":"699633b01268a6b79e0d01f9","name":"Ziyan Liu","hidden":false},{"_id":"699633b01268a6b79e0d01fa","name":"Dan Gutfreund","hidden":false},{"_id":"699633b01268a6b79e0d01fb","name":"Julian Shun","hidden":false},{"_id":"699633b01268a6b79e0d01fc","name":"Yada Zhu","hidden":false}],"publishedAt":"2026-02-10T19:04:01.000Z","submittedOnDailyAt":"2026-02-18T19:34:31.086Z","title":"How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge","submittedOnDailyBy":{"_id":"6660de6a78c9d00c8d3bebfd","avatarUrl":"/avatars/77a77185e9e3d9a92ab5f35dfe76a1fc.svg","isPro":false,"fullname":"Junhong Lin","user":"junhongmit","type":"user"},"summary":"Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.","upvotes":1,"discussionId":"699633b01268a6b79e0d01fd","projectPage":"https://junhongmit.github.io/HybridRAG-Bench/","githubRepo":"https://github.com/junhongmit/HybridRAG-Bench","githubRepoAddedBy":"user","ai_summary":"HybridRAG-Bench evaluates retrieval-intensive multi-hop reasoning in large language models by combining unstructured text and structured knowledge graphs from recent scientific literature, providing a contamination-aware benchmark that distinguishes genuine retrieval and reasoning from parametric recall.","ai_keywords":["large language models","hybrid external knowledge","knowledge graphs","retrieval-intensive reasoning","multi-hop reasoning","parametric recall","contamination-aware evaluation","knowledge-augmented reasoning"],"githubStars":1,"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}},"publishedAt":"2026-02-10T14:04:01.000Z","title":"How Much Reasoning Do Retrieval-Augmented Models Add beyond LLMs? A Benchmarking Framework for Multi-Hop Inference over Hybrid Knowledge","summary":"Large language models (LLMs) continue to struggle with knowledge-intensive questions that require up-to-date information and multi-hop reasoning. Augmenting LLMs with hybrid external knowledge, such as unstructured text and structured knowledge graphs, offers a promising alternative to costly continual pretraining. As such, reliable evaluation of their retrieval and reasoning capabilities becomes critical. However, many existing benchmarks increasingly overlap with LLM pretraining data, which means answers or supporting knowledge may already be encoded in model parameters, making it difficult to distinguish genuine retrieval and reasoning from parametric recall. We introduce HybridRAG-Bench, a framework for constructing benchmarks to evaluate retrieval-intensive, multi-hop reasoning over hybrid knowledge. HybridRAG-Bench automatically couples unstructured text and structured knowledge graph representations derived from recent scientific literature on arXiv, and generates knowledge-intensive question-answer pairs grounded in explicit reasoning paths. The framework supports flexible domain and time-frame selection, enabling contamination-aware and customizable evaluation as models and knowledge evolve. Experiments across three domains (artificial intelligence, governance and policy, and bioinformatics) demonstrate that HybridRAG-Bench rewards genuine retrieval and reasoning rather than parametric recall, offering a principled testbed for evaluating hybrid knowledge-augmented reasoning systems. We release our code and data at github.com/junhongmit/HybridRAG-Bench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10210.png","numComments":2,"submittedBy":{"_id":"6660de6a78c9d00c8d3bebfd","avatarUrl":"/avatars/77a77185e9e3d9a92ab5f35dfe76a1fc.svg","fullname":"Junhong Lin","name":"junhongmit","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"63728bde14d543d507ae970d","name":"MIT","fullname":"Massachusetts Institute of Technology","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"},"isAuthorParticipating":true}]