[{"paper":{"id":"2602.12675","authors":[{"_id":"69967cd61268a6b79e0d02d9","user":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","isPro":false,"fullname":"Jintao Zhang","user":"jt-zhang","type":"user"},"name":"Jintao Zhang","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:51:51.054Z","hidden":false},{"_id":"69967cd61268a6b79e0d02da","user":{"_id":"658c1802a1105f8157ad1db9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/658c1802a1105f8157ad1db9/WzjY29SkngxkKfiTYcssh.jpeg","isPro":false,"fullname":"whx1003","user":"whx1003","type":"user"},"name":"Haoxu Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:51:48.596Z","hidden":false},{"_id":"69967cd61268a6b79e0d02db","name":"Kai Jiang","hidden":false},{"_id":"69967cd61268a6b79e0d02dc","name":"Kaiwen Zheng","hidden":false},{"_id":"69967cd61268a6b79e0d02dd","name":"Youhe Jiang","hidden":false},{"_id":"69967cd61268a6b79e0d02de","name":"Ion Stoica","hidden":false},{"_id":"69967cd61268a6b79e0d02df","name":"Jianfei Chen","hidden":false},{"_id":"69967cd61268a6b79e0d02e0","name":"Jun Zhu","hidden":false},{"_id":"69967cd61268a6b79e0d02e1","name":"Joseph E. Gonzalez","hidden":false}],"publishedAt":"2026-02-13T07:16:02.000Z","submittedOnDailyAt":"2026-02-19T00:32:11.371Z","title":"SLA2: Sparse-Linear Attention with Learnable Routing and QAT","submittedOnDailyBy":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","isPro":false,"fullname":"Jintao Zhang","user":"jt-zhang","type":"user"},"summary":"Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.","upvotes":45,"discussionId":"69967cd61268a6b79e0d02e2","projectPage":"https://github.com/thu-ml/SLA","ai_summary":"SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.","ai_keywords":["sparse-linear attention","diffusion models","attention sparsity","learnable router","quantization-aware fine-tuning","attention error","direct decomposition"],"organization":{"_id":"61f20a9ce108f2cba2dc0730","name":"Berkeley","fullname":"UC Berkeley","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}},"publishedAt":"2026-02-13T02:16:02.000Z","title":"SLA2: Sparse-Linear Attention with Learnable Routing and QAT","summary":"Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12675.png","numComments":4,"submittedBy":{"_id":"66c0a08bac74db25de8427ec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg","fullname":"Jintao Zhang","name":"jt-zhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":47,"isUserFollowing":false},"organization":{"_id":"61f20a9ce108f2cba2dc0730","name":"Berkeley","fullname":"UC Berkeley","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.14296","authors":[{"_id":"6997c36c7a658569d5a1012a","name":"Yifan Wu","hidden":false},{"_id":"6997c36c7a658569d5a1012b","name":"Yiran Peng","hidden":false},{"_id":"6997c36c7a658569d5a1012c","name":"Yiyu Chen","hidden":false},{"_id":"6997c36c7a658569d5a1012d","name":"Jianhao Ruan","hidden":false},{"_id":"6997c36c7a658569d5a1012e","name":"Zijie Zhuang","hidden":false},{"_id":"6997c36c7a658569d5a1012f","name":"Cheng Yang","hidden":false},{"_id":"6997c36c7a658569d5a10130","name":"Jiayi Zhang","hidden":false},{"_id":"6997c36c7a658569d5a10131","name":"Man Chen","hidden":false},{"_id":"6997c36c7a658569d5a10132","name":"Yenchi Tseng","hidden":false},{"_id":"6997c36c7a658569d5a10133","name":"Zhaoyang Yu","hidden":false},{"_id":"6997c36c7a658569d5a10134","name":"Liang Chen","hidden":false},{"_id":"6997c36c7a658569d5a10135","name":"Yuyao Zhai","hidden":false},{"_id":"6997c36c7a658569d5a10136","name":"Bang Liu","hidden":false},{"_id":"6997c36c7a658569d5a10137","name":"Chenglin Wu","hidden":false},{"_id":"6997c36c7a658569d5a10138","name":"Yuyu Luo","hidden":false}],"publishedAt":"2026-02-15T20:03:19.000Z","submittedOnDailyAt":"2026-02-19T23:44:05.401Z","title":"AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines","submittedOnDailyBy":{"_id":"65685ef7d0a121b8e81afe2a","avatarUrl":"/avatars/bdf540632ff7e27564353ba4d799f9c9.svg","isPro":false,"fullname":"Evan","user":"Evanwu50020","type":"user"},"summary":"The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.","upvotes":34,"discussionId":"6997c36d7a658569d5a10139","projectPage":"https://evanwu1125.github.io/AWW_homepage/","ai_summary":"AutoWebWorld synthesizes verifiable web environments using finite state machines and coding agents, enabling efficient training of autonomous Web GUI agents through automated trajectory generation and verification.","ai_keywords":["Finite State Machines","coding agents","web environments","automated search-and-verify pipeline","synthetic data","trajectory generation","verifiable environments","autonomous Web GUI agents"],"organization":{"_id":"665abecde9121df9e6e43e33","name":"HKUST-GZ2","fullname":"Hong Kong University of Science and Technology(GuangZhou)"}},"publishedAt":"2026-02-15T15:03:19.000Z","title":"AutoWebWorld: Synthesizing Infinite Verifiable Web Environments via Finite State Machines","summary":"The performance of autonomous Web GUI agents heavily relies on the quality and quantity of their training data. However, a fundamental bottleneck persists: collecting interaction trajectories from real-world websites is expensive and difficult to verify. The underlying state transitions are hidden, leading to reliance on inconsistent and costly external verifiers to evaluate step-level correctness. To address this, we propose AutoWebWorld, a novel framework for synthesizing controllable and verifiable web environments by modeling them as Finite State Machines (FSMs) and use coding agents to translate FSMs into interactive websites. Unlike real websites, where state transitions are implicit, AutoWebWorld explicitly defines all states, actions, and transition rules. This enables programmatic verification: action correctness is checked against predefined rules, and task success is confirmed by reaching a goal state in the FSM graph. AutoWebWorld enables a fully automated search-and-verify pipeline, generating over 11,663 verified trajectories from 29 diverse web environments at only $0.04 per trajectory. Training on this synthetic data significantly boosts real-world performance. Our 7B Web GUI agent outperforms all baselines within 15 steps on WebVoyager. Furthermore, we observe a clear scaling law: as the synthetic data volume increases, performance on WebVoyager and Online-Mind2Web consistently improves.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14296.png","numComments":1,"submittedBy":{"_id":"65685ef7d0a121b8e81afe2a","avatarUrl":"/avatars/bdf540632ff7e27564353ba4d799f9c9.svg","fullname":"Evan","name":"Evanwu50020","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5,"isUserFollowing":false},"organization":{"_id":"665abecde9121df9e6e43e33","name":"HKUST-GZ2","fullname":"Hong Kong University of Science and Technology(GuangZhou)"},"isAuthorParticipating":false},{"paper":{"id":"2602.14979","authors":[{"_id":"69968e1e1268a6b79e0d02f1","user":{"_id":"64731a68a7f23affe7736d3d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8wESKLFcS2ltPzL-wpG4Z.jpeg","isPro":false,"fullname":"Ronghao Dang","user":"RH-Dang","type":"user"},"name":"Ronghao Dang","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:00:05.563Z","hidden":false},{"_id":"69968e1e1268a6b79e0d02f2","user":{"_id":"66224557c61c7fbd98099079","avatarUrl":"/avatars/a4f2144585c808865c73b5b7f0087c1f.svg","isPro":false,"fullname":"Jiayan Guo","user":"SpaceProduct","type":"user"},"name":"Jiayan Guo","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:00:12.888Z","hidden":false},{"_id":"69968e1e1268a6b79e0d02f3","name":"Bohan Hou","hidden":false},{"_id":"69968e1e1268a6b79e0d02f4","user":{"_id":"609115c79a8bcaa437b234a9","avatarUrl":"/avatars/1631a91030703d8397133363cf82c863.svg","isPro":false,"fullname":"Leng Sicong","user":"Sicong","type":"user"},"name":"Sicong Leng","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:00:23.420Z","hidden":false},{"_id":"69968e1e1268a6b79e0d02f5","user":{"_id":"6388af095a3d2a335622cb7c","avatarUrl":"/avatars/f548ce6a902cee8bdc74179bcd45534c.svg","isPro":false,"fullname":"Kehan Li","user":"lkhl","type":"user"},"name":"Kehan Li","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:51:43.746Z","hidden":false},{"_id":"69968e1e1268a6b79e0d02f6","name":"Xin Li","hidden":false},{"_id":"69968e1e1268a6b79e0d02f7","user":{"_id":"67a6082b2f32323bfb5e6641","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NZvjm4Kapc7AgReexgRgi.png","isPro":false,"fullname":"jiangpin","user":"jiangpinliu","type":"user"},"name":"Jiangpin Liu","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:00:30.219Z","hidden":false},{"_id":"69968e1e1268a6b79e0d02f8","user":{"_id":"67fcc97cede5c434e0cc37e3","avatarUrl":"/avatars/b07e0a4744c1045828a621146ee6d3c2.svg","isPro":false,"fullname":"yunxuan mao","user":"maoyunxuan","type":"user"},"name":"Yunxuan Mao","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:00:35.800Z","hidden":false},{"_id":"69968e1e1268a6b79e0d02f9","name":"Zhikai Wang","hidden":false},{"_id":"69968e1e1268a6b79e0d02fa","name":"Yuqian Yuan","hidden":false},{"_id":"69968e1e1268a6b79e0d02fb","name":"Minghao Zhu","hidden":false},{"_id":"69968e1e1268a6b79e0d02fc","user":{"_id":"667e2e4ebfbdb7d21df57084","avatarUrl":"/avatars/dfc6e9e2805c6f22773495c4f02399b8.svg","isPro":false,"fullname":"Xiao Lin","user":"Chrislin21","type":"user"},"name":"Xiao Lin","status":"claimed_verified","statusLastChangedAt":"2026-02-19T11:48:50.334Z","hidden":false},{"_id":"69968e1e1268a6b79e0d02fd","name":"Yang Bai","hidden":false},{"_id":"69968e1e1268a6b79e0d02fe","name":"Qian Jiang","hidden":false},{"_id":"69968e1e1268a6b79e0d02ff","name":"Yaxi Zhao","hidden":false},{"_id":"69968e1e1268a6b79e0d0300","name":"Minghua Zeng","hidden":false},{"_id":"69968e1e1268a6b79e0d0301","user":{"_id":"64bbe6904d2052b1aaf4f2d7","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64bbe6904d2052b1aaf4f2d7/g0A43kSoy2Wba5JZ2Hcry.jpeg","isPro":false,"fullname":"junlong gao","user":"jlgao23","type":"user"},"name":"Junlong Gao","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:01:11.266Z","hidden":false},{"_id":"69968e1e1268a6b79e0d0302","name":"Yuming Jiang","hidden":false},{"_id":"69968e1e1268a6b79e0d0303","name":"Jun Cen","hidden":false},{"_id":"69968e1e1268a6b79e0d0304","user":{"_id":"65fd82762bf2cd20ddaa193f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png","isPro":false,"fullname":"Siteng Huang","user":"huangsiteng","type":"user"},"name":"Siteng Huang","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:51:46.265Z","hidden":false},{"_id":"69968e1e1268a6b79e0d0305","name":"Liuyi Wang","hidden":false},{"_id":"69968e1e1268a6b79e0d0306","name":"Wenqiao Zhang","hidden":false},{"_id":"69968e1e1268a6b79e0d0307","name":"Chengju Liu","hidden":false},{"_id":"69968e1e1268a6b79e0d0308","name":"Jianfei Yang","hidden":false},{"_id":"69968e1e1268a6b79e0d0309","name":"Shijian Lu","hidden":false},{"_id":"69968e1e1268a6b79e0d030a","name":"Deli Zhao","hidden":false}],"publishedAt":"2026-02-13T18:59:56.000Z","submittedOnDailyAt":"2026-02-19T02:00:43.226Z","title":"RynnBrain: Open Embodied Foundation Models","submittedOnDailyBy":{"_id":"609115c79a8bcaa437b234a9","avatarUrl":"/avatars/1631a91030703d8397133363cf82c863.svg","isPro":false,"fullname":"Leng Sicong","user":"Sicong","type":"user"},"summary":"Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.","upvotes":27,"discussionId":"69968e1e1268a6b79e0d030b","projectPage":"https://alibaba-damo-academy.github.io/RynnBrain.github.io/","githubRepo":"https://github.com/alibaba-damo-academy/RynnBrain","githubRepoAddedBy":"user","ai_summary":"RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.","ai_keywords":["multimodal foundation models","embodied intelligence","spatiotemporal foundation model","egocentric understanding","spatiotemporal localization","physically grounded reasoning","physics-aware planning","MoE","post-trained variants","embodied foundation models","vision understanding benchmarks"],"githubStars":402,"organization":{"_id":"6808e7522a4d69d5111da55f","name":"Alibaba-DAMO-Academy","fullname":"DAMO Academy","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"}},"publishedAt":"2026-02-13T13:59:56.000Z","title":"RynnBrain: Open Embodied Foundation Models","summary":"Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14979.png","numComments":4,"submittedBy":{"_id":"609115c79a8bcaa437b234a9","avatarUrl":"/avatars/1631a91030703d8397133363cf82c863.svg","fullname":"Leng Sicong","name":"Sicong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":20,"isUserFollowing":false},"organization":{"_id":"6808e7522a4d69d5111da55f","name":"Alibaba-DAMO-Academy","fullname":"DAMO Academy","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.16705","authors":[{"_id":"69967a291268a6b79e0d02bb","user":{"_id":"6201fc5d91d53938a6432fbf","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg","isPro":false,"fullname":"Runpei Dong","user":"RunpeiDong","type":"user"},"name":"Runpei Dong","status":"claimed_verified","statusLastChangedAt":"2026-02-19T09:51:52.994Z","hidden":false},{"_id":"69967a291268a6b79e0d02bc","name":"Ziyan Li","hidden":false},{"_id":"69967a291268a6b79e0d02bd","name":"Xialin He","hidden":false},{"_id":"69967a291268a6b79e0d02be","name":"Saurabh Gupta","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"],"publishedAt":"2026-02-18T18:55:02.000Z","submittedOnDailyAt":"2026-02-19T00:21:31.776Z","title":"Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation","submittedOnDailyBy":{"_id":"6201fc5d91d53938a6432fbf","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg","isPro":false,"fullname":"Runpei Dong","user":"RunpeiDong","type":"user"},"summary":"Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.","upvotes":25,"discussionId":"69967a2a1268a6b79e0d02bf","projectPage":"https://hero-humanoid.github.io/","ai_summary":"HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.","ai_keywords":["end-effector tracking policy","inverse kinematics","neural forward model","goal adjustment","replanning","open-vocabulary large vision models","loco-manipulation","visual generalization"],"organization":{"_id":"65448bef5b5d9185ba3202b9","name":"UIUC-CS","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}},"publishedAt":"2026-02-18T13:55:02.000Z","title":"Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation","summary":"Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16705.png","numComments":3,"submittedBy":{"_id":"6201fc5d91d53938a6432fbf","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg","fullname":"Runpei Dong","name":"RunpeiDong","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7,"isUserFollowing":false},"organization":{"_id":"65448bef5b5d9185ba3202b9","name":"UIUC-CS","fullname":"University of Illinois at Urbana-Champaign","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2602.16317","authors":[{"_id":"6996e36c2b6c6794ff97a40f","name":"Maksim Elistratov","hidden":false},{"_id":"6996e36c2b6c6794ff97a410","name":"Marina Barannikov","hidden":false},{"_id":"6996e36c2b6c6794ff97a411","name":"Gregory Ivanov","hidden":false},{"_id":"6996e36c2b6c6794ff97a412","name":"Valentin Khrulkov","hidden":false},{"_id":"6996e36c2b6c6794ff97a413","name":"Anton Konushin","hidden":false},{"_id":"6996e36c2b6c6794ff97a414","name":"Andrey Kuznetsov","hidden":false},{"_id":"6996e36c2b6c6794ff97a415","user":{"_id":"67d5a331eab66ce9cb01bae4","avatarUrl":"/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg","isPro":false,"fullname":"DMITRII ZHEMCHUZHNIKOV","user":"zhemchuzhnikov","type":"user"},"name":"Dmitrii Zhemchuzhnikov","status":"claimed_verified","statusLastChangedAt":"2026-02-19T11:48:52.062Z","hidden":false}],"publishedAt":"2026-02-18T09:54:57.000Z","submittedOnDailyAt":"2026-02-19T11:41:42.311Z","title":"CADEvolve: Creating Realistic CAD via Program Evolution","submittedOnDailyBy":{"_id":"67d5a331eab66ce9cb01bae4","avatarUrl":"/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg","isPro":false,"fullname":"DMITRII ZHEMCHUZHNIKOV","user":"zhemchuzhnikov","type":"user"},"summary":"Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.","upvotes":19,"discussionId":"6996e36d2b6c6794ff97a416","githubRepo":"https://github.com/zhemdi/CADEvolve","githubRepoAddedBy":"user","ai_summary":"CADEvolve presents an evolution-based approach using VLM-guided edits to generate complex CAD programs from simple primitives, creating a large dataset for improved Image2CAD performance.","ai_keywords":["CAD","VLM","evolution-based pipeline","parametric generators","CadQuery","Image2CAD","DeepCAD","Fusion 360","MCB"],"githubStars":7},"publishedAt":"2026-02-18T04:54:57.000Z","title":"CADEvolve: Creating Realistic CAD via Program Evolution","summary":"Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16317.png","numComments":3,"submittedBy":{"_id":"67d5a331eab66ce9cb01bae4","avatarUrl":"/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg","fullname":"DMITRII ZHEMCHUZHNIKOV","name":"zhemchuzhnikov","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.14080","authors":[{"_id":"699694c21268a6b79e0d031a","name":"Nitay Calderon","hidden":false},{"_id":"699694c21268a6b79e0d031b","name":"Eyal Ben-David","hidden":false},{"_id":"699694c21268a6b79e0d031c","name":"Zorik Gekhman","hidden":false},{"_id":"699694c21268a6b79e0d031d","name":"Eran Ofek","hidden":false},{"_id":"699694c21268a6b79e0d031e","name":"Gal Yona","hidden":false}],"publishedAt":"2026-02-15T10:13:30.000Z","submittedOnDailyAt":"2026-02-19T02:14:32.347Z","title":"Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality","submittedOnDailyBy":{"_id":"62d6a0c18faee0ac953c51fa","avatarUrl":"/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg","isPro":false,"fullname":"Nitay Calderon","user":"nitay","type":"user"},"summary":"Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.","upvotes":16,"discussionId":"699694c21268a6b79e0d031f","ai_summary":"LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information.","ai_keywords":["factuality evaluations","LLMs","factual knowledge","encoded facts","recall accessibility","long-tail facts","reverse questions","thinking","reasoning","encoding saturation"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-15T05:13:30.000Z","title":"Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality","summary":"Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14080.png","numComments":3,"submittedBy":{"_id":"62d6a0c18faee0ac953c51fa","avatarUrl":"/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg","fullname":"Nitay Calderon","name":"nitay","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.16008","authors":[{"_id":"6996e55577d9294348451e19","user":{"_id":"6671be9ff022d14aa10df864","avatarUrl":"/avatars/dd085abefa38c1604dc2ceabf472816d.svg","isPro":false,"fullname":"Adnan El Assadi","user":"AdnanElAssadi","type":"user"},"name":"Adnan El Assadi","status":"claimed_verified","statusLastChangedAt":"2026-02-19T14:42:12.758Z","hidden":false},{"_id":"6996e55577d9294348451e1a","user":{"_id":"64cc0e80a257a3212c0c4b24","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png","isPro":false,"fullname":"Isaac Chung","user":"isaacchung","type":"user"},"name":"Isaac Chung","status":"claimed_verified","statusLastChangedAt":"2026-02-19T11:04:09.879Z","hidden":false},{"_id":"6996e55577d9294348451e1b","name":"Chenghao Xiao","hidden":false},{"_id":"6996e55577d9294348451e1c","user":{"_id":"61af4544d691b3aadd1f62b6","avatarUrl":"/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg","isPro":false,"fullname":"Solomatin Roman","user":"Samoed","type":"user"},"name":"Roman Solomatin","status":"claimed_verified","statusLastChangedAt":"2026-02-19T11:04:05.803Z","hidden":false},{"_id":"6996e55577d9294348451e1d","name":"Animesh Jha","hidden":false},{"_id":"6996e55577d9294348451e1e","name":"Rahul Chand","hidden":false},{"_id":"6996e55577d9294348451e1f","name":"Silky Singh","hidden":false},{"_id":"6996e55577d9294348451e20","user":{"_id":"67d0ea5d94e08c03254dfb56","avatarUrl":"/avatars/8e511f5bcebfa869e00bc644e9957162.svg","isPro":false,"fullname":"kaitlyn wang","user":"wangusbeef","type":"user"},"name":"Kaitlyn Wang","status":"claimed_verified","statusLastChangedAt":"2026-02-19T14:41:41.369Z","hidden":false},{"_id":"6996e55577d9294348451e21","name":"Ali Sartaz Khan","hidden":false},{"_id":"6996e55577d9294348451e22","name":"Marc Moussa Nasser","hidden":false},{"_id":"6996e55577d9294348451e23","name":"Sufen Fong","hidden":false},{"_id":"6996e55577d9294348451e24","name":"Pengfei He","hidden":false},{"_id":"6996e55577d9294348451e25","name":"Alan Xiao","hidden":false},{"_id":"6996e55577d9294348451e26","user":{"_id":"6754994f0a4a1144aec6ef57","avatarUrl":"/avatars/9dc00280582bcb0ace57cb34d25e91a0.svg","isPro":false,"fullname":"Ayush Sunil Munot","user":"AyushM6","type":"user"},"name":"Ayush Sunil Munot","status":"claimed_verified","statusLastChangedAt":"2026-02-19T11:04:07.782Z","hidden":false},{"_id":"6996e55577d9294348451e27","name":"Aditya Shrivastava","hidden":false},{"_id":"6996e55577d9294348451e28","name":"Artem Gazizov","hidden":false},{"_id":"6996e55577d9294348451e29","name":"Niklas Muennighoff","hidden":false},{"_id":"6996e55577d9294348451e2a","name":"Kenneth Enevoldsen","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/61af4544d691b3aadd1f62b6/vkcutGZImTRS5bB6Bk_uY.png"],"publishedAt":"2026-02-17T21:00:51.000Z","submittedOnDailyAt":"2026-02-19T07:59:30.545Z","title":"MAEB: Massive Audio Embedding Benchmark","submittedOnDailyBy":{"_id":"61af4544d691b3aadd1f62b6","avatarUrl":"/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg","isPro":false,"fullname":"Solomatin Roman","user":"Samoed","type":"user"},"summary":"We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.","upvotes":13,"discussionId":"6996e55577d9294348451e2b","projectPage":"http://mteb-leaderboard.hf.space/?benchmark_name=MAEB%28beta%29","ai_summary":"MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.","ai_keywords":["audio embedding benchmark","audio-text reasoning","multilingual speech tasks","acoustic understanding","linguistic tasks","audio encoders","audio large language models","MTEB ecosystem"],"organization":{"_id":"624bfda5459c48438cc39f80","name":"mteb","fullname":"Massive Text Embedding Benchmark","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/OrZxdlg8doDNO2TZ6Q58G.png"}},"publishedAt":"2026-02-17T16:00:51.000Z","title":"MAEB: Massive Audio Embedding Benchmark","summary":"We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/61af4544d691b3aadd1f62b6/vkcutGZImTRS5bB6Bk_uY.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16008.png","numComments":2,"submittedBy":{"_id":"61af4544d691b3aadd1f62b6","avatarUrl":"/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg","fullname":"Solomatin Roman","name":"Samoed","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":45,"isUserFollowing":false},"organization":{"_id":"624bfda5459c48438cc39f80","name":"mteb","fullname":"Massive Text Embedding Benchmark","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/OrZxdlg8doDNO2TZ6Q58G.png"},"isAuthorParticipating":true},{"paper":{"id":"2602.16666","authors":[{"_id":"69967a061268a6b79e0d02b3","name":"Stephan Rabanser","hidden":false},{"_id":"69967a061268a6b79e0d02b4","name":"Sayash Kapoor","hidden":false},{"_id":"69967a061268a6b79e0d02b5","name":"Peter Kirgis","hidden":false},{"_id":"69967a061268a6b79e0d02b6","name":"Kangheng Liu","hidden":false},{"_id":"69967a061268a6b79e0d02b7","name":"Saiteja Utpala","hidden":false},{"_id":"69967a061268a6b79e0d02b8","name":"Arvind Narayanan","hidden":false}],"publishedAt":"2026-02-18T18:05:44.000Z","submittedOnDailyAt":"2026-02-19T00:18:48.136Z","title":"Towards a Science of AI Agent Reliability","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.","upvotes":11,"discussionId":"69967a071268a6b79e0d02b9","projectPage":"https://hal.cs.princeton.edu/reliability","ai_summary":"Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.","ai_keywords":["AI agents","reliability","consistency","robustness","predictability","safety","performance profiling","benchmark evaluation"],"organization":{"_id":"64374111a701a7e744c02b0e","name":"princetonu","fullname":"Princeton University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"}},"publishedAt":"2026-02-18T13:05:44.000Z","title":"Towards a Science of AI Agent Reliability","summary":"AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16666.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":235,"isUserFollowing":false},"organization":{"_id":"64374111a701a7e744c02b0e","name":"princetonu","fullname":"Princeton University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.16301","authors":[{"_id":"69967b821268a6b79e0d02c1","name":"Marissa A. Weis","hidden":false},{"_id":"69967b821268a6b79e0d02c2","name":"Maciej Wołczyk","hidden":false},{"_id":"69967b821268a6b79e0d02c3","name":"Rajai Nasser","hidden":false},{"_id":"69967b821268a6b79e0d02c4","name":"Rif A. Saurous","hidden":false},{"_id":"69967b821268a6b79e0d02c5","name":"Blaise Agüera y Arcas","hidden":false},{"_id":"69967b821268a6b79e0d02c6","name":"João Sacramento","hidden":false},{"_id":"69967b821268a6b79e0d02c7","name":"Alexander Meulemans","hidden":false}],"publishedAt":"2026-02-18T09:31:43.000Z","submittedOnDailyAt":"2026-02-19T00:25:09.660Z","title":"Multi-agent cooperation through in-context co-player inference","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.","upvotes":10,"discussionId":"69967b821268a6b79e0d02c8","ai_summary":"Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.","ai_keywords":["multi-agent reinforcement learning","sequence models","in-context learning","cooperative behavior","learning-aware agents","learning dynamics","fast timescale","meta-learners","decentralized reinforcement learning","co-player diversity"],"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}},"publishedAt":"2026-02-18T04:31:43.000Z","title":"Multi-agent cooperation through in-context co-player inference","summary":"Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16301.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":235,"isUserFollowing":false},"organization":{"_id":"5e6aca39878b8b2bf9806447","name":"google","fullname":"Google","avatar":"https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.16704","authors":[{"_id":"699749f57a658569d5a100a5","name":"Hee Seung Hwang","hidden":false},{"_id":"699749f57a658569d5a100a6","name":"Xindi Wu","hidden":false},{"_id":"699749f57a658569d5a100a7","name":"Sanghyuk Chun","hidden":false},{"_id":"699749f57a658569d5a100a8","name":"Olga Russakovsky","hidden":false}],"publishedAt":"2026-02-18T18:53:18.000Z","submittedOnDailyAt":"2026-02-19T15:40:27.587Z","title":"Reinforced Fast Weights with Next-Sequence Prediction","submittedOnDailyBy":{"_id":"613940c0905b1938233881e3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png","isPro":false,"fullname":"Xindi Wu","user":"xindiw","type":"user"},"summary":"Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.","upvotes":9,"discussionId":"699749f57a658569d5a100a9","githubRepo":"https://github.com/princetonvisualai/ReFINE","githubRepoAddedBy":"user","ai_summary":"REFINE is a reinforcement learning framework that improves fast weight models for long-context modeling by training under next-sequence prediction instead of next-token prediction, enhancing their ability to capture long-range dependencies.","ai_keywords":["fast weight architectures","attention-based transformers","next-token prediction","next-sequence prediction","reinforcement learning","prediction entropy","multi-token rollouts","self-supervised sequence-level rewards","group relative policy optimization","needle-in-a-haystack retrieval","long-context question answering","LongBench"],"githubStars":1,"organization":{"_id":"6735d51c08a190b1caea1f29","name":"PrincetonUniversity","fullname":"Princeton University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"}},"publishedAt":"2026-02-18T13:53:18.000Z","title":"Reinforced Fast Weights with Next-Sequence Prediction","summary":"Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16704.png","numComments":2,"submittedBy":{"_id":"613940c0905b1938233881e3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png","fullname":"Xindi Wu","name":"xindiw","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"6735d51c08a190b1caea1f29","name":"PrincetonUniversity","fullname":"Princeton University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.15922","authors":[{"_id":"6996793f1268a6b79e0d028d","name":"Seonghyeon Ye","hidden":false},{"_id":"6996793f1268a6b79e0d028e","name":"Yunhao Ge","hidden":false},{"_id":"6996793f1268a6b79e0d028f","name":"Kaiyuan Zheng","hidden":false},{"_id":"6996793f1268a6b79e0d0290","name":"Shenyuan Gao","hidden":false},{"_id":"6996793f1268a6b79e0d0291","name":"Sihyun Yu","hidden":false},{"_id":"6996793f1268a6b79e0d0292","name":"George Kurian","hidden":false},{"_id":"6996793f1268a6b79e0d0293","name":"Suneel Indupuru","hidden":false},{"_id":"6996793f1268a6b79e0d0294","name":"You Liang Tan","hidden":false},{"_id":"6996793f1268a6b79e0d0295","name":"Chuning Zhu","hidden":false},{"_id":"6996793f1268a6b79e0d0296","name":"Jiannan Xiang","hidden":false},{"_id":"6996793f1268a6b79e0d0297","name":"Ayaan Malik","hidden":false},{"_id":"6996793f1268a6b79e0d0298","name":"Kyungmin Lee","hidden":false},{"_id":"6996793f1268a6b79e0d0299","name":"William Liang","hidden":false},{"_id":"6996793f1268a6b79e0d029a","name":"Nadun Ranawaka","hidden":false},{"_id":"6996793f1268a6b79e0d029b","name":"Jiasheng Gu","hidden":false},{"_id":"6996793f1268a6b79e0d029c","name":"Yinzhen Xu","hidden":false},{"_id":"6996793f1268a6b79e0d029d","name":"Guanzhi Wang","hidden":false},{"_id":"6996793f1268a6b79e0d029e","name":"Fengyuan Hu","hidden":false},{"_id":"6996793f1268a6b79e0d029f","name":"Avnish Narayan","hidden":false},{"_id":"6996793f1268a6b79e0d02a0","name":"Johan Bjorck","hidden":false},{"_id":"6996793f1268a6b79e0d02a1","name":"Jing Wang","hidden":false},{"_id":"6996793f1268a6b79e0d02a2","name":"Gwanghyun Kim","hidden":false},{"_id":"6996793f1268a6b79e0d02a3","name":"Dantong Niu","hidden":false},{"_id":"6996793f1268a6b79e0d02a4","name":"Ruijie Zheng","hidden":false},{"_id":"6996793f1268a6b79e0d02a5","name":"Yuqi Xie","hidden":false},{"_id":"6996793f1268a6b79e0d02a6","name":"Jimmy Wu","hidden":false},{"_id":"6996793f1268a6b79e0d02a7","name":"Qi Wang","hidden":false},{"_id":"6996793f1268a6b79e0d02a8","name":"Ryan Julian","hidden":false},{"_id":"6996793f1268a6b79e0d02a9","name":"Danfei Xu","hidden":false},{"_id":"6996793f1268a6b79e0d02aa","name":"Yilun Du","hidden":false},{"_id":"6996793f1268a6b79e0d02ab","name":"Yevgen Chebotar","hidden":false},{"_id":"6996793f1268a6b79e0d02ac","name":"Scott Reed","hidden":false},{"_id":"6996793f1268a6b79e0d02ad","name":"Jan Kautz","hidden":false},{"_id":"6996793f1268a6b79e0d02ae","name":"Yuke Zhu","hidden":false},{"_id":"6996793f1268a6b79e0d02af","name":"Linxi \"Jim\" Fan","hidden":false},{"_id":"6996793f1268a6b79e0d02b0","name":"Joel Jang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/aeAgRW6Fq1wOWJzNGPdUm.mp4"],"publishedAt":"2026-02-17T15:04:02.000Z","submittedOnDailyAt":"2026-02-19T00:17:09.596Z","title":"World Action Models are Zero-shot Policies","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.","upvotes":9,"discussionId":"699679401268a6b79e0d02b1","projectPage":"https://dreamzero0.github.io/","githubRepo":"https://github.com/dreamzero0/dreamzero","githubRepoAddedBy":"user","ai_summary":"DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.","ai_keywords":["World Action Model","video diffusion","video backbone","physical dynamics","autoregressive video diffusion model","closed-loop control","cross-embodiment transfer","few-shot embodiment adaptation"],"githubStars":742,"organization":{"_id":"676eec5d639faf44bc95708b","name":"NVIDIA-DIR","fullname":"NVIDIA Deep Imagination Research"}},"publishedAt":"2026-02-17T10:04:02.000Z","title":"World Action Models are Zero-shot Policies","summary":"State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/aeAgRW6Fq1wOWJzNGPdUm.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15922.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":235,"isUserFollowing":false},"organization":{"_id":"676eec5d639faf44bc95708b","name":"NVIDIA-DIR","fullname":"NVIDIA Deep Imagination Research"},"isAuthorParticipating":false},{"paper":{"id":"2602.15989","authors":[{"_id":"699678401268a6b79e0d0275","name":"Xitong Yang","hidden":false},{"_id":"699678401268a6b79e0d0276","name":"Devansh Kukreja","hidden":false},{"_id":"699678401268a6b79e0d0277","name":"Don Pinkus","hidden":false},{"_id":"699678401268a6b79e0d0278","name":"Anushka Sagar","hidden":false},{"_id":"699678401268a6b79e0d0279","name":"Taosha Fan","hidden":false},{"_id":"699678401268a6b79e0d027a","name":"Jinhyung Park","hidden":false},{"_id":"699678401268a6b79e0d027b","name":"Soyong Shin","hidden":false},{"_id":"699678401268a6b79e0d027c","name":"Jinkun Cao","hidden":false},{"_id":"699678401268a6b79e0d027d","name":"Jiawei Liu","hidden":false},{"_id":"699678401268a6b79e0d027e","name":"Nicolas Ugrinovic","hidden":false},{"_id":"699678401268a6b79e0d027f","name":"Matt Feiszli","hidden":false},{"_id":"699678401268a6b79e0d0280","name":"Jitendra Malik","hidden":false},{"_id":"699678401268a6b79e0d0281","name":"Piotr Dollar","hidden":false},{"_id":"699678401268a6b79e0d0282","name":"Kris Kitani","hidden":false}],"publishedAt":"2026-02-17T20:26:37.000Z","submittedOnDailyAt":"2026-02-19T00:11:13.148Z","title":"SAM 3D Body: Robust Full-Body Human Mesh Recovery","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.","upvotes":8,"discussionId":"699678401268a6b79e0d0283","projectPage":"https://ai.meta.com/research/sam3d/","githubRepo":"https://github.com/facebookresearch/sam-3d-body","githubRepoAddedBy":"user","ai_summary":"A promptable 3D human mesh recovery model using a novel parametric representation and encoder-decoder architecture achieves state-of-the-art performance with strong generalization across diverse conditions.","ai_keywords":["3D human mesh recovery","encoder-decoder architecture","parametric mesh representation","Momentum Human Rig","2D keypoints","masks","auxiliary prompts","differentiable optimization","multi-view geometry","dense keypoint detection","data engine","evaluation dataset","qualitative user preference studies","quantitative analysis"],"githubStars":2627},"publishedAt":"2026-02-17T15:26:37.000Z","title":"SAM 3D Body: Robust Full-Body Human Mesh Recovery","summary":"We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15989.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":235,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.07345","authors":[{"_id":"698ab82f1b2dc6b37d61b07f","name":"Lichen Bai","hidden":false},{"_id":"698ab82f1b2dc6b37d61b080","user":{"_id":"651f8133dbf879b8c58f5136","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/651f8133dbf879b8c58f5136/0L8Ecgi5Ietkm_DchJwE-.png","isPro":false,"fullname":"Zikai Zhou","user":"Klayand","type":"user"},"name":"Zikai Zhou","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:04:44.311Z","hidden":false},{"_id":"698ab82f1b2dc6b37d61b081","name":"Shitong Shao","hidden":false},{"_id":"698ab82f1b2dc6b37d61b082","user":{"_id":"6432362fdec2a70d8136877a","avatarUrl":"/avatars/331bf4a691f5b8a662da0cd96c3e6e9b.svg","isPro":false,"fullname":"Wenliang Zhong","user":"Arvin-zhong","type":"user"},"name":"Wenliang Zhong","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:05:13.831Z","hidden":false},{"_id":"698ab82f1b2dc6b37d61b083","name":"Shuo Yang","hidden":false},{"_id":"698ab82f1b2dc6b37d61b084","user":{"_id":"636fd739af79dd193608f7b8","avatarUrl":"/avatars/6fffc6d1abd6adff93e7fe49ab687ee9.svg","isPro":false,"fullname":"Shuo Chen","user":"shuochen","type":"user"},"name":"Shuo Chen","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:05:20.107Z","hidden":false},{"_id":"698ab82f1b2dc6b37d61b085","user":{"_id":"698299703ea114b8ee067149","avatarUrl":"/avatars/07a8cd858178ffae9fd14d91984a202d.svg","isPro":false,"fullname":"Bojun Chen","user":"cbj92","type":"user"},"name":"Bojun Chen","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:05:27.114Z","hidden":false},{"_id":"698ab82f1b2dc6b37d61b086","name":"Zeke Xie","hidden":false}],"publishedAt":"2026-02-07T04:00:20.000Z","submittedOnDailyAt":"2026-02-19T04:15:02.274Z","title":"Optimizing Few-Step Generation with Adaptive Matching Distillation","submittedOnDailyBy":{"_id":"651f8133dbf879b8c58f5136","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/651f8133dbf879b8c58f5136/0L8Ecgi5Ietkm_DchJwE-.png","isPro":false,"fullname":"Zikai Zhou","user":"Klayand","type":"user"},"summary":"Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.","upvotes":6,"discussionId":"698ab82f1b2dc6b37d61b087","ai_summary":"Adaptive Matching Distillation introduces a self-correcting mechanism to improve generative model training by detecting and escaping unstable regions in the optimization landscape.","ai_keywords":["Distribution Matching Distillation","Forbidden Zone","reward proxies","structural signal decomposition","Repulsive Landscape Sharpening","few-step generative models","optimization trajectories"],"organization":{"_id":"65ad19cac14c3cf579ad9b68","name":"HKUSTGZ","fullname":"HKUSTGZ"}},"publishedAt":"2026-02-06T23:00:20.000Z","title":"Optimizing Few-Step Generation with Adaptive Matching Distillation","summary":"Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07345.png","numComments":2,"submittedBy":{"_id":"651f8133dbf879b8c58f5136","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/651f8133dbf879b8c58f5136/0L8Ecgi5Ietkm_DchJwE-.png","fullname":"Zikai Zhou","name":"Klayand","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3,"isUserFollowing":false},"organization":{"_id":"65ad19cac14c3cf579ad9b68","name":"HKUSTGZ","fullname":"HKUSTGZ"},"isAuthorParticipating":true},{"paper":{"id":"2602.16682","authors":[{"_id":"69967cbb1268a6b79e0d02cf","name":"Chuhan Li","hidden":false},{"_id":"69967cbb1268a6b79e0d02d0","name":"Ruilin Han","hidden":false},{"_id":"69967cbb1268a6b79e0d02d1","name":"Joy Hsu","hidden":false},{"_id":"69967cbb1268a6b79e0d02d2","name":"Yongyuan Liang","hidden":false},{"_id":"69967cbb1268a6b79e0d02d3","name":"Rajiv Dhawan","hidden":false},{"_id":"69967cbb1268a6b79e0d02d4","name":"Jiajun Wu","hidden":false},{"_id":"69967cbb1268a6b79e0d02d5","name":"Ming-Hsuan Yang","hidden":false},{"_id":"69967cbb1268a6b79e0d02d6","name":"Xin Eric Wang","hidden":false}],"publishedAt":"2026-02-18T18:22:52.000Z","submittedOnDailyAt":"2026-02-19T00:30:39.390Z","title":"Learning Situated Awareness in the Real World","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.","upvotes":5,"discussionId":"69967cbc1268a6b79e0d02d7","projectPage":"https://sawbench.github.io/","ai_summary":"SAW-Bench presents a new benchmark for evaluating egocentric situated awareness in multimodal foundation models through real-world video datasets with human-annotated question-answer pairs, focusing on observer-centric spatial reasoning tasks.","ai_keywords":["multimodal foundation models","egocentric videos","observer-centric relationships","situated awareness","spatial reasoning","camera geometry","real-world videos","question-answer pairs"]},"publishedAt":"2026-02-18T13:22:52.000Z","title":"Learning Situated Awareness in the Real World","summary":"A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16682.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":235,"isUserFollowing":false},"isAuthorParticipating":false},{"paper":{"id":"2602.16493","authors":[{"_id":"69967f361268a6b79e0d02e4","name":"Yihao Lu","hidden":false},{"_id":"69967f361268a6b79e0d02e5","name":"Wanru Cheng","hidden":false},{"_id":"69967f361268a6b79e0d02e6","name":"Zeyu Zhang","hidden":false},{"_id":"69967f361268a6b79e0d02e7","name":"Hao Tang","hidden":false}],"publishedAt":"2026-02-18T14:30:35.000Z","submittedOnDailyAt":"2026-02-19T00:48:16.410Z","title":"MMA: Multimodal Memory Agent","submittedOnDailyBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"summary":"Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.","upvotes":5,"discussionId":"69967f371268a6b79e0d02e8","githubRepo":"https://github.com/AIGeeksGroup/MMA","githubRepoAddedBy":"user","ai_summary":"Multimodal Memory Agent (MMA) improves long-horizon agent performance by dynamically scoring memory reliability and handling visual biases in retrieval-augmented systems.","ai_keywords":["multimodal agents","external memory","similarity-based retrieval","reliability scoring","source credibility","temporal decay","conflict-aware network consensus","evidence reweighting","belief dynamics","RAG-based agents","visual placebo effect","retrieval-augmented generation","MMA-Bench","Type-B accuracy"],"githubStars":5,"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}},"publishedAt":"2026-02-18T09:30:35.000Z","title":"MMA: Multimodal Memory Agent","summary":"Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16493.png","numComments":2,"submittedBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","fullname":"Zeyu Zhang","name":"SteveZeyuZhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4,"isUserFollowing":false},"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.16173","authors":[{"_id":"699774bb7a658569d5a100df","name":"Kaiqu Liang","hidden":false},{"_id":"699774bb7a658569d5a100e0","name":"Julia Kruk","hidden":false},{"_id":"699774bb7a658569d5a100e1","name":"Shengyi Qian","hidden":false},{"_id":"699774bb7a658569d5a100e2","name":"Xianjun Yang","hidden":false},{"_id":"699774bb7a658569d5a100e3","name":"Shengjie Bi","hidden":false},{"_id":"699774bb7a658569d5a100e4","name":"Yuanshun Yao","hidden":false},{"_id":"699774bb7a658569d5a100e5","name":"Shaoliang Nie","hidden":false},{"_id":"699774bb7a658569d5a100e6","name":"Mingyang Zhang","hidden":false},{"_id":"699774bb7a658569d5a100e7","name":"Lijuan Liu","hidden":false},{"_id":"699774bb7a658569d5a100e8","name":"Jaime Fernández Fisac","hidden":false},{"_id":"699774bb7a658569d5a100e9","name":"Shuyan Zhou","hidden":false},{"_id":"699774bb7a658569d5a100ea","name":"Saghar Hosseini","hidden":false}],"publishedAt":"2026-02-18T04:18:47.000Z","submittedOnDailyAt":"2026-02-19T18:15:19.925Z","title":"Learning Personalized Agents from Human Feedback","submittedOnDailyBy":{"_id":"650237b5d1b0b0db4f29ae8a","avatarUrl":"/avatars/8f92cf8f3f1ddb45c2c58c4a59ce4633.svg","isPro":false,"fullname":"KAIQU LIANG","user":"kaiquliang","type":"user"},"summary":"Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.","upvotes":5,"discussionId":"699774bb7a658569d5a100eb","projectPage":"https://personalized-ai.github.io/","githubRepo":"https://github.com/facebookresearch/PAHF","githubRepoAddedBy":"user","ai_summary":"PAHF framework enables continual personalization of AI agents through explicit user memory and dual feedback channels, allowing rapid adaptation to changing user preferences.","ai_keywords":["personalized agents","human feedback","continual personalization","explicit memory","dual feedback channels","preference modeling","user profiles","embodied manipulation","online shopping"],"githubStars":2,"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}},"publishedAt":"2026-02-17T23:18:47.000Z","title":"Learning Personalized Agents from Human Feedback","summary":"Modern AI agents are powerful but often fail to align with the idiosyncratic, evolving preferences of individual users. Prior approaches typically rely on static datasets, either training implicit preference models on interaction history or encoding user profiles in external memory. However, these approaches struggle with new users and with preferences that change over time. We introduce Personalized Agents from Human Feedback (PAHF), a framework for continual personalization in which agents learn online from live interaction using explicit per-user memory. PAHF operationalizes a three-step loop: (1) seeking pre-action clarification to resolve ambiguity, (2) grounding actions in preferences retrieved from memory, and (3) integrating post-action feedback to update memory when preferences drift. To evaluate this capability, we develop a four-phase protocol and two benchmarks in embodied manipulation and online shopping. These benchmarks quantify an agent's ability to learn initial preferences from scratch and subsequently adapt to persona shifts. Our theoretical analysis and empirical results show that integrating explicit memory with dual feedback channels is critical: PAHF learns substantially faster and consistently outperforms both no-memory and single-channel baselines, reducing initial personalization error and enabling rapid adaptation to preference shifts.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16173.png","numComments":2,"submittedBy":{"_id":"650237b5d1b0b0db4f29ae8a","avatarUrl":"/avatars/8f92cf8f3f1ddb45c2c58c4a59ce4633.svg","fullname":"KAIQU LIANG","name":"kaiquliang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1,"isUserFollowing":false},"organization":{"_id":"5e63d8713071d5be688861b8","name":"facebook","fullname":"AI at Meta","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"},"isAuthorParticipating":false},{"paper":{"id":"2602.15927","authors":[{"_id":"6996b5701268a6b79e0d0361","user":{"_id":"6310a6bb0a43f97f6c5567d3","avatarUrl":"/avatars/04b07c3a6c811337939c951567cb2bf2.svg","isPro":false,"fullname":"Christian Schlarmann","user":"chs20","type":"user"},"name":"Christian Schlarmann","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:04:16.476Z","hidden":false},{"_id":"6996b5701268a6b79e0d0362","name":"Matthias Hein","hidden":false}],"publishedAt":"2026-02-17T18:34:59.000Z","submittedOnDailyAt":"2026-02-19T04:33:14.737Z","title":"Visual Memory Injection Attacks for Multi-Turn Conversations","submittedOnDailyBy":{"_id":"6310a6bb0a43f97f6c5567d3","avatarUrl":"/avatars/04b07c3a6c811337939c951567cb2bf2.svg","isPro":false,"fullname":"Christian Schlarmann","user":"chs20","type":"user"},"summary":"Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection","upvotes":3,"discussionId":"6996b5711268a6b79e0d0363","githubRepo":"https://github.com/chs20/visual-memory-injection","githubRepoAddedBy":"user","ai_summary":"Visual Memory Injection attack enables covert manipulation of generative vision-language models through manipulated images that trigger targeted responses only under specific prompts during multi-turn conversations.","ai_keywords":["generative large vision-language models","Visual Memory Injection","multi-turn conversation","adversarial marketing","political persuasion","trigger prompt","user manipulation"],"githubStars":0},"publishedAt":"2026-02-17T13:34:59.000Z","title":"Visual Memory Injection Attacks for Multi-Turn Conversations","summary":"Generative large vision-language models (LVLMs) have recently achieved impressive performance gains, and their user base is growing rapidly. However, the security of LVLMs, in particular in a long-context multi-turn setting, is largely underexplored. In this paper, we consider the realistic scenario in which an attacker uploads a manipulated image to the web/social media. A benign user downloads this image and uses it as input to the LVLM. Our novel stealthy Visual Memory Injection (VMI) attack is designed such that on normal prompts the LVLM exhibits nominal behavior, but once the user gives a triggering prompt, the LVLM outputs a specific prescribed target message to manipulate the user, e.g. for adversarial marketing or political persuasion. Compared to previous work that focused on single-turn attacks, VMI is effective even after a long multi-turn conversation with the user. We demonstrate our attack on several recent open-weight LVLMs. This article thereby shows that large-scale manipulation of users is feasible with perturbed images in multi-turn conversation settings, calling for better robustness of LVLMs against these attacks. We release the source code at https://github.com/chs20/visual-memory-injection","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15927.png","numComments":2,"submittedBy":{"_id":"6310a6bb0a43f97f6c5567d3","avatarUrl":"/avatars/04b07c3a6c811337939c951567cb2bf2.svg","fullname":"Christian Schlarmann","name":"chs20","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"isAuthorParticipating":true},{"paper":{"id":"2602.14498","authors":[{"_id":"6997c5907a658569d5a10142","name":"Aryan Das","hidden":false},{"_id":"6997c5907a658569d5a10143","name":"Tanishq Rachamalla","hidden":false},{"_id":"6997c5907a658569d5a10144","name":"Koushik Biswas","hidden":false},{"_id":"6997c5907a658569d5a10145","name":"Swalpa Kumar Roy","hidden":false},{"_id":"6997c5907a658569d5a10146","name":"Vinay Kumar Verma","hidden":false}],"publishedAt":"2026-02-16T06:27:51.000Z","submittedOnDailyAt":"2026-02-19T23:54:33.648Z","title":"Uncertainty-Aware Vision-Language Segmentation for Medical Imaging","submittedOnDailyBy":{"_id":"64c4c761958100e5bd302cfd","avatarUrl":"/avatars/dec3f49fb3c78dd4e029e1937b0202de.svg","isPro":false,"fullname":"Vinay Kumar Verma","user":"vkvermaa","type":"user"},"summary":"We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling. To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS","upvotes":3,"discussionId":"6997c5917a658569d5a10147","githubRepo":"https://github.com/arya-domain/UA-VLS","githubRepoAddedBy":"user","ai_summary":"A novel uncertainty-aware multimodal segmentation framework uses radiological images and clinical text with Modality Decoding Attention Blocks and Spectral-Entropic Uncertainty Loss for improved medical diagnosis accuracy.","ai_keywords":["Modality Decoding Attention Block","State Space Mixer","cross-modal fusion","long-range dependency modelling","Spectral-Entropic Uncertainty Loss","vision-language medical segmentation","uncertainty modelling","multimodal segmentation"],"githubStars":9,"organization":{"_id":"6204e7c6522e40b4a18d86f4","name":"IITK","fullname":"Indian Institute of Technology, Kanpur","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1644486419327-6204dac5f5be158b3a1f7bcc.jpeg"}},"publishedAt":"2026-02-16T01:27:51.000Z","title":"Uncertainty-Aware Vision-Language Segmentation for Medical Imaging","summary":"We introduce a novel uncertainty-aware multimodal segmentation framework that leverages both radiological images and associated clinical text for precise medical diagnosis. We propose a Modality Decoding Attention Block (MoDAB) with a lightweight State Space Mixer (SSMix) to enable efficient cross-modal fusion and long-range dependency modelling. To guide learning under ambiguity, we propose the Spectral-Entropic Uncertainty (SEU) Loss, which jointly captures spatial overlap, spectral consistency, and predictive uncertainty in a unified objective. In complex clinical circumstances with poor image quality, this formulation improves model reliability. Extensive experiments on various publicly available medical datasets, QATA-COVID19, MosMed++, and Kvasir-SEG, demonstrate that our method achieves superior segmentation performance while being significantly more computationally efficient than existing State-of-the-Art (SoTA) approaches. Our results highlight the importance of incorporating uncertainty modelling and structured modality alignment in vision-language medical segmentation tasks. Code: https://github.com/arya-domain/UA-VLS","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14498.png","numComments":1,"submittedBy":{"_id":"64c4c761958100e5bd302cfd","avatarUrl":"/avatars/dec3f49fb3c78dd4e029e1937b0202de.svg","fullname":"Vinay Kumar Verma","name":"vkvermaa","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6204e7c6522e40b4a18d86f4","name":"IITK","fullname":"Indian Institute of Technology, Kanpur","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1644486419327-6204dac5f5be158b3a1f7bcc.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.14514","authors":[{"_id":"6997c4ff7a658569d5a1013b","name":"Aryan Das","hidden":false},{"_id":"6997c4ff7a658569d5a1013c","name":"Koushik Biswas","hidden":false},{"_id":"6997c4ff7a658569d5a1013d","name":"Swalpa Kumar Roy","hidden":false},{"_id":"6997c4ff7a658569d5a1013e","name":"Badri Narayana Patro","hidden":false},{"_id":"6997c4ff7a658569d5a1013f","name":"Vinay Kumar Verma","hidden":false}],"publishedAt":"2026-02-16T06:51:29.000Z","submittedOnDailyAt":"2026-02-19T23:52:19.085Z","title":"Efficient Text-Guided Convolutional Adapter for the Diffusion Model","submittedOnDailyBy":{"_id":"64c4c761958100e5bd302cfd","avatarUrl":"/avatars/dec3f49fb3c78dd4e029e1937b0202de.svg","isPro":false,"fullname":"Vinay Kumar Verma","user":"vkvermaa","type":"user"},"summary":"We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters","upvotes":2,"discussionId":"6997c4ff7a658569d5a10140","githubRepo":"https://github.com/arya-domain/Nexus-Adapters","githubRepoAddedBy":"user","ai_summary":"Nexus Adapters are efficient text-guided adapters for diffusion models that preserve structure while enhancing prompt understanding through cross-attention mechanisms, achieving state-of-the-art results with significantly fewer parameters.","ai_keywords":["diffusion-based framework","Structure Preserving Conditional Generation","text-guided efficient adapters","T2I-Adapter","cross-attention mechanisms","multimodal conditioning","Nexus Prime","Nexus Slim"],"githubStars":9,"organization":{"_id":"6204e7c6522e40b4a18d86f4","name":"IITK","fullname":"Indian Institute of Technology, Kanpur","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1644486419327-6204dac5f5be158b3a1f7bcc.jpeg"}},"publishedAt":"2026-02-16T01:51:29.000Z","title":"Efficient Text-Guided Convolutional Adapter for the Diffusion Model","summary":"We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14514.png","numComments":1,"submittedBy":{"_id":"64c4c761958100e5bd302cfd","avatarUrl":"/avatars/dec3f49fb3c78dd4e029e1937b0202de.svg","fullname":"Vinay Kumar Verma","name":"vkvermaa","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"organization":{"_id":"6204e7c6522e40b4a18d86f4","name":"IITK","fullname":"Indian Institute of Technology, Kanpur","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1644486419327-6204dac5f5be158b3a1f7bcc.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2602.08392","authors":[{"_id":"6996a6241268a6b79e0d0337","name":"Xin Wu","hidden":false},{"_id":"6996a6241268a6b79e0d0338","user":{"_id":"662a471e94baa018b00c0f5c","avatarUrl":"/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg","isPro":false,"fullname":"Zhixuan Liang","user":"Liang-ZX","type":"user"},"name":"Zhixuan Liang","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:03:50.355Z","hidden":false},{"_id":"6996a6241268a6b79e0d0339","name":"Yue Ma","hidden":false},{"_id":"6996a6241268a6b79e0d033a","name":"Mengkang Hu","hidden":false},{"_id":"6996a6241268a6b79e0d033b","user":{"_id":"6788b65515046ac33e8dd991","avatarUrl":"/avatars/02a748773277ba1b2cef546a344ed78c.svg","isPro":false,"fullname":"Zhiyuan Qin","user":"70nyq2y","type":"user"},"name":"Zhiyuan Qin","status":"admin_assigned","statusLastChangedAt":"2026-02-19T10:03:58.259Z","hidden":false},{"_id":"6996a6241268a6b79e0d033c","name":"Xiu Li","hidden":false}],"publishedAt":"2026-02-09T08:47:14.000Z","submittedOnDailyAt":"2026-02-19T03:34:23.141Z","title":"BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models","submittedOnDailyBy":{"_id":"662a471e94baa018b00c0f5c","avatarUrl":"/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg","isPro":false,"fullname":"Zhixuan Liang","user":"Liang-ZX","type":"user"},"summary":"Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.","upvotes":2,"discussionId":"6996a6251268a6b79e0d033d","projectPage":"https://bimanibench.github.io/","githubRepo":"https://github.com/bimanibench/BiManiBench","githubRepoAddedBy":"user","ai_summary":"BiManiBench evaluates multimodal large language models on bimanual robotic tasks, revealing limitations in spatial grounding and control despite strong high-level reasoning capabilities.","ai_keywords":["multimodal large language models","embodied AI","robotic intelligence","bimanual tasks","spatial reasoning","action planning","end-effector control","kinematic constraints","perceptual hallucinations","planning failures","inter-arm collision-avoidance","temporal sequencing"],"githubStars":2,"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"}},"publishedAt":"2026-02-09T03:47:14.000Z","title":"BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models","summary":"Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08392.png","numComments":2,"submittedBy":{"_id":"662a471e94baa018b00c0f5c","avatarUrl":"/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg","fullname":"Zhixuan Liang","name":"Liang-ZX","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2,"isUserFollowing":false},"organization":{"_id":"628735cbc83a2d6ab8d14a66","name":"Tsinghua","fullname":"Tsinghua University"},"isAuthorParticipating":true},{"paper":{"id":"2602.14602","authors":[{"_id":"6997ada87a658569d5a1011c","name":"Tianyi Ma","hidden":false},{"_id":"6997ada87a658569d5a1011d","name":"Yiyang Li","hidden":false},{"_id":"6997ada87a658569d5a1011e","name":"Yiyue Qian","hidden":false},{"_id":"6997ada87a658569d5a1011f","name":"Zheyuan Zhang","hidden":false},{"_id":"6997ada87a658569d5a10120","name":"Zehong Wang","hidden":false},{"_id":"6997ada87a658569d5a10121","name":"Chuxu Zhang","hidden":false},{"_id":"6997ada87a658569d5a10122","name":"Yanfang Ye","hidden":false}],"publishedAt":"2026-02-16T10:04:57.000Z","submittedOnDailyAt":"2026-02-19T22:12:13.674Z","title":"OPBench: A Graph Benchmark to Combat the Opioid Crisis","submittedOnDailyBy":{"_id":"660c4dd73134c1a046d0bb23","avatarUrl":"/avatars/fbffd94ef6b2f60e0716b03301cdf9ee.svg","isPro":true,"fullname":"Tianyi (Billy) Ma","user":"mtybilly","type":"user"},"summary":"The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.","upvotes":0,"discussionId":"6997ada97a658569d5a10123","githubRepo":"https://github.com/Tianyi-Billy-Ma/OPBench","githubRepoAddedBy":"user","githubStars":0},"publishedAt":"2026-02-16T05:04:57.000Z","title":"OPBench: A Graph Benchmark to Combat the Opioid Crisis","summary":"The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14602.png","numComments":2,"submittedBy":{"_id":"660c4dd73134c1a046d0bb23","avatarUrl":"/avatars/fbffd94ef6b2f60e0716b03301cdf9ee.svg","fullname":"Tianyi (Billy) Ma","name":"mtybilly","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"isUserFollowing":false},"isAuthorParticipating":false}]